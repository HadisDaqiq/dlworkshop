{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jendrik/.local/share/virtualenvs/dlworkshop-VuPJaqFs/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jendrik/.local/share/virtualenvs/dlworkshop-VuPJaqFs/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "from typing import Dict, List, Iterator, Union, Callable\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.iterators import BucketIterator, BasicIterator\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.models import Model\n",
    "\n",
    "\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MAX_LENGTH = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/cornell_movie_dialogs_corpus/cornell movie-dialogs corpus/formatted_movie_lines.txt'\n",
    "sp_data = home + '/data/workshop_data/southpark_full.csv'\n",
    "log_dir = home + '/data/logs/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process whole documents\n",
    "text = (df.iloc[0]['sentence'])\n",
    "doc = nlp(text)\n",
    "[token.vector for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for reading dialogue data from a csv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.vocab = None\n",
    "\n",
    "    def text_to_instance(self, query: List[Token], reply: List[Token] = None) -> Instance:\n",
    "        query = TextField(query, self.token_indexers)\n",
    "        fields = {\"query\": query}\n",
    "        if reply is not None:\n",
    "            reply = TextField(reply, self.token_indexers)\n",
    "            fields[\"reply\"] = reply\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Lowercase, trim, and remove non-letter characters\n",
    "    @staticmethod\n",
    "    def normalize_string(s):\n",
    "        s = DialogueReader.unicode_to_ascii(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = s.replace(\" . . .\", \" ...\")\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path, header=None, names=['query', 'reply'], sep='\\t')\n",
    "        for _, row in df.iterrows():\n",
    "            query = self.normalize_string(row['query']).split(' ')\n",
    "            reply = self.normalize_string(row['reply']).split(' ')\n",
    "            if len(query) >= MAX_LENGTH or len(reply) >= MAX_LENGTH:\n",
    "                continue\n",
    "            if self.vocab is not None:\n",
    "                if np.any([self.vocab.get_token_index(x) == 1 for x in query]):\n",
    "                    # print(f\"skipped {query} {[self.vocab.get_token_index(x) for x in query]}\")\n",
    "                    continue\n",
    "            query = [Token(word) for word in query] + [Token('#EOS#')]\n",
    "            # print(self.vocab)\n",
    "            if self.vocab is not None:\n",
    "                if np.any([self.vocab.get_token_index(x) == 1 for x in reply]):\n",
    "                    # print(f\"skipped {reply} {[self.vocab.get_token_index(x) for x in reply]}\")\n",
    "                    continue\n",
    "            reply = [Token(word) for word in reply] + [Token('#EOS#')]\n",
    "            yield self.text_to_instance(query, reply)\n",
    " \n",
    "\n",
    "class CartmanReader(DialogueReader):\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        for i in range(len(df)):\n",
    "            if (df['character'][i] != 'Cartman' or\n",
    "                df['episode'][i] != df['episode'][i] or\n",
    "                df['setting'][i] != df['setting'][i] or\n",
    "                not pd.notnull(df['spoken'][i-1]) or\n",
    "                not pd.notnull(df['spoken'][i])):\n",
    "                    continue\n",
    "            query = self.normalize_string(df['spoken'][i-1]).split(' ')\n",
    "            reply = self.normalize_string(df['spoken'][i]).split(' ')\n",
    "            if len(reply) < 3:\n",
    "                continue\n",
    "            if len(query) >= MAX_LENGTH or len(reply) >= MAX_LENGTH:\n",
    "                continue\n",
    "            if self.vocab is not None:\n",
    "                if np.any([self.vocab.get_token_index(x) == 1 for x in query]):\n",
    "                    # print(f\"skipped {query} {[self.vocab.get_token_index(x) for x in query]}\")\n",
    "                    continue\n",
    "            query = [Token(word) for word in query] + [Token('#EOS#')]\n",
    "            # print(self.vocab)\n",
    "            if self.vocab is not None:\n",
    "                if np.any([self.vocab.get_token_index(x) == 1 for x in reply]):\n",
    "                    # print(f\"skipped {reply} {[self.vocab.get_token_index(x) for x in reply]}\")\n",
    "                    continue\n",
    "            reply = [Token(word) for word in reply] + [Token('#EOS#')]\n",
    "            yield self.text_to_instance(query, reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67210it [00:30, 2188.36it/s]\n",
      "1576it [00:01, 920.06it/s]\n",
      "100%|██████████| 68786/68786 [00:00<00:00, 162649.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12103"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader1 = DialogueReader()\n",
    "dialogue_dataset1 = reader1.read(data)\n",
    "reader2 = CartmanReader()\n",
    "cartman_dataset1 = reader2.read(sp_data)\n",
    "\n",
    "sos_token = '#SOS#'\n",
    "vocab = Vocabulary.from_instances(dialogue_dataset1 + cartman_dataset1, min_count={'tokens': 2})\n",
    "vocab.add_token_to_namespace(sos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61340it [00:31, 1925.62it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = DialogueReader()\n",
    "reader.vocab = vocab\n",
    "dialogue_dataset = reader.read(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Vocabulary Statistics----\n",
      "\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'tokens':\n",
      "\tToken: #EOS#\t\tFrequency: 137572\n",
      "\tToken: .\t\tFrequency: 85316\n",
      "\tToken: ?\t\tFrequency: 46524\n",
      "\tToken: you\t\tFrequency: 31360\n",
      "\tToken: i\t\tFrequency: 23926\n",
      "\tToken: s\t\tFrequency: 13499\n",
      "\tToken: what\t\tFrequency: 13444\n",
      "\tToken: !\t\tFrequency: 13254\n",
      "\tToken: it\t\tFrequency: 12442\n",
      "\tToken: the\t\tFrequency: 11704\n",
      "\n",
      "Top 10 longest tokens in namespace 'tokens':\n",
      "\tToken: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\t\tlength: 33\tFrequency: 1\n",
      "\tToken: phreakphreakphreakphreakphreak\t\tlength: 30\tFrequency: 1\n",
      "\tToken: mmmmmmmmmmmmmnnnnnnmmmmmmmm\t\tlength: 27\tFrequency: 1\n",
      "\tToken: bebebudjuhbluhbluhbluhbluh\t\tlength: 26\tFrequency: 1\n",
      "\tToken: bwolololololololololololo\t\tlength: 25\tFrequency: 1\n",
      "\tToken: dudedudedudedudedudedude\t\tlength: 24\tFrequency: 1\n",
      "\tToken: aaaaahhnnnhaaaaaannnhhh\t\tlength: 23\tFrequency: 1\n",
      "\tToken: aaaahhhhnnnahhhnnnggnnn\t\tlength: 23\tFrequency: 1\n",
      "\tToken: aaaaannnnnuuunnnuhhhhh\t\tlength: 22\tFrequency: 1\n",
      "\tToken: rowrrowrrowrrowrrowr\t\tlength: 20\tFrequency: 1\n",
      "\n",
      "Top 10 shortest tokens in namespace 'tokens':\n",
      "\tToken: \t\tlength: 0\tFrequency: 1\n",
      "\tToken: z\t\tlength: 1\tFrequency: 6\n",
      "\tToken: q\t\tlength: 1\tFrequency: 7\n",
      "\tToken: x\t\tlength: 1\tFrequency: 11\n",
      "\tToken: v\t\tlength: 1\tFrequency: 15\n",
      "\tToken: r\t\tlength: 1\tFrequency: 23\n",
      "\tToken: w\t\tlength: 1\tFrequency: 24\n",
      "\tToken: g\t\tlength: 1\tFrequency: 25\n",
      "\tToken: p\t\tlength: 1\tFrequency: 26\n",
      "\tToken: f\t\tlength: 1\tFrequency: 27\n",
      "12104\n"
     ]
    }
   ],
   "source": [
    "vocab.print_statistics()\n",
    "print(vocab.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding: TextFieldEmbedder, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = PytorchSeq2SeqWrapper(nn.GRU(\n",
    "            hidden_size, hidden_size, n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout), bidirectional=True, batch_first=True))\n",
    "\n",
    "    def forward(self, query: Dict[str, torch.Tensor]):\n",
    "        # Convert word indexes to embeddings\n",
    "        mask = get_text_field_mask(query)\n",
    "        \n",
    "        embeddings = self.embedding(query)\n",
    "        \n",
    "        encoder_out, final_states = self.gru(embeddings, mask)\n",
    "        \n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = encoder_out[:, :, :self.hidden_size] + encoder_out[:, : ,self.hidden_size:]\n",
    "        \n",
    "        # Return output and final hidden state\n",
    "        return outputs, final_states\n",
    "    \n",
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(-1, encoder_output.size(1), -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        # attn_energies = attn_energies.t()\n",
    "        # print(attn_energies.shape)\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(2)\n",
    "    \n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding: TextFieldEmbedder, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size, hidden_size, n_layers, \n",
    "            dropout=(0 if n_layers == 1 else dropout), batch_first=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, encoder_outputs, last_hidden=None):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding({'tokens': input_step})\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)#.squeeze(2)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = torch.sum(attn_weights * encoder_outputs, dim=1)\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot(Model):\n",
    "    def __init__(self, encoder: EncoderRNN, decoder: LuongAttnDecoderRNN,\n",
    "                 vocab: Vocabulary, sos_token: Token, batch_size: int,\n",
    "                 max_target_len: int, teacher_forcing_ratio):\n",
    "        super().__init__(vocab)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab = vocab\n",
    "        self.sos_token = sos_token\n",
    "        self.max_target_len = max_target_len\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "      \n",
    "    @staticmethod\n",
    "    def mask_nll_loss(inp, target, mask):\n",
    "        n_total = mask.sum()\n",
    "        print(inp.shape)\n",
    "        print(target.shape)\n",
    "        log_likelihood = nn.NLLLoss(reduce=False)(inp.permute(0, 2, 1), target)\n",
    "        loss = log_likelihood.masked_select(mask.byte()).mean()\n",
    "        return loss# , n_total.item()\n",
    "\n",
    "    def forward(self, query: Dict[str, torch.Tensor], reply: Dict[str, torch.Tensor] = None):\n",
    "        if reply is None:\n",
    "            print(query)\n",
    "        \n",
    "        max_target_len = self.max_target_len if reply is None else reply['tokens'].shape[1]\n",
    "        output = {}\n",
    "        encoder_outputs, final_states = encoder(query)\n",
    "        mask = get_text_field_mask(query)\n",
    "        selector = mask.sum(dim=1)-1\n",
    "        batch_size = query['tokens'].shape[0]\n",
    "        decoder_hidden = None #final_states[:decoder.n_layers] # [0,2]] + final_states[[1,3]]\n",
    "        # for i in range(batch_size):\n",
    "        #     decoder_hidden.append(encoder_outputs[i, selector[i]])\n",
    "        # print(decoder_hidden[0].shape)\n",
    "        # decoder_hidden = torch.stack(decoder_hidden, dim=0)\n",
    "        decoder_input = torch.LongTensor([[self.vocab.get_token_index(self.sos_token)] for _ in range(batch_size)])\n",
    "        decoder_input = decoder_input.to(self.device)\n",
    "        # Determine if we are using teacher forcing this iteration\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        ret = []\n",
    "        # Forward batch of sequences through decoder one time step at a time\n",
    "        if use_teacher_forcing and reply is not None:\n",
    "            loss = 0\n",
    "            for t in range(max_target_len):\n",
    "                decoder_output, decoder_hidden = self.decoder(\n",
    "                    decoder_input, encoder_outputs, decoder_hidden)\n",
    "                # Teacher forcing: next input is current target\n",
    "                decoder_input = reply['tokens'][:, t].unsqueeze(1)\n",
    "                ret.append(decoder_output)# torch.LongTensor([topi[i][0] for i in range(batch_size)]))\n",
    "                # Calculate and accumulate loss\n",
    "        else:\n",
    "            for t in range(max_target_len):\n",
    "                decoder_output, decoder_hidden = self.decoder(\n",
    "                    decoder_input, encoder_outputs, decoder_hidden)\n",
    "                # print(decoder_hidden.shape, encoder_outputs.shape, decoder_input.shape)\n",
    "                # No teacher forcing: next input is decoder's own current output\n",
    "                # _, topi = decoder_output.topk(1)\n",
    "                decoder_input = decoder_output.argmax(dim=1).unsqueeze(1)\n",
    "                # print(decoder_output.shape, decoder_input.shape)\n",
    "                # print(decoder_input)\n",
    "                # decoder_input = torch.LongTensor([[topi[i][0]] for i in range(batch_size)])\n",
    "                # print(decoder_output.shape, decoder_input.shape)\n",
    "                # print(decoder_input)\n",
    "                ret.append(decoder_output)\n",
    "                decoder_input = decoder_input.to(self.device)\n",
    "        ret = torch.stack(ret, dim=1)\n",
    "        # print(ret)\n",
    "        # print(reply['tokens'])\n",
    "        if reply is not None:\n",
    "            # print(ret.shape, reply['tokens'].shape)\n",
    "            mask = get_text_field_mask(reply)\n",
    "            # mask = mask.float()*((reply['tokens'] == self.vocab.get_token_index('#EOS#') |\n",
    "            #                     (reply['tokens'] == self.vocab.get_token_index('.')).float() * -.75 + 1)\n",
    "            loss = sequence_cross_entropy_with_logits(ret, reply['tokens'], mask)\n",
    "            output[\"loss\"] = loss\n",
    "            # print(f'Averaged loss {loss}')\n",
    "            #mask = mask.float()        \n",
    "            #loss = sequence_cross_entropy_with_logits(ret, reply['tokens'], mask, average=None)\n",
    "            #output[\"loss\"] = (loss*mask.sum(1) / mask.sum(1).mean()).mean()\n",
    "            # print(f'Calculated loss {output[\"loss\"]}')\n",
    "        output[\"reply\"] = ret.argmax(dim=2)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 10\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "device = 'cuda:0'\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=hidden_size)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size,word_embeddings, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, word_embeddings, hidden_size,\n",
    "                              vocab.get_vocab_size('tokens'), decoder_n_layers, dropout)\n",
    "# Use appropriate device\n",
    "print('Models built and ready to go!')\n",
    "model = Chatbot(encoder, decoder, vocab, sos_token, batch_size, MAX_LENGTH, teacher_forcing_ratio)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam([\n",
    "                {'params': model.encoder.parameters()},\n",
    "                {'params': [param for name, param in model.decoder.named_parameters() \n",
    "                            if 'embedding' not in name], 'lr': 5*learning_rate}\n",
    "                        ], lr=learning_rate)\n",
    "\n",
    "iterator = BucketIterator(batch_size=batch_size, sorting_keys=[(\"query\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "log_path = log_dir + datetime.ctime()\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=dialogue_dataset,\n",
    "                  grad_clipping=clip,\n",
    "                  patience=1,\n",
    "                  num_epochs=n_epochs,\n",
    "                  cuda_device=int(device[-1]),\n",
    "                  serialization_dir = log_path,\n",
    "                  histogram_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "vec = predictor.predict(\"where are you from?\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])\n",
    "vec = predictor.predict(\"what up?\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])\n",
    "vec = predictor.predict(\"I hate you!\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])\n",
    "vec = predictor.predict(\"Are you mad?\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to save the model.\n",
    "with open(\"/tmp/dialogue_bot.th\", 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)\n",
    "vocab.save_to_files(\"/tmp/vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/tmp/dialogue_bot.th\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CartmanReader()\n",
    "reader.vocab = vocab\n",
    "cartman_dataset = reader.read(sp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cartman_dataset[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=cartman_dataset,\n",
    "                  grad_clipping=clip,\n",
    "                  patience=1,\n",
    "                  num_epochs=12,\n",
    "                  cuda_device=int(device[-1]),\n",
    "                  serialization_dir=home+'/data/logs',\n",
    "                  histogram_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to save the model.\n",
    "with open(\"/tmp/cartman_bot.th\", 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "vec = predictor.predict(\"where are you from?\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])\n",
    "vec = predictor.predict(\"what up?\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])\n",
    "vec = predictor.predict(\"Kyle is so nice.\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])\n",
    "vec = predictor.predict(\"Kenny?\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])\n",
    "vec = predictor.predict(\"Aw shit, I missed it!\")['reply']\n",
    "print(len(vec))\n",
    "print([vocab.get_token_from_index(x) for x in vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
