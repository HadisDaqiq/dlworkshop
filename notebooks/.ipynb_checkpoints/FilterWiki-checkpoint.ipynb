{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/data/workshop_data\n",
    "! wget -c --retry-connrefused --tries=0 https://dumps.wikimedia.org/enwiki/20190120/enwiki-20190120-pages-articles-multistream.xml.bz2 -O ~/data/workshop_data/enwiki-20190120-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open\n",
    "from gensim.corpora.wikicorpus import extract_pages, filter_wiki\n",
    "from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from multiprocessing import Lock\n",
    "from os.path import expanduser\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = expanduser(\"~\")\n",
    "in_file = home + '/data/workshop_data/enwiki-20190120-pages-articles-multistream.xml.bz2'\n",
    "out_file = home + '/data/workshop_data/wiki_out.csv'\n",
    "sentence_splitter = SpacySentenceSplitter(rule_based=True)\n",
    "\n",
    "\n",
    "lock = Lock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all URLs and email addresses with \"THISISAURL\" and \"THISISANEMAIL\".\n",
    "    Replace all versions of \"E-Mail\" with \"Email\" since dashes will be replaced by spaces.\n",
    "    Fix misplaced periods to allow for better sentence tokenization.\n",
    "    Remove \"QUOTE\" string.\n",
    "    Replace dashes by spaces.\n",
    "\n",
    "    This method is useful as a first pre-processing before learning a word2vec model.\n",
    "    Follow this step by sentence tokenization and preparing sentences.\n",
    "\n",
    "    :param text: text to be prepared\n",
    "    :return: prepared text\n",
    "    \"\"\"\n",
    "\n",
    "    text_mod = re.sub(r\"e-?mail\", \"Email\", text, flags=re.IGNORECASE)       # replace \"E-Mail\"/\"e-mail\" with \"Email\"\n",
    "    text_mod = re.sub(r\"http\\S+(\\s|$)\", \"THISISAURL \", text_mod)            # remove urls\n",
    "    text_mod = re.sub(r\"www\\.\\S+(\\s|$)\", \"THISISAURL \", text_mod)           # remove urls\n",
    "    text_mod = re.sub(r\"\\b\\S+@\\S+\\b\", \"THISISANEMAIL \", text_mod)           # remove emails\n",
    "    text_mod = re.sub(r\"\\.+\", \".\", text_mod)                               # replace multiple periods with one\n",
    "    text_mod = re.sub(r\"\\s\\.[ ^\\.]\", \". \", text_mod)                        # fix misplaced periods\n",
    "    text_mod = re.sub(r\"(?<!\\d)\\.[ ^\\.]\", \". \", text_mod)                   # fix misplaced periods\n",
    "    text_mod = re.sub(r\"\\-+\", \" \", text_mod)                                # replace \"-\" with a space\n",
    "    text_mod = re.sub(r\":(?=\\S)\", \": \", text_mod)                      # add space after colon if there was none before\n",
    "    # text_mod = re.sub(r\"(?<=[^\\w\\s])(?=\\S)\", \" \", text_mod)          # add space after any remaining special char\n",
    "    # text_mod = re.sub(r\"QUOTE\", \"\", text_mod)     # remove QUOTE -- causes issues because some texts will be empty\n",
    "\n",
    "    # if len(text_mod) < 10:\n",
    "    #     logger.debug(\"len(text_mod) < 10:\")\n",
    "    #     logger.debug(text_mod)\n",
    "    #     logger.debug(\"original:\")\n",
    "    #     logger.debug(text)\n",
    "\n",
    "    return text_mod\n",
    "\n",
    "\n",
    "def prepare_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Remove all punctuation.\n",
    "    Replace any white space with single space.\n",
    "    Remove excess white space at beginning or end of sentence.\n",
    "\n",
    "    This method is useful as a second pre-processing before learning a word2vec model.\n",
    "    This step should be preceded by prepare_text() and sentence_tokenize().\n",
    "\n",
    "    :param sentence: sentence to be prepared\n",
    "    :return: prepared sentence\n",
    "    \"\"\"\n",
    "\n",
    "    sent_mod = re.sub(r\"_+\", \" \", sentence)             # replace underscores with single white space\n",
    "    sent_mod = re.sub(r\"'+\", \"\", sent_mod)              # remove '\n",
    "    sent_mod = re.sub(r'\"+', \"\", sent_mod)               # remove \"\n",
    "    sent_mod = re.sub(r\"{+\", \"\", sent_mod)             # remove {\n",
    "    sent_mod = re.sub(r\"}+\", \"\", sent_mod)             # remove }\n",
    "    sent_mod = re.sub(r\"\\[+\", \"\", sent_mod)             # remove [\n",
    "    sent_mod = re.sub(r\"\\]+\", \"\", sent_mod)             # remove ]\n",
    "    sent_mod = re.sub(r\"\\|+\", \" \", sent_mod)             # replace | with single white space\n",
    "    sent_mod = re.sub(r\"\\*+\", \"\", sent_mod)             # remove *\n",
    "    sent_mod = re.sub(r\"=+\", \" \", sent_mod)             # replace equal with single white space\n",
    "    sent_mod = re.sub(r\";+\", \",\", sent_mod)             # replace semicolon with single white space\n",
    "    sent_mod = re.sub(r\"\\s+\", \" \", sent_mod)            # replace any whitespace with single space\n",
    "    sent_mod = re.sub(r\",+\", \"\", sent_mod)              # remove comma\n",
    "    sent_mod = re.sub(r\".+\", \"\", sent_mod)              # remove point\n",
    "    sent_mod = re.sub(r\"\\!+\", \"\", sent_mod)             # remove exclemation mark\n",
    "    sent_mod = re.sub(r\"\\?+\", \"\", sent_mod)             # remove question mark\n",
    "    sent_mod = re.sub(r\"\\0+\", \"\", sent_mod)             # remove null byte\n",
    "    sent_mod = sent_mod.strip()                         # remove any white space at beginning or end of sentence\n",
    "\n",
    "    # if len(sent_mod) < 10:\n",
    "    #     logger.debug(\"len(sent_mod) < 10: {}\".format(len(sent_mod)))\n",
    "    #     logger.debug(sent_mod)\n",
    "    #     logger.debug(\"original:\")\n",
    "    #     logger.debug(sentence)\n",
    "\n",
    "    return sent_mod\n",
    "\n",
    "\n",
    "def filter_article(x):\n",
    "    title, text, page_id = x\n",
    "    sentences = sentence_splitter.split_sentences(prepare_text(filter_wiki(text)))\n",
    "    out_str = ''\n",
    "    sentences = [prepare_sentence(sent) for sent in sentences]\n",
    "    out_str = ''\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if not (word.startswith('{') or word.startswith('[') or word.startswith('File:')):\n",
    "                out_str += word + ';'\n",
    "        out_str += '\\n'\n",
    "    with lock:\n",
    "        with open(out_file, 'a') as f:\n",
    "            f.write(out_str)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(out_file):\n",
    "    os.remove(out_file)\n",
    "with open(out_file, 'a') as f:\n",
    "    f.write('sentence\\n')\n",
    "pool = Pool()\n",
    "pool.imap(filter_article, tqdm(extract_pages(smart_open(in_file))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences in tqdm(extract_pages(smart_open(in_file))):\n",
    "    sentences = filter_article(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
