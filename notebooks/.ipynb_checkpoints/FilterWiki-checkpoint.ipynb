{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open\n",
    "from gensim.corpora.wikicorpus import extract_pages, filter_wiki\n",
    "from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = '/home/jendrik/data/enwiki-20190120-pages-articles-multistream.xml.bz2'\n",
    "out_file = '/home/jendrik/data/wiki_out.csv'\n",
    "sentence_splitter = SpacySentenceSplitter(rule_based=True)\n",
    "\n",
    "\n",
    "lock = Lock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all URLs and email addresses with \"THISISAURL\" and \"THISISANEMAIL\".\n",
    "    Replace all versions of \"E-Mail\" with \"Email\" since dashes will be replaced by spaces.\n",
    "    Fix misplaced periods to allow for better sentence tokenization.\n",
    "    Remove \"QUOTE\" string.\n",
    "    Replace dashes by spaces.\n",
    "\n",
    "    This method is useful as a first pre-processing before learning a word2vec model.\n",
    "    Follow this step by sentence tokenization and preparing sentences.\n",
    "\n",
    "    :param text: text to be prepared\n",
    "    :return: prepared text\n",
    "    \"\"\"\n",
    "\n",
    "    text_mod = re.sub(r\"e-?mail\", \"Email\", text, flags=re.IGNORECASE)       # replace \"E-Mail\"/\"e-mail\" with \"Email\"\n",
    "    text_mod = re.sub(r\"http\\S+(\\s|$)\", \"THISISAURL \", text_mod)            # remove urls\n",
    "    text_mod = re.sub(r\"www\\.\\S+(\\s|$)\", \"THISISAURL \", text_mod)           # remove urls\n",
    "    text_mod = re.sub(r\"\\b\\S+@\\S+\\b\", \"THISISANEMAIL \", text_mod)           # remove emails\n",
    "    text_mod = re.sub(r\"\\.+\", \".\", text_mod)                               # replace multiple periods with one\n",
    "    text_mod = re.sub(r\"\\s\\.[ ^\\.]\", \". \", text_mod)                        # fix misplaced periods\n",
    "    text_mod = re.sub(r\"(?<!\\d)\\.[ ^\\.]\", \". \", text_mod)                   # fix misplaced periods\n",
    "    text_mod = re.sub(r\"\\-+\", \" \", text_mod)                                # replace \"-\" with a space\n",
    "    text_mod = re.sub(r\":(?=\\S)\", \": \", text_mod)                      # add space after colon if there was none before\n",
    "    # text_mod = re.sub(r\"(?<=[^\\w\\s])(?=\\S)\", \" \", text_mod)          # add space after any remaining special char\n",
    "    # text_mod = re.sub(r\"QUOTE\", \"\", text_mod)     # remove QUOTE -- causes issues because some texts will be empty\n",
    "\n",
    "    # if len(text_mod) < 10:\n",
    "    #     logger.debug(\"len(text_mod) < 10:\")\n",
    "    #     logger.debug(text_mod)\n",
    "    #     logger.debug(\"original:\")\n",
    "    #     logger.debug(text)\n",
    "\n",
    "    return text_mod\n",
    "\n",
    "def filter_article(x):\n",
    "    title, text, page_id = x\n",
    "    sentences = sentence_splitter.split_sentences(prepare_text(filter_wiki(text)))\n",
    "    print(len(sentences))\n",
    "    out_str = ''\n",
    "    for sentence in sentences:\n",
    "        out_str += sentence + '\\n'\n",
    "    print(len(out_str))\n",
    "    with lock:\n",
    "        with open(out_file, 'w+') as f:\n",
    "            f.write(out_str)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool()\n",
    "pool.imap(filter_article, tqdm(extract_pages(smart_open(in_file))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "with open(out_file, 'w') as f:\n",
    "    f.write('sentence\\n')\n",
    "    for title, text, page_id in tqdm(extract_pages(smart_open(in_file))):\n",
    "        buffer.append(filter_wiki(text))\n",
    "        if len(buffer) > 1023:\n",
    "            articles = sentence_splitter.batch_split_sentences(buffer)\n",
    "            for art in articles:\n",
    "                out_str = ''\n",
    "                for sentence in art:\n",
    "                    out_str += sentence + '\\n'\n",
    "                f.write(out_str)\n",
    "            buffer = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
