{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1d2563a950>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import multiprocessing as mp\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import functools\n",
    "from typing import Dict, List, Iterator, Union, Callable\n",
    "# In AllenNLP each training example is represented as an Instance containing Fields\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.models import Model\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.data.tokenizers import WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n",
    "\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset():\n",
    "\n",
    "    def __init__(self, data_series: List[pd.Series], csvs: List[str], \n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, \n",
    "                 tokenizer: WordTokenizer = SpacyWordSplitter(),\n",
    "                 sentence_splitter: SpacySentenceSplitter = SpacySentenceSplitter(rule_based=True),\n",
    "                 context_size: int = 1) -> None:\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentence_splitter = sentence_splitter\n",
    "        self.context_size = context_size\n",
    "        self.data_series = data_series\n",
    "        self.csvs = csvs\n",
    "        self.queue = mp.Queue(100000)\n",
    "        # self.sentences = []\n",
    "        # self.next_sentence_ids = []\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_instance(tokens: List[Token], token_indexers, context: List[Token] = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "        \n",
    "        if context:\n",
    "            label_field = TextField(context, token_indexers)\n",
    "            fields[\"context\"] = label_field\n",
    "        \n",
    "        return Instance(fields)\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Instance]:\n",
    "        select_context_part = functools.partial(self.select_context_from_sentence, \n",
    "                                                context_size = self.context_size)\n",
    "        instances = self.iterate_data(select_context_part, True)\n",
    "        if isinstance(instances, list):\n",
    "            raise ConfigurationError(\"For a lazy dataset reader, word_context() must return a generator\")\n",
    "        return instances\n",
    "         \n",
    "    @staticmethod       \n",
    "    def select_context_from_sentence(sentence: List[Token], context_size: int):\n",
    "        j = random.randint(0, len(sentence))\n",
    "        context1 = [word for word in sentence[j-context_size:j]]\n",
    "        while len(context1) < context_size:\n",
    "            context1.insert(0, Token())\n",
    "        context2 = [word for word in sentence[j+1:j+context_size+1]]\n",
    "        while len(context1) < context_size:\n",
    "            context2.append(Token())\n",
    "        context1.extend(context2)\n",
    "        return [sentence[j]], context1\n",
    "\n",
    "    def it_from_ser(self) -> Iterator[Instance]:\n",
    "        instances = self.iterate_data(None, False)\n",
    "        if isinstance(instances, list):\n",
    "            raise ConfigurationError(\"For a lazy dataset reader, word_context() must return a generator\")\n",
    "            \n",
    "        return instances\n",
    "    \n",
    "    def process_chunk(self, sentences: List[str], apply_to_sentence: Callable = None, \n",
    "                      split_sentences: bool = True):\n",
    "        if split_sentences:\n",
    "            sentences = self.sentence_splitter.batch_split_sentences(sentences)\n",
    "            ret_list = []\n",
    "            for sent in sentences:\n",
    "                ret_list.extend(sent)\n",
    "            sentences = ret_list\n",
    "        ret_list = self.tokenizer.batch_split_words(sentences)\n",
    "        return ret_list\n",
    "                               \n",
    "    \n",
    "    def iterate_data(self, apply_to_sentence: Callable,\n",
    "                     split_sentences: bool = True) -> Iterator[Union[Instance, Instance]]:\n",
    "        next_sentence_id = 0\n",
    "        for ser in self.data_series:\n",
    "            sentences = ser[pd.notnull(ser)].values\n",
    "            sentence_word_list = self.process_chunk(sentences, apply_to_sentence, split_sentences)\n",
    "            for sentence in sentence_word_list:\n",
    "                if len(sentence) < 3:\n",
    "                    continue\n",
    "                if apply_to_sentence is not None:\n",
    "                    sentence, context = apply_to_sentence(sentence)\n",
    "                    yield SentenceDataset.text_to_instance(sentence, self.token_indexers, context)\n",
    "                else:\n",
    "                    yield SentenceDataset.text_to_instance(sentence, self.token_indexers)\n",
    "        for f in self.csvs:\n",
    "            reader = pd.read_csv(f, chunksize=int(1e4), dtype=str)\n",
    "            for chunk in reader:\n",
    "                sentences = chunk['sentence']\n",
    "                sentences = sentences[pd.notnull(sentences)].values\n",
    "                sentence_word_list = self.process_chunk(sentences, apply_to_sentence, False)\n",
    "                for sentence in sentence_word_list:\n",
    "                    if len(sentence) < 3:\n",
    "                        continue\n",
    "                    if apply_to_sentence is not None:\n",
    "                        sentence, context = apply_to_sentence(sentence)\n",
    "                        yield SentenceDataset.text_to_instance(sentence, self.token_indexers, context)\n",
    "                    else:\n",
    "                        yield SentenceDataset.text_to_instance(sentence, self.token_indexers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramLanguageModelerSparse(Model):\n",
    "\n",
    "    def __init__(self, vocab: Vocabulary, embedding_dim, context_size, \n",
    "                 embedding_freq, negative_sampling_size):\n",
    "        super().__init__(vocab)\n",
    "        vocab_size = vocab.get_vocab_size('tokens')\n",
    "        print(vocab_size)\n",
    "        self.embedding_freq = torch.autograd.Variable(torch.Tensor(embedding_freq)**(0.75))\n",
    "        self.negative_sampling_size = negative_sampling_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last_loss = None\n",
    "\n",
    "    def forward(self, sentence, context=None):\n",
    "        \n",
    "        word = sentence['tokens']\n",
    "        \n",
    "        # Allen NLP demands a dictionary to be returned\n",
    "        # It has to contain the key loss for training purposes\n",
    "        output = {}\n",
    "        word_emb = self.word_embedding(word)\n",
    "        output['vec'] = word_emb + self.context_embedding(word)\n",
    "\n",
    "        if context is None:\n",
    "            #return embedding\n",
    "            return output\n",
    "        \n",
    "        context = context['tokens']\n",
    "        \n",
    "        context = self.context_embedding(context)\n",
    "        word_pos = word_emb.expand_as(context)\n",
    "        \n",
    "        product_pos = (word_pos*context).sum(dim=-1).mean()\n",
    "        target_pos = torch.autograd.Variable(torch.ones(product_pos.size()))\n",
    "        loss_positive = nn.functional.binary_cross_entropy_with_logits(product_pos, target_pos)\n",
    "        negative_context = self.embedding_freq.multinomial(\n",
    "            self.negative_sampling_size*context.size(0)*context.size(1), replacement=True).to(self.device())\n",
    "        negative_context = negative_context.view(-1,self.negative_sampling_size*context.size(1))\n",
    "        negative_context = self.context_embedding(negative_context)\n",
    "        word_neg = word_emb.expand_as(negative_context)\n",
    "        product_neg = (word_neg*negative_context).sum(dim=-1).mean()\n",
    "        target_neg = torch.autograd.Variable(torch.zeros(product_neg.size()))\n",
    "        loss_negative = nn.functional.binary_cross_entropy_with_logits(product_neg,target_neg)\n",
    "        self.last_loss = loss_positive + loss_negative\n",
    "        output['loss'] = loss_positive + loss_negative\n",
    "        output['vec'] = word_pos + context\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"bce\": self.last_loss}\n",
    "    \n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def log_probs(self, inp, out, neg):\n",
    "        losses = []\n",
    "        inp = self.embeddings(inp)\n",
    "        out = self.embeddings(out)\n",
    "        neg = self.embeddings(neg)\n",
    "        score = torch.mul(inp, out)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = nn.functional.logsigmoid(score)\n",
    "        neg_score = torch.bmm(neg, inp.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(neg_score, dim=0)\n",
    "        neg_score = nn.functional.logsigmoid(-1 * neg_score)\n",
    "        losses.append(sum(neg_score))\n",
    "        losses.append(sum(score))\n",
    "        return -1 * sum(losses)\n",
    "    \n",
    "class SkipGramLanguageModelerSparseNoNeg(Model):\n",
    "\n",
    "    def __init__(self, vocab: Vocabulary, embedding_dim, context_size):\n",
    "        super().__init__(vocab)\n",
    "        vocab_size = vocab.get_vocab_size('tokens')\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last_loss = None\n",
    "\n",
    "    def forward(self, sentence, context=None):\n",
    "        \n",
    "        word = sentence['tokens']\n",
    "        \n",
    "        # Allen NLP demands a dictionary to be returned\n",
    "        # It has to contain the key loss for training purposes\n",
    "        output = {}\n",
    "        word_emb = self.word_embedding(word)\n",
    "        output['vec'] = word_emb + self.context_embedding(word)\n",
    "\n",
    "        if context is None:\n",
    "            #return embedding\n",
    "            return output\n",
    "        \n",
    "        context = context['tokens']\n",
    "        \n",
    "        context = self.context_embedding(context)\n",
    "        word_pos = word_emb.expand_as(context)\n",
    "        \n",
    "        product_pos = (word_pos*context).sum(dim=-1).mean()\n",
    "        target_pos = torch.autograd.Variable(torch.ones(product_pos.size()))\n",
    "        loss_positive = nn.functional.binary_cross_entropy_with_logits(product_pos, target_pos)\n",
    "        self.last_loss = loss_positive\n",
    "        output['loss'] = loss_positive\n",
    "        output['vec'] = word_pos + context\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"bce\": self.last_loss}\n",
    "    \n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def log_probs(self, inp, out, neg):\n",
    "        losses = []\n",
    "        inp = self.embeddings(inp)\n",
    "        out = self.embeddings(out)\n",
    "        neg = self.embeddings(neg)\n",
    "        score = torch.mul(inp, out)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = nn.functional.logsigmoid(score)\n",
    "        neg_score = torch.bmm(neg, inp.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(neg_score, dim=0)\n",
    "        neg_score = nn.functional.logsigmoid(-1 * neg_score)\n",
    "        losses.append(sum(neg_score))\n",
    "        losses.append(sum(score))\n",
    "        return -1 * sum(losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jendrik/git/DLWorkshop/notebooks/southpark_full.csv'\n",
    "corpus = pd.read_csv(path, sep='\\t', dtype=str)\n",
    "wiki_path = '/home/jendrik/data/enwiki_2018_09_25.csv'\n",
    "ds = SentenceDataset([corpus['spoken']], [wiki_path])\n",
    "train_dataset = ds.it_from_ser()\n",
    "word2vec_dataset = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset, min_count={'': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save_to_files('vocab.store')\n",
    "with open('vocab.dict', 'w') as f:\n",
    "    for k, v in vocab._retained_counter['tokens'].items():\n",
    "        f.write(f'{k},{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "97151\n",
      "97153\n"
     ]
    }
   ],
   "source": [
    "add_dict = {}\n",
    "with open('vocab.dict', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    # line = f.readline()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            comma_index = line.rfind(',')\n",
    "            token = line[:comma_index]\n",
    "            if ('|' in token or '/' in token \n",
    "                or len(token) == 1 or 'style=\"' in token or '<' in token\n",
    "               or '!colspan=' in token):\n",
    "                continue\n",
    "            count = int(line[comma_index+1:])\n",
    "            if count < 500:\n",
    "                continue\n",
    "            add_dict[token] = count\n",
    "            # line = f.readline()\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "            \n",
    "    \n",
    "vocab_new = Vocabulary(counter={'tokens': add_dict}, min_count={'tokens': 10}) \n",
    "print(len(list(add_dict.values())))\n",
    "print(vocab_new.get_vocab_size('tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Vocabulary Statistics----\n",
      "\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'tokens':\n",
      "\tToken: the\t\tFrequency: 129471660\n",
      "\tToken: of\t\tFrequency: 71053610\n",
      "\tToken: and\t\tFrequency: 60957403\n",
      "\tToken: in\t\tFrequency: 53374979\n",
      "\tToken: to\t\tFrequency: 43053589\n",
      "\tToken: was\t\tFrequency: 26424022\n",
      "\tToken: is\t\tFrequency: 20789341\n",
      "\tToken: The\t\tFrequency: 19647930\n",
      "\tToken: for\t\tFrequency: 17131870\n",
      "\tToken: as\t\tFrequency: 16128262\n",
      "\n",
      "Top 10 longest tokens in namespace 'tokens':\n",
      "\tToken: Verwaltungsgemeinschaft\t\tlength: 23\tFrequency: 751\n",
      "\tToken: ProcellariiformesFamily\t\tlength: 23\tFrequency: 678\n",
      "\tToken: PricewaterhouseCoopers\t\tlength: 22\tFrequency: 824\n",
      "\tToken: CharadriiformesFamily\t\tlength: 21\tFrequency: 2596\n",
      "\tToken: electionParliamentary\t\tlength: 21\tFrequency: 1309\n",
      "\tToken: AccipitriformesFamily\t\tlength: 21\tFrequency: 660\n",
      "\tToken: PelecaniformesFamily\t\tlength: 20\tFrequency: 953\n",
      "\tToken: internationalization\t\tlength: 20\tFrequency: 935\n",
      "\tToken: uncharacteristically\t\tlength: 20\tFrequency: 876\n",
      "\tToken: institutionalization\t\tlength: 20\tFrequency: 705\n",
      "\n",
      "Top 10 shortest tokens in namespace 'tokens':\n",
      "\tToken: MZ\t\tlength: 2\tFrequency: 500\n",
      "\tToken: ff\t\tlength: 2\tFrequency: 500\n",
      "\tToken: yn\t\tlength: 2\tFrequency: 501\n",
      "\tToken: 1d\t\tlength: 2\tFrequency: 501\n",
      "\tToken: GJ\t\tlength: 2\tFrequency: 501\n",
      "\tToken: là\t\tlength: 2\tFrequency: 502\n",
      "\tToken: mt\t\tlength: 2\tFrequency: 505\n",
      "\tToken: vz\t\tlength: 2\tFrequency: 506\n",
      "\tToken: E5\t\tlength: 2\tFrequency: 506\n",
      "\tToken: L7\t\tlength: 2\tFrequency: 506\n"
     ]
    }
   ],
   "source": [
    "vocab_new.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:vocabulary serialization directory vocab_new.store is not empty\n"
     ]
    }
   ],
   "source": [
    "vocab_new.save_to_files('vocab_new.store')\n",
    "with open('vocab_new.dict', 'w') as f:\n",
    "    for k, v in add_dict.items():\n",
    "        f.write(f'{k},{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vocab_new = Vocabulary.from_files('vocab_new.store')\n",
    "add_dict = {}\n",
    "with open('vocab_new.dict', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    # line = f.readline()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            comma_index = line.rfind(',')\n",
    "            token = line[:comma_index]\n",
    "            count = int(line[comma_index+1:])\n",
    "            add_dict[token] = count\n",
    "            # line = f.readline()\n",
    "        except ValueError:\n",
    "            print(line)\n",
    "vocab_new = Vocabulary(counter={'tokens': add_dict}, min_count={'tokens': 10}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97151\n",
      "97153\n"
     ]
    }
   ],
   "source": [
    "print(len(add_dict.values()))\n",
    "print(vocab_new.get_vocab_size('tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkipGramLanguageModelerSparse(\n",
       "  (word_embedding): Embedding(97153, 128, sparse=True)\n",
       "  (context_embedding): Embedding(97153, 128, sparse=True)\n",
       "  (linear): Linear(in_features=128, out_features=97153, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dims = 128\n",
    "batch_size = 256\n",
    "context_size = 3\n",
    "n_batches = 64#len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "neg_samples = 10\n",
    "dict_values = list(add_dict.values())\n",
    "dict_values.insert(0, 0)\n",
    "dict_values.insert(0, 0)\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipGramLanguageModelerSparse(\n",
    "    vocab_new, embedding_dims, context_size, \n",
    "    dict_values, \n",
    "    neg_samples)\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=1e-4)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97153, 128])\n",
      "torch.Size([97153, 128])\n",
      "torch.Size([97153, 128])\n",
      "torch.Size([97153])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "bce: 0.8895, loss: 0.8895 ||: 100%|██████████| 1/1 [00:21<00:00, 21.35s/it]\u001b[A\n",
      "bce: 0.8836, loss: 1.3152 ||: : 13it [00:21, 14.95s/it]                    \u001b[A\n",
      "bce: 1.2716, loss: 1.1761 ||: : 24it [00:21, 10.47s/it]\u001b[A\n",
      "bce: 0.9747, loss: 1.1275 ||: : 36it [00:21,  7.33s/it]\u001b[A\n",
      "bce: 0.9131, loss: 1.0851 ||: : 47it [00:21,  5.13s/it]\u001b[A\n",
      "bce: 0.3580, loss: 1.0735 ||: : 59it [00:21,  3.60s/it]\u001b[A\n",
      "bce: 0.8715, loss: 1.0573 ||: : 71it [00:21,  2.52s/it]\u001b[A\n",
      "bce: 0.6480, loss: 1.0160 ||: : 82it [00:22,  1.77s/it]\u001b[A\n",
      "bce: 0.6376, loss: 0.9907 ||: : 93it [00:22,  1.24s/it]\u001b[A\n",
      "bce: 0.6708, loss: 0.9860 ||: : 104it [00:22,  1.15it/s]\u001b[A\n",
      "bce: 0.9630, loss: 0.9596 ||: : 116it [00:22,  1.63it/s]\u001b[A\n",
      "bce: 0.8554, loss: 0.9684 ||: : 127it [00:22,  2.32it/s]\u001b[A\n",
      "bce: 0.4161, loss: 0.9371 ||: : 139it [00:22,  3.29it/s]\u001b[A\n",
      "bce: 0.6441, loss: 0.9169 ||: : 151it [00:22,  4.64it/s]\u001b[A\n",
      "bce: 0.8480, loss: 0.8895 ||: : 163it [00:22,  6.51it/s]\u001b[A\n",
      "bce: 0.2427, loss: 0.8809 ||: : 175it [00:22,  9.07it/s]\u001b[A\n",
      "bce: 0.6974, loss: 0.8621 ||: : 187it [00:23, 12.51it/s]\u001b[A\n",
      "bce: 0.8312, loss: 0.8397 ||: : 199it [00:23, 17.07it/s]\u001b[A\n",
      "bce: 0.4107, loss: 0.8162 ||: : 211it [00:23, 22.74it/s]\u001b[A\n",
      "bce: 0.4009, loss: 0.8030 ||: : 222it [00:23, 29.79it/s]\u001b[A\n",
      "bce: 0.5762, loss: 0.7893 ||: : 233it [00:23, 38.13it/s]\u001b[A\n",
      "bce: 0.5229, loss: 0.7765 ||: : 245it [00:23, 47.62it/s]\u001b[A\n",
      "bce: 0.3725, loss: 0.7677 ||: : 257it [00:23, 57.36it/s]\u001b[A\n",
      "bce: 0.5049, loss: 0.7573 ||: : 269it [00:23, 66.98it/s]\u001b[A\n",
      "bce: 0.4299, loss: 0.7464 ||: : 281it [00:23, 76.39it/s]\u001b[A\n",
      "bce: 0.3799, loss: 0.7317 ||: : 293it [00:23, 84.21it/s]\u001b[A\n",
      "bce: 0.6270, loss: 0.7214 ||: : 305it [00:24, 88.90it/s]\u001b[A\n",
      "bce: 0.6173, loss: 0.7095 ||: : 317it [00:24, 95.62it/s]\u001b[A\n",
      "bce: 0.3872, loss: 0.7063 ||: : 329it [00:24, 100.26it/s]\u001b[A\n",
      "bce: 0.5792, loss: 0.7036 ||: : 341it [00:24, 103.44it/s]\u001b[A\n",
      "bce: 0.1470, loss: 0.6935 ||: : 353it [00:24, 106.36it/s]\u001b[A\n",
      "bce: 0.3021, loss: 0.6810 ||: : 365it [00:24, 106.58it/s]\u001b[A\n",
      "bce: 0.9372, loss: 0.6740 ||: : 377it [00:24, 108.22it/s]\u001b[A\n",
      "bce: 0.3727, loss: 0.6640 ||: : 389it [00:24, 108.31it/s]\u001b[A\n",
      "bce: 0.2508, loss: 0.6566 ||: : 401it [00:24, 109.32it/s]\u001b[A\n",
      "bce: 0.5575, loss: 0.6472 ||: : 413it [00:25, 109.46it/s]\u001b[A\n",
      "bce: 0.3107, loss: 0.6383 ||: : 425it [00:25, 110.62it/s]\u001b[A\n",
      "bce: 0.2504, loss: 0.6307 ||: : 437it [00:25, 109.88it/s]\u001b[A\n",
      "bce: 0.4757, loss: 0.6226 ||: : 449it [00:25, 107.93it/s]\u001b[A\n",
      "bce: 0.2009, loss: 0.6158 ||: : 461it [00:25, 109.26it/s]\u001b[A\n",
      "bce: 0.4561, loss: 0.6079 ||: : 472it [00:25, 108.40it/s]\u001b[A\n",
      "bce: 0.4909, loss: 0.6006 ||: : 483it [00:25, 108.71it/s]\u001b[A\n",
      "bce: 0.1797, loss: 0.5946 ||: : 494it [00:25, 108.33it/s]\u001b[A\n",
      "bce: 0.0559, loss: 0.5873 ||: : 505it [00:25, 104.54it/s]\u001b[A\n",
      "bce: 0.1644, loss: 0.5803 ||: : 517it [00:26, 107.10it/s]\u001b[A\n",
      "bce: 0.2978, loss: 0.5744 ||: : 529it [00:26, 108.70it/s]\u001b[A\n",
      "bce: 0.1763, loss: 0.5672 ||: : 540it [00:26, 108.21it/s]\u001b[A\n",
      "bce: 0.2186, loss: 0.5594 ||: : 552it [00:26, 110.82it/s]\u001b[A\n",
      "bce: 0.3932, loss: 0.5532 ||: : 564it [00:26, 111.19it/s]\u001b[A\n",
      "bce: 0.2168, loss: 0.5481 ||: : 576it [00:26, 112.00it/s]\u001b[A\n",
      "bce: 0.1182, loss: 0.5417 ||: : 588it [00:26, 110.88it/s]\u001b[A\n",
      "bce: 0.3048, loss: 0.5389 ||: : 600it [00:26, 108.85it/s]\u001b[A\n",
      "bce: 0.2213, loss: 0.5321 ||: : 611it [00:26, 105.54it/s]\u001b[A\n",
      "bce: 0.1562, loss: 0.5260 ||: : 622it [00:27, 104.91it/s]\u001b[A\n",
      "bce: 0.2518, loss: 0.5204 ||: : 633it [00:27, 106.13it/s]\u001b[A\n",
      "bce: 0.3279, loss: 0.5171 ||: : 644it [00:27, 104.55it/s]\u001b[A\n",
      "bce: 0.1360, loss: 0.5101 ||: : 655it [00:27, 103.67it/s]\u001b[A\n",
      "bce: 0.9825, loss: 0.5060 ||: : 666it [00:27, 100.47it/s]\u001b[A\n",
      "bce: 0.0628, loss: 0.5054 ||: : 677it [00:27, 96.28it/s] \u001b[A\n",
      "bce: 0.1676, loss: 0.5003 ||: : 688it [00:27, 98.29it/s]\u001b[A\n",
      "bce: 0.9403, loss: 0.4988 ||: : 698it [00:27, 98.44it/s]\u001b[A\n",
      "bce: 0.5304, loss: 0.4946 ||: : 709it [00:27, 100.68it/s]\u001b[A\n",
      "bce: 0.4581, loss: 0.4898 ||: : 720it [00:28, 97.81it/s] \u001b[A\n",
      "bce: 0.1023, loss: 0.4859 ||: : 731it [00:28, 100.19it/s]\u001b[A\n",
      "bce: 0.1596, loss: 0.4819 ||: : 742it [00:28, 96.68it/s] \u001b[A\n",
      "bce: 0.1221, loss: 0.4766 ||: : 754it [00:28, 99.56it/s]\u001b[A\n",
      "bce: 0.1253, loss: 0.4737 ||: : 765it [00:28, 94.52it/s]\u001b[A\n",
      "bce: 0.1094, loss: 0.4692 ||: : 777it [00:28, 99.10it/s]\u001b[A\n",
      "bce: 0.6358, loss: 0.4659 ||: : 788it [00:28, 95.20it/s]\u001b[A\n",
      "bce: 0.1871, loss: 0.4614 ||: : 798it [00:28, 92.35it/s]\u001b[A\n",
      "bce: 0.0639, loss: 0.4577 ||: : 808it [00:28, 92.32it/s]\u001b[A\n",
      "bce: 0.3725, loss: 0.4541 ||: : 818it [00:29, 92.55it/s]\u001b[A\n",
      "bce: 0.0787, loss: 0.4510 ||: : 828it [00:29, 92.19it/s]\u001b[A\n",
      "bce: 0.1862, loss: 0.4475 ||: : 838it [00:29, 90.08it/s]\u001b[A\n",
      "bce: 0.1074, loss: 0.4435 ||: : 848it [00:29, 88.31it/s]\u001b[A\n",
      "bce: 0.4829, loss: 0.4402 ||: : 858it [00:29, 90.45it/s]\u001b[A\n",
      "bce: 0.1624, loss: 0.4367 ||: : 869it [00:29, 95.10it/s]\u001b[A\n",
      "bce: 0.3157, loss: 0.4332 ||: : 879it [00:29, 96.45it/s]\u001b[A\n",
      "bce: 0.0296, loss: 0.4310 ||: : 889it [00:29, 91.48it/s]\u001b[A\n",
      "bce: 0.1776, loss: 0.4280 ||: : 899it [00:29, 87.14it/s]\u001b[A\n",
      "bce: 0.0521, loss: 0.4252 ||: : 908it [00:30, 86.71it/s]\u001b[A\n",
      "bce: 0.0743, loss: 0.4213 ||: : 920it [00:30, 93.93it/s]\u001b[A\n",
      "bce: 0.0233, loss: 0.4176 ||: : 933it [00:30, 100.36it/s]\u001b[A\n",
      "bce: 0.4234, loss: 0.4154 ||: : 944it [00:30, 103.03it/s]\u001b[A\n",
      "bce: 0.1221, loss: 0.4119 ||: : 956it [00:30, 105.36it/s]\u001b[A\n",
      "bce: 0.0153, loss: 0.4085 ||: : 969it [00:30, 109.10it/s]\u001b[A\n",
      "bce: 0.3544, loss: 0.4051 ||: : 981it [00:30, 107.22it/s]\u001b[A\n",
      "bce: 0.0954, loss: 0.4020 ||: : 992it [00:30, 103.17it/s]\u001b[A\n",
      "bce: 0.6861, loss: 0.3998 ||: : 1003it [00:30, 104.88it/s]\u001b[A\n",
      "bce: 0.0504, loss: 0.3971 ||: : 1014it [00:31, 105.89it/s]\u001b[A\n",
      "bce: 0.4627, loss: 0.3942 ||: : 1025it [00:31, 102.79it/s]\u001b[A\n",
      "bce: 0.0876, loss: 0.3914 ||: : 1036it [00:31, 103.48it/s]\u001b[A\n",
      "bce: 0.4462, loss: 0.3892 ||: : 1047it [00:31, 92.52it/s] \u001b[A\n",
      "bce: 0.0374, loss: 0.3866 ||: : 1057it [00:31, 82.61it/s]\u001b[A\n",
      "bce: 0.0352, loss: 0.3842 ||: : 1066it [00:31, 78.42it/s]\u001b[A\n",
      "bce: 0.1140, loss: 0.3817 ||: : 1076it [00:31, 82.95it/s]\u001b[A\n",
      "bce: 0.5609, loss: 0.3804 ||: : 1086it [00:31, 85.13it/s]\u001b[A\n",
      "bce: 0.1861, loss: 0.3791 ||: : 1095it [00:31, 83.94it/s]\u001b[A\n",
      "bce: 0.3924, loss: 0.3775 ||: : 1105it [00:32, 86.16it/s]\u001b[A\n",
      "bce: 0.1356, loss: 0.3755 ||: : 1114it [00:32, 85.46it/s]\u001b[A\n",
      "bce: 0.0588, loss: 0.3729 ||: : 1126it [00:32, 93.16it/s]\u001b[A\n",
      "bce: 0.0553, loss: 0.3700 ||: : 1139it [00:32, 100.05it/s]\u001b[A\n",
      "bce: 0.3636, loss: 0.3677 ||: : 1150it [00:32, 95.94it/s] \u001b[A\n",
      "bce: 0.1199, loss: 0.3650 ||: : 1160it [00:32, 88.11it/s]\u001b[A\n",
      "bce: 0.0207, loss: 0.3632 ||: : 1170it [00:32, 84.04it/s]\u001b[A\n",
      "bce: 0.0929, loss: 0.3611 ||: : 1179it [00:32, 80.20it/s]\u001b[A\n",
      "bce: 0.0351, loss: 0.3592 ||: : 1188it [00:33, 77.88it/s]\u001b[A\n",
      "bce: 0.0398, loss: 0.3574 ||: : 1196it [00:33, 77.17it/s]\u001b[A\n",
      "bce: 0.1640, loss: 0.3558 ||: : 1204it [00:33, 74.20it/s]\u001b[A\n",
      "bce: 0.0843, loss: 0.3539 ||: : 1215it [00:33, 82.00it/s]\u001b[A\n",
      "bce: 0.3321, loss: 0.3525 ||: : 1227it [00:33, 90.26it/s]\u001b[A\n",
      "bce: 0.0339, loss: 0.3498 ||: : 1240it [00:33, 97.84it/s]\u001b[A\n",
      "bce: 0.0814, loss: 0.3471 ||: : 1252it [00:33, 103.05it/s]\u001b[A\n",
      "bce: 0.0561, loss: 0.3442 ||: : 1265it [00:33, 108.11it/s]\u001b[A\n",
      "bce: 0.2115, loss: 0.3420 ||: : 1277it [00:33, 111.39it/s]\u001b[A\n",
      "bce: 0.2176, loss: 0.3404 ||: : 1289it [00:33, 112.14it/s]\u001b[A\n",
      "bce: 0.2221, loss: 0.3383 ||: : 1301it [00:34, 114.02it/s]\u001b[A\n",
      "bce: 0.0335, loss: 0.3358 ||: : 1315it [00:34, 118.53it/s]\u001b[A\n",
      "bce: 0.0280, loss: 0.3347 ||: : 1328it [00:34, 120.25it/s]\u001b[A\n",
      "bce: 0.0789, loss: 0.3322 ||: : 1341it [00:34, 121.66it/s]\u001b[A\n",
      "bce: 0.2470, loss: 0.3305 ||: : 1354it [00:34, 121.59it/s]\u001b[A\n",
      "bce: 0.0114, loss: 0.3282 ||: : 1367it [00:34, 122.60it/s]\u001b[A\n",
      "bce: 0.2181, loss: 0.3260 ||: : 1380it [00:34, 124.41it/s]\u001b[A\n",
      "bce: 0.1153, loss: 0.3238 ||: : 1394it [00:34, 126.65it/s]\u001b[A\n",
      "bce: 0.1490, loss: 0.3212 ||: : 1407it [00:34, 124.49it/s]\u001b[A\n",
      "bce: 0.0503, loss: 0.3195 ||: : 1420it [00:35, 125.12it/s]\u001b[A\n",
      "bce: 0.1116, loss: 0.3175 ||: : 1434it [00:35, 126.35it/s]\u001b[A\n",
      "bce: 0.0510, loss: 0.3156 ||: : 1447it [00:35, 127.04it/s]\u001b[A\n",
      "bce: 0.1139, loss: 0.3139 ||: : 1460it [00:35, 126.84it/s]\u001b[A\n",
      "bce: 0.2313, loss: 0.3118 ||: : 1473it [00:35, 126.02it/s]\u001b[A\n",
      "bce: 0.0196, loss: 0.3098 ||: : 1486it [00:35, 125.69it/s]\u001b[A\n",
      "bce: 0.0374, loss: 0.3079 ||: : 1499it [00:35, 126.31it/s]\u001b[A\n",
      "bce: 0.0275, loss: 0.3062 ||: : 1512it [00:35, 125.07it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0502, loss: 0.3049 ||: : 1526it [00:35, 127.07it/s]\u001b[A\n",
      "bce: 0.0601, loss: 0.3032 ||: : 1539it [00:36, 114.38it/s]\u001b[A\n",
      "bce: 0.0509, loss: 0.3014 ||: : 1551it [00:36, 99.51it/s] \u001b[A\n",
      "bce: 0.0307, loss: 0.2998 ||: : 1562it [00:36, 90.87it/s]\u001b[A\n",
      "bce: 0.0694, loss: 0.2981 ||: : 1572it [00:36, 85.25it/s]\u001b[A\n",
      "bce: 0.1205, loss: 0.2968 ||: : 1581it [00:36, 82.32it/s]\u001b[A\n",
      "bce: 0.0529, loss: 0.2952 ||: : 1592it [00:36, 87.97it/s]\u001b[A\n",
      "bce: 0.0195, loss: 0.2938 ||: : 1602it [00:36, 89.93it/s]\u001b[A\n",
      "bce: 0.0473, loss: 0.2920 ||: : 1614it [00:36, 97.05it/s]\u001b[A\n",
      "bce: 0.0250, loss: 0.2902 ||: : 1627it [00:36, 104.26it/s]\u001b[A\n",
      "bce: 0.0116, loss: 0.2887 ||: : 1639it [00:37, 106.96it/s]\u001b[A\n",
      "bce: 0.0250, loss: 0.2866 ||: : 1653it [00:37, 113.19it/s]\u001b[A\n",
      "bce: 0.0107, loss: 0.2849 ||: : 1667it [00:37, 118.07it/s]\u001b[A\n",
      "bce: 0.0524, loss: 0.2829 ||: : 1681it [00:37, 122.07it/s]\u001b[A\n",
      "bce: 0.0557, loss: 0.2811 ||: : 1695it [00:37, 124.87it/s]\u001b[A\n",
      "bce: 0.5113, loss: 0.2797 ||: : 1708it [00:37, 122.41it/s]\u001b[A\n",
      "bce: 0.0236, loss: 0.2778 ||: : 1722it [00:37, 124.94it/s]\u001b[A\n",
      "bce: 0.2708, loss: 0.2763 ||: : 1736it [00:37, 127.54it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.2746 ||: : 1749it [00:37, 127.21it/s]\u001b[A\n",
      "bce: 0.0362, loss: 0.2731 ||: : 1762it [00:38, 124.97it/s]\u001b[A\n",
      "bce: 0.0250, loss: 0.2718 ||: : 1775it [00:38, 116.13it/s]\u001b[A\n",
      "bce: 0.1062, loss: 0.2703 ||: : 1787it [00:38, 117.12it/s]\u001b[A\n",
      "bce: 0.0152, loss: 0.2691 ||: : 1799it [00:38, 108.13it/s]\u001b[A\n",
      "bce: 0.0386, loss: 0.2678 ||: : 1811it [00:38, 100.85it/s]\u001b[A\n",
      "bce: 0.0234, loss: 0.2661 ||: : 1824it [00:38, 106.91it/s]\u001b[A\n",
      "bce: 0.0207, loss: 0.2647 ||: : 1836it [00:38, 110.21it/s]\u001b[A\n",
      "bce: 0.0991, loss: 0.2635 ||: : 1849it [00:38, 114.40it/s]\u001b[A\n",
      "bce: 0.0568, loss: 0.2619 ||: : 1862it [00:38, 116.37it/s]\u001b[A\n",
      "bce: 0.0872, loss: 0.2604 ||: : 1875it [00:39, 119.80it/s]\u001b[A\n",
      "bce: 0.0323, loss: 0.2596 ||: : 1888it [00:39, 120.72it/s]\u001b[A\n",
      "bce: 0.0170, loss: 0.2581 ||: : 1901it [00:39, 120.51it/s]\u001b[A\n",
      "bce: 0.0144, loss: 0.2566 ||: : 1914it [00:39, 122.30it/s]\u001b[A\n",
      "bce: 0.0258, loss: 0.2553 ||: : 1927it [00:39, 124.41it/s]\u001b[A\n",
      "bce: 0.0622, loss: 0.2539 ||: : 1940it [00:39, 123.91it/s]\u001b[A\n",
      "bce: 0.0163, loss: 0.2525 ||: : 1953it [00:39, 124.62it/s]\u001b[A\n",
      "bce: 0.0195, loss: 0.2513 ||: : 1966it [00:39, 123.60it/s]\u001b[A\n",
      "bce: 0.2437, loss: 0.2504 ||: : 1979it [00:39, 122.97it/s]\u001b[A\n",
      "bce: 0.0112, loss: 0.2490 ||: : 1992it [00:40, 123.87it/s]\u001b[A\n",
      "bce: 0.0331, loss: 0.2478 ||: : 2005it [00:40, 119.95it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.2466 ||: : 2018it [00:40, 122.59it/s]\u001b[A\n",
      "bce: 0.0250, loss: 0.2452 ||: : 2031it [00:40, 122.17it/s]\u001b[A\n",
      "bce: 0.0677, loss: 0.2442 ||: : 2044it [00:40, 123.53it/s]\u001b[A\n",
      "bce: 0.0046, loss: 0.2428 ||: : 2058it [00:40, 125.76it/s]\u001b[A\n",
      "bce: 0.0242, loss: 0.2414 ||: : 2072it [00:40, 127.72it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.2401 ||: : 2085it [00:40, 116.34it/s]\u001b[A\n",
      "bce: 0.0769, loss: 0.2388 ||: : 2098it [00:40, 119.96it/s]\u001b[A\n",
      "bce: 0.0501, loss: 0.2375 ||: : 2111it [00:40, 120.59it/s]\u001b[A\n",
      "bce: 0.0413, loss: 0.2362 ||: : 2125it [00:41, 124.03it/s]\u001b[A\n",
      "bce: 0.0095, loss: 0.2349 ||: : 2139it [00:41, 126.49it/s]\u001b[A\n",
      "bce: 0.0603, loss: 0.2337 ||: : 2153it [00:41, 128.35it/s]\u001b[A\n",
      "bce: 0.2437, loss: 0.2330 ||: : 2167it [00:41, 129.77it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.2319 ||: : 2181it [00:41, 126.75it/s]\u001b[A\n",
      "bce: 0.0111, loss: 0.2307 ||: : 2195it [00:41, 128.76it/s]\u001b[A\n",
      "bce: 0.0624, loss: 0.2295 ||: : 2208it [00:41, 127.69it/s]\u001b[A\n",
      "bce: 0.0791, loss: 0.2284 ||: : 2222it [00:41, 129.48it/s]\u001b[A\n",
      "bce: 0.0344, loss: 0.2278 ||: : 2235it [00:41, 127.78it/s]\u001b[A\n",
      "bce: 0.0035, loss: 0.2268 ||: : 2249it [00:42, 129.27it/s]\u001b[A\n",
      "bce: 0.0176, loss: 0.2255 ||: : 2263it [00:42, 130.14it/s]\u001b[A\n",
      "bce: 0.0577, loss: 0.2244 ||: : 2277it [00:42, 130.42it/s]\u001b[A\n",
      "bce: 0.2775, loss: 0.2233 ||: : 2291it [00:42, 131.04it/s]\u001b[A\n",
      "bce: 0.0159, loss: 0.2222 ||: : 2305it [00:42, 114.90it/s]\u001b[A\n",
      "bce: 0.0289, loss: 0.2211 ||: : 2318it [00:42, 116.77it/s]\u001b[A\n",
      "bce: 0.0572, loss: 0.2200 ||: : 2331it [00:42, 118.55it/s]\u001b[A\n",
      "bce: 0.0441, loss: 0.2190 ||: : 2344it [00:42, 118.74it/s]\u001b[A\n",
      "bce: 0.0110, loss: 0.2179 ||: : 2357it [00:42, 120.89it/s]\u001b[A\n",
      "bce: 0.0199, loss: 0.2171 ||: : 2370it [00:43, 122.21it/s]\u001b[A\n",
      "bce: 0.0057, loss: 0.2162 ||: : 2384it [00:43, 124.75it/s]\u001b[A\n",
      "bce: 0.0085, loss: 0.2151 ||: : 2397it [00:43, 114.80it/s]\u001b[A\n",
      "bce: 0.0147, loss: 0.2140 ||: : 2410it [00:43, 117.24it/s]\u001b[A\n",
      "bce: 0.0221, loss: 0.2132 ||: : 2424it [00:43, 121.60it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.2124 ||: : 2438it [00:43, 125.15it/s]\u001b[A\n",
      "bce: 0.0734, loss: 0.2113 ||: : 2451it [00:43, 124.78it/s]\u001b[A\n",
      "bce: 0.0920, loss: 0.2104 ||: : 2464it [00:43, 123.62it/s]\u001b[A\n",
      "bce: 0.1187, loss: 0.2095 ||: : 2478it [00:43, 126.31it/s]\u001b[A\n",
      "bce: 0.0013, loss: 0.2088 ||: : 2492it [00:44, 128.55it/s]\u001b[A\n",
      "bce: 0.0393, loss: 0.2079 ||: : 2505it [00:44, 126.84it/s]\u001b[A\n",
      "bce: 0.0157, loss: 0.2070 ||: : 2518it [00:44, 126.32it/s]\u001b[A\n",
      "bce: 0.2024, loss: 0.2062 ||: : 2531it [00:44, 114.79it/s]\u001b[A\n",
      "bce: 0.0079, loss: 0.2052 ||: : 2545it [00:44, 119.68it/s]\u001b[A\n",
      "bce: 0.0312, loss: 0.2043 ||: : 2558it [00:44, 120.25it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.2034 ||: : 2572it [00:44, 123.74it/s]\u001b[A\n",
      "bce: 0.0924, loss: 0.2025 ||: : 2585it [00:44, 113.04it/s]\u001b[A\n",
      "bce: 0.0085, loss: 0.2017 ||: : 2597it [00:44, 106.46it/s]\u001b[A\n",
      "bce: 0.0028, loss: 0.2011 ||: : 2608it [00:45, 106.46it/s]\u001b[A\n",
      "bce: 0.0616, loss: 0.2004 ||: : 2620it [00:45, 108.02it/s]\u001b[A\n",
      "bce: 0.0863, loss: 0.1996 ||: : 2632it [00:45, 110.69it/s]\u001b[A\n",
      "bce: 0.2384, loss: 0.1989 ||: : 2645it [00:45, 115.80it/s]\u001b[A\n",
      "bce: 0.0038, loss: 0.1982 ||: : 2657it [00:45, 107.73it/s]\u001b[A\n",
      "bce: 0.0144, loss: 0.1975 ||: : 2670it [00:45, 112.00it/s]\u001b[A\n",
      "bce: 0.0186, loss: 0.1967 ||: : 2682it [00:45, 114.11it/s]\u001b[A\n",
      "bce: 0.1377, loss: 0.1960 ||: : 2694it [00:45, 112.89it/s]\u001b[A\n",
      "bce: 0.0057, loss: 0.1953 ||: : 2706it [00:45, 112.06it/s]\u001b[A\n",
      "bce: 0.0184, loss: 0.1945 ||: : 2718it [00:46, 110.33it/s]\u001b[A\n",
      "bce: 0.0290, loss: 0.1939 ||: : 2731it [00:46, 114.58it/s]\u001b[A\n",
      "bce: 0.0035, loss: 0.1931 ||: : 2745it [00:46, 119.82it/s]\u001b[A\n",
      "bce: 0.0083, loss: 0.1923 ||: : 2759it [00:46, 123.80it/s]\u001b[A\n",
      "bce: 0.0048, loss: 0.1915 ||: : 2773it [00:46, 125.94it/s]\u001b[A\n",
      "bce: 0.0026, loss: 0.1907 ||: : 2786it [00:46, 125.51it/s]\u001b[A\n",
      "bce: 0.0700, loss: 0.1899 ||: : 2800it [00:46, 124.88it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.1892 ||: : 2813it [00:46, 125.60it/s]\u001b[A\n",
      "bce: 0.0124, loss: 0.1884 ||: : 2826it [00:46, 125.02it/s]\u001b[A\n",
      "bce: 0.0762, loss: 0.1878 ||: : 2839it [00:46, 125.70it/s]\u001b[A\n",
      "bce: 0.0076, loss: 0.1871 ||: : 2852it [00:47, 126.03it/s]\u001b[A\n",
      "bce: 0.0063, loss: 0.1863 ||: : 2866it [00:47, 127.41it/s]\u001b[A\n",
      "bce: 0.0032, loss: 0.1855 ||: : 2880it [00:47, 128.44it/s]\u001b[A\n",
      "bce: 0.0117, loss: 0.1847 ||: : 2893it [00:47, 118.54it/s]\u001b[A\n",
      "bce: 0.0069, loss: 0.1840 ||: : 2906it [00:47, 115.38it/s]\u001b[A\n",
      "bce: 0.1110, loss: 0.1833 ||: : 2919it [00:47, 117.70it/s]\u001b[A\n",
      "bce: 0.0181, loss: 0.1827 ||: : 2931it [00:47, 116.11it/s]\u001b[A\n",
      "bce: 0.0040, loss: 0.1819 ||: : 2945it [00:47, 120.41it/s]\u001b[A\n",
      "bce: 0.0585, loss: 0.1812 ||: : 2958it [00:48, 112.30it/s]\u001b[A\n",
      "bce: 0.0111, loss: 0.1804 ||: : 2972it [00:48, 116.65it/s]\u001b[A\n",
      "bce: 0.0262, loss: 0.1799 ||: : 2986it [00:48, 120.66it/s]\u001b[A\n",
      "bce: 0.0435, loss: 0.1793 ||: : 2999it [00:48, 122.83it/s]\u001b[A\n",
      "bce: 0.0333, loss: 0.1786 ||: : 3012it [00:48, 121.56it/s]\u001b[A\n",
      "bce: 0.0110, loss: 0.1779 ||: : 3026it [00:48, 124.45it/s]\u001b[A\n",
      "bce: 0.1107, loss: 0.1773 ||: : 3039it [00:48, 124.08it/s]\u001b[A\n",
      "bce: 0.0111, loss: 0.1766 ||: : 3052it [00:48, 113.73it/s]\u001b[A\n",
      "bce: 0.0052, loss: 0.1759 ||: : 3065it [00:48, 116.43it/s]\u001b[A\n",
      "bce: 0.0082, loss: 0.1752 ||: : 3078it [00:48, 120.11it/s]\u001b[A\n",
      "bce: 0.0040, loss: 0.1747 ||: : 3091it [00:49, 120.83it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.1740 ||: : 3104it [00:49, 119.01it/s]\u001b[A\n",
      "bce: 0.0063, loss: 0.1732 ||: : 3118it [00:49, 122.11it/s]\u001b[A\n",
      "bce: 0.0850, loss: 0.1726 ||: : 3131it [00:49, 116.82it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.1721 ||: : 3143it [00:49, 112.02it/s]\u001b[A\n",
      "bce: 0.1446, loss: 0.1715 ||: : 3156it [00:49, 116.12it/s]\u001b[A\n",
      "bce: 0.0131, loss: 0.1708 ||: : 3170it [00:49, 120.37it/s]\u001b[A\n",
      "bce: 0.0088, loss: 0.1702 ||: : 3184it [00:49, 123.72it/s]\u001b[A\n",
      "bce: 0.0154, loss: 0.1695 ||: : 3198it [00:49, 126.46it/s]\u001b[A\n",
      "bce: 0.0160, loss: 0.1688 ||: : 3211it [00:50, 125.47it/s]\u001b[A\n",
      "bce: 0.0422, loss: 0.1682 ||: : 3224it [00:50, 126.44it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.1677 ||: : 3237it [00:50, 124.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0008, loss: 0.1671 ||: : 3251it [00:50, 127.07it/s]\u001b[A\n",
      "bce: 0.0099, loss: 0.1666 ||: : 3264it [00:50, 127.16it/s]\u001b[A\n",
      "bce: 0.1268, loss: 0.1662 ||: : 3277it [00:56,  7.52it/s] \u001b[A\n",
      "bce: 0.3250, loss: 0.1667 ||: : 3290it [00:56, 10.48it/s]\u001b[A\n",
      "bce: 0.0111, loss: 0.1672 ||: : 3303it [00:56, 14.44it/s]\u001b[A\n",
      "bce: 0.2965, loss: 0.1672 ||: : 3314it [00:56, 19.32it/s]\u001b[A\n",
      "bce: 0.5284, loss: 0.1682 ||: : 3326it [00:56, 25.80it/s]\u001b[A\n",
      "bce: 0.0546, loss: 0.1686 ||: : 3339it [00:56, 33.97it/s]\u001b[A\n",
      "bce: 0.0204, loss: 0.1687 ||: : 3351it [00:56, 43.27it/s]\u001b[A\n",
      "bce: 0.0532, loss: 0.1688 ||: : 3364it [00:56, 54.08it/s]\u001b[A\n",
      "bce: 0.3033, loss: 0.1689 ||: : 3376it [00:56, 61.53it/s]\u001b[A\n",
      "bce: 0.0226, loss: 0.1687 ||: : 3389it [00:56, 72.33it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.1691 ||: : 3401it [00:57, 81.36it/s]\u001b[A\n",
      "bce: 0.4487, loss: 0.1693 ||: : 3414it [00:57, 90.65it/s]\u001b[A\n",
      "bce: 0.0887, loss: 0.1703 ||: : 3427it [00:57, 99.68it/s]\u001b[A\n",
      "bce: 0.0135, loss: 0.1701 ||: : 3440it [00:57, 107.09it/s]\u001b[A\n",
      "bce: 0.0030, loss: 0.1697 ||: : 3453it [00:57, 110.54it/s]\u001b[A\n",
      "bce: 0.0814, loss: 0.1695 ||: : 3466it [00:57, 115.71it/s]\u001b[A\n",
      "bce: 0.0076, loss: 0.1692 ||: : 3480it [00:57, 120.01it/s]\u001b[A\n",
      "bce: 1.3923, loss: 0.1693 ||: : 3493it [00:57, 109.00it/s]\u001b[A\n",
      "bce: 0.0150, loss: 0.1692 ||: : 3506it [00:57, 112.60it/s]\u001b[A\n",
      "bce: 0.1346, loss: 0.1689 ||: : 3520it [00:58, 117.54it/s]\u001b[A\n",
      "bce: 0.0643, loss: 0.1688 ||: : 3533it [00:58, 110.12it/s]\u001b[A\n",
      "bce: 0.0063, loss: 0.1695 ||: : 3546it [00:58, 113.34it/s]\u001b[A\n",
      "bce: 0.0974, loss: 0.1693 ||: : 3560it [00:58, 118.07it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.1693 ||: : 3573it [00:58, 120.38it/s]\u001b[A\n",
      "bce: 0.1172, loss: 0.1700 ||: : 3586it [01:03,  8.05it/s] \u001b[A\n",
      "bce: 0.0054, loss: 0.1697 ||: : 3599it [01:03, 11.19it/s]\u001b[A\n",
      "bce: 0.0583, loss: 0.1696 ||: : 3611it [01:03, 15.37it/s]\u001b[A\n",
      "bce: 0.0181, loss: 0.1691 ||: : 3624it [01:03, 20.88it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.1694 ||: : 3637it [01:04, 27.88it/s]\u001b[A\n",
      "bce: 0.0174, loss: 0.1690 ||: : 3650it [01:04, 36.47it/s]\u001b[A\n",
      "bce: 0.0863, loss: 0.1691 ||: : 3663it [01:04, 46.25it/s]\u001b[A\n",
      "bce: 0.0063, loss: 0.1688 ||: : 3676it [01:04, 56.92it/s]\u001b[A\n",
      "bce: 0.1930, loss: 0.1684 ||: : 3690it [01:04, 68.54it/s]\u001b[A\n",
      "bce: 0.0188, loss: 0.1681 ||: : 3703it [01:04, 73.29it/s]\u001b[A\n",
      "bce: 0.0541, loss: 0.1676 ||: : 3716it [01:04, 84.02it/s]\u001b[A\n",
      "bce: 0.0180, loss: 0.1673 ||: : 3730it [01:04, 94.15it/s]\u001b[A\n",
      "bce: 0.0508, loss: 0.1669 ||: : 3743it [01:04, 93.04it/s]\u001b[A\n",
      "bce: 0.5993, loss: 0.1666 ||: : 3756it [01:05, 100.37it/s]\u001b[A\n",
      "bce: 0.0407, loss: 0.1662 ||: : 3769it [01:05, 106.36it/s]\u001b[A\n",
      "bce: 0.0057, loss: 0.1658 ||: : 3782it [01:05, 112.34it/s]\u001b[A\n",
      "bce: 0.1997, loss: 0.1658 ||: : 3796it [01:05, 117.64it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.1654 ||: : 3809it [01:05, 118.91it/s]\u001b[A\n",
      "bce: 0.0026, loss: 0.1651 ||: : 3822it [01:05, 121.46it/s]\u001b[A\n",
      "bce: 0.0028, loss: 0.1647 ||: : 3835it [01:05, 122.01it/s]\u001b[A\n",
      "bce: 0.0964, loss: 0.1647 ||: : 3848it [01:05, 112.95it/s]\u001b[A\n",
      "bce: 0.0237, loss: 0.1645 ||: : 3860it [01:05, 113.03it/s]\u001b[A\n",
      "bce: 0.0544, loss: 0.1643 ||: : 3872it [01:06, 107.38it/s]\u001b[A\n",
      "bce: 0.0031, loss: 0.1640 ||: : 3885it [01:06, 113.02it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.1637 ||: : 3897it [01:11,  7.36it/s] \u001b[A\n",
      "bce: 0.2214, loss: 0.1634 ||: : 3906it [01:11, 10.03it/s]\u001b[A\n",
      "bce: 0.0328, loss: 0.1632 ||: : 3914it [01:11, 13.50it/s]\u001b[A\n",
      "bce: 0.0812, loss: 0.1630 ||: : 3922it [01:11, 17.73it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.1629 ||: : 3930it [01:11, 22.76it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.1629 ||: : 3941it [01:11, 29.73it/s]\u001b[A\n",
      "bce: 0.1874, loss: 0.1629 ||: : 3952it [01:12, 38.04it/s]\u001b[A\n",
      "bce: 0.0035, loss: 0.1626 ||: : 3963it [01:12, 47.08it/s]\u001b[A\n",
      "bce: 0.0084, loss: 0.1623 ||: : 3976it [01:12, 57.97it/s]\u001b[A\n",
      "bce: 0.1538, loss: 0.1621 ||: : 3987it [01:12, 65.24it/s]\u001b[A\n",
      "bce: 0.0147, loss: 0.1619 ||: : 3999it [01:12, 74.71it/s]\u001b[A\n",
      "bce: 0.0681, loss: 0.1616 ||: : 4010it [01:12, 80.93it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.1616 ||: : 4022it [01:12, 88.13it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1612 ||: : 4034it [01:12, 95.11it/s]\u001b[A\n",
      "bce: 1.1067, loss: 0.1611 ||: : 4046it [01:12, 101.41it/s]\u001b[A\n",
      "bce: 0.0095, loss: 0.1609 ||: : 4058it [01:13, 104.74it/s]\u001b[A\n",
      "bce: 0.0019, loss: 0.1606 ||: : 4070it [01:13, 103.65it/s]\u001b[A\n",
      "bce: 0.1074, loss: 0.1602 ||: : 4081it [01:13, 104.26it/s]\u001b[A\n",
      "bce: 0.0077, loss: 0.1599 ||: : 4093it [01:13, 107.79it/s]\u001b[A\n",
      "bce: 0.0213, loss: 0.1599 ||: : 4105it [01:13, 104.16it/s]\u001b[A\n",
      "bce: 0.0031, loss: 0.1596 ||: : 4117it [01:13, 108.09it/s]\u001b[A\n",
      "bce: 0.0690, loss: 0.1595 ||: : 4129it [01:13, 108.96it/s]\u001b[A\n",
      "bce: 0.3049, loss: 0.1594 ||: : 4141it [01:13, 107.10it/s]\u001b[A\n",
      "bce: 0.0313, loss: 0.1595 ||: : 4153it [01:13, 108.78it/s]\u001b[A\n",
      "bce: 0.0041, loss: 0.1592 ||: : 4164it [01:14, 108.39it/s]\u001b[A\n",
      "bce: 0.0136, loss: 0.1589 ||: : 4176it [01:14, 109.15it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.1586 ||: : 4188it [01:14, 108.06it/s]\u001b[A\n",
      "bce: 0.0170, loss: 0.1583 ||: : 4199it [01:14, 107.59it/s]\u001b[A\n",
      "bce: 0.0301, loss: 0.1580 ||: : 4210it [01:19,  7.31it/s] \u001b[A\n",
      "bce: 0.0996, loss: 0.1577 ||: : 4224it [01:19, 10.20it/s]\u001b[A\n",
      "bce: 0.0492, loss: 0.1575 ||: : 4238it [01:19, 14.11it/s]\u001b[A\n",
      "bce: 0.0406, loss: 0.1573 ||: : 4252it [01:19, 19.27it/s]\u001b[A\n",
      "bce: 0.1577, loss: 0.1572 ||: : 4266it [01:19, 25.89it/s]\u001b[A\n",
      "bce: 0.0032, loss: 0.1569 ||: : 4280it [01:19, 34.10it/s]\u001b[A\n",
      "bce: 0.0371, loss: 0.1566 ||: : 4293it [01:19, 43.76it/s]\u001b[A\n",
      "bce: 0.1604, loss: 0.1564 ||: : 4306it [01:19, 52.97it/s]\u001b[A\n",
      "bce: 0.0702, loss: 0.1562 ||: : 4318it [01:19, 63.02it/s]\u001b[A\n",
      "bce: 0.9642, loss: 0.1560 ||: : 4330it [01:20, 73.49it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.1556 ||: : 4343it [01:20, 83.44it/s]\u001b[A\n",
      "bce: 0.3008, loss: 0.1553 ||: : 4357it [01:20, 93.71it/s]\u001b[A\n",
      "bce: 0.0070, loss: 0.1548 ||: : 4371it [01:20, 102.11it/s]\u001b[A\n",
      "bce: 2.9154, loss: 0.1552 ||: : 4384it [01:20, 104.04it/s]\u001b[A\n",
      "bce: 0.0054, loss: 0.1550 ||: : 4397it [01:20, 110.59it/s]\u001b[A\n",
      "bce: 0.0036, loss: 0.1546 ||: : 4410it [01:20, 113.60it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1548 ||: : 4423it [01:20, 115.13it/s]\u001b[A\n",
      "bce: 0.0019, loss: 0.1544 ||: : 4436it [01:20, 119.00it/s]\u001b[A\n",
      "bce: 0.0445, loss: 0.1540 ||: : 4449it [01:21, 121.68it/s]\u001b[A\n",
      "bce: 0.0276, loss: 0.1536 ||: : 4462it [01:21, 122.26it/s]\u001b[A\n",
      "bce: 0.0056, loss: 0.1534 ||: : 4475it [01:21, 122.28it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.1531 ||: : 4488it [01:21, 117.57it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.1530 ||: : 4500it [01:21, 111.73it/s]\u001b[A\n",
      "bce: 0.0081, loss: 0.1527 ||: : 4512it [01:21, 112.98it/s]\u001b[A\n",
      "bce: 0.0079, loss: 0.1526 ||: : 4524it [01:26,  7.74it/s] \u001b[A\n",
      "bce: 0.1062, loss: 0.1523 ||: : 4538it [01:26, 10.79it/s]\u001b[A\n",
      "bce: 0.0322, loss: 0.1519 ||: : 4552it [01:26, 14.89it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1517 ||: : 4566it [01:26, 20.29it/s]\u001b[A\n",
      "bce: 0.0547, loss: 0.1514 ||: : 4580it [01:26, 27.20it/s]\u001b[A\n",
      "bce: 0.0635, loss: 0.1510 ||: : 4594it [01:27, 35.71it/s]\u001b[A\n",
      "bce: 0.2380, loss: 0.1508 ||: : 4607it [01:27, 45.35it/s]\u001b[A\n",
      "bce: 0.0090, loss: 0.1506 ||: : 4621it [01:27, 56.48it/s]\u001b[A\n",
      "bce: 0.0031, loss: 0.1503 ||: : 4634it [01:27, 67.29it/s]\u001b[A\n",
      "bce: 0.2586, loss: 0.1500 ||: : 4647it [01:27, 78.66it/s]\u001b[A\n",
      "bce: 0.3133, loss: 0.1499 ||: : 4660it [01:27, 87.73it/s]\u001b[A\n",
      "bce: 0.0037, loss: 0.1496 ||: : 4673it [01:27, 95.52it/s]\u001b[A\n",
      "bce: 0.0013, loss: 0.1493 ||: : 4686it [01:27, 101.87it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.1494 ||: : 4699it [01:27, 106.50it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.1491 ||: : 4712it [01:28, 106.84it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1492 ||: : 4725it [01:28, 111.61it/s]\u001b[A\n",
      "bce: 0.0105, loss: 0.1491 ||: : 4737it [01:28, 110.04it/s]\u001b[A\n",
      "bce: 0.5004, loss: 0.1491 ||: : 4749it [01:28, 108.86it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.1488 ||: : 4761it [01:28, 108.23it/s]\u001b[A\n",
      "bce: 0.0562, loss: 0.1491 ||: : 4774it [01:28, 112.99it/s]\u001b[A\n",
      "bce: 0.0121, loss: 0.1487 ||: : 4788it [01:28, 117.65it/s]\u001b[A\n",
      "bce: 0.0029, loss: 0.1485 ||: : 4800it [01:28, 114.61it/s]\u001b[A\n",
      "bce: 0.0362, loss: 0.1482 ||: : 4812it [01:28, 114.00it/s]\u001b[A\n",
      "bce: 0.0199, loss: 0.1479 ||: : 4824it [01:33,  7.58it/s] \u001b[A\n",
      "bce: 0.0015, loss: 0.1476 ||: : 4837it [01:34, 10.56it/s]\u001b[A\n",
      "bce: 0.0030, loss: 0.1478 ||: : 4850it [01:34, 14.57it/s]\u001b[A\n",
      "bce: 0.0054, loss: 0.1480 ||: : 4862it [01:34, 19.72it/s]\u001b[A\n",
      "bce: 0.1334, loss: 0.1477 ||: : 4875it [01:34, 26.41it/s]\u001b[A\n",
      "bce: 0.0402, loss: 0.1475 ||: : 4888it [01:34, 34.69it/s]\u001b[A\n",
      "bce: 0.0442, loss: 0.1472 ||: : 4901it [01:34, 44.28it/s]\u001b[A\n",
      "bce: 0.0056, loss: 0.1470 ||: : 4915it [01:34, 55.37it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0191, loss: 0.1467 ||: : 4929it [01:34, 67.12it/s]\u001b[A\n",
      "bce: 0.0019, loss: 0.1466 ||: : 4942it [01:34, 76.24it/s]\u001b[A\n",
      "bce: 0.0527, loss: 0.1469 ||: : 4955it [01:35, 77.98it/s]\u001b[A\n",
      "bce: 0.1327, loss: 0.1468 ||: : 4967it [01:35, 85.61it/s]\u001b[A\n",
      "bce: 0.0029, loss: 0.1466 ||: : 4979it [01:35, 93.62it/s]\u001b[A\n",
      "bce: 0.0059, loss: 0.1463 ||: : 4992it [01:35, 100.39it/s]\u001b[A\n",
      "bce: 0.0225, loss: 0.1461 ||: : 5004it [01:35, 101.60it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1459 ||: : 5016it [01:35, 104.01it/s]\u001b[A\n",
      "bce: 0.0522, loss: 0.1456 ||: : 5028it [01:35, 98.42it/s] \u001b[A\n",
      "bce: 0.0755, loss: 0.1454 ||: : 5039it [01:35, 97.68it/s]\u001b[A\n",
      "bce: 0.0715, loss: 0.1452 ||: : 5050it [01:35, 98.54it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.1449 ||: : 5061it [01:36, 99.55it/s]\u001b[A\n",
      "bce: 0.1328, loss: 0.1450 ||: : 5072it [01:36, 96.42it/s]\u001b[A\n",
      "bce: 0.0858, loss: 0.1448 ||: : 5082it [01:36, 94.31it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.1448 ||: : 5092it [01:36, 91.38it/s]\u001b[A\n",
      "bce: 0.1631, loss: 0.1446 ||: : 5102it [01:36, 89.81it/s]\u001b[A\n",
      "bce: 0.2725, loss: 0.1444 ||: : 5114it [01:36, 95.22it/s]\u001b[A\n",
      "bce: 0.0046, loss: 0.1442 ||: : 5124it [01:36, 94.44it/s]\u001b[A\n",
      "bce: 0.0359, loss: 0.1442 ||: : 5134it [01:36, 92.26it/s]\u001b[A\n",
      "bce: 0.0065, loss: 0.1440 ||: : 5144it [01:42,  6.05it/s]\u001b[A\n",
      "bce: 0.0333, loss: 0.1437 ||: : 5158it [01:42,  8.48it/s]\u001b[A\n",
      "bce: 0.0049, loss: 0.1435 ||: : 5170it [01:42, 11.73it/s]\u001b[A\n",
      "bce: 0.1611, loss: 0.1434 ||: : 5183it [01:42, 16.09it/s]\u001b[A\n",
      "bce: 0.0181, loss: 0.1432 ||: : 5196it [01:42, 21.73it/s]\u001b[A\n",
      "bce: 0.0130, loss: 0.1430 ||: : 5209it [01:42, 28.86it/s]\u001b[A\n",
      "bce: 0.0784, loss: 0.1427 ||: : 5222it [01:42, 37.50it/s]\u001b[A\n",
      "bce: 0.0036, loss: 0.1424 ||: : 5235it [01:42, 47.61it/s]\u001b[A\n",
      "bce: 0.0213, loss: 0.1421 ||: : 5248it [01:42, 58.24it/s]\u001b[A\n",
      "bce: 0.0109, loss: 0.1418 ||: : 5261it [01:43, 69.11it/s]\u001b[A\n",
      "bce: 0.0097, loss: 0.1415 ||: : 5274it [01:43, 79.10it/s]\u001b[A\n",
      "bce: 0.0766, loss: 0.1413 ||: : 5287it [01:43, 86.65it/s]\u001b[A\n",
      "bce: 0.0396, loss: 0.1410 ||: : 5300it [01:43, 92.92it/s]\u001b[A\n",
      "bce: 0.2483, loss: 0.1408 ||: : 5313it [01:43, 100.66it/s]\u001b[A\n",
      "bce: 0.0094, loss: 0.1405 ||: : 5325it [01:43, 104.78it/s]\u001b[A\n",
      "bce: 0.0074, loss: 0.1403 ||: : 5338it [01:43, 109.99it/s]\u001b[A\n",
      "bce: 0.0453, loss: 0.1400 ||: : 5350it [01:43, 111.28it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.1399 ||: : 5364it [01:43, 116.68it/s]\u001b[A\n",
      "bce: 0.0144, loss: 0.1396 ||: : 5378it [01:43, 120.71it/s]\u001b[A\n",
      "bce: 0.0577, loss: 0.1394 ||: : 5392it [01:44, 123.82it/s]\u001b[A\n",
      "bce: 0.0271, loss: 0.1391 ||: : 5405it [01:44, 118.67it/s]\u001b[A\n",
      "bce: 0.0030, loss: 0.1388 ||: : 5418it [01:44, 118.34it/s]\u001b[A\n",
      "bce: 0.0026, loss: 0.1385 ||: : 5431it [01:44, 119.03it/s]\u001b[A\n",
      "bce: 0.1276, loss: 0.1382 ||: : 5444it [01:44, 120.30it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.1383 ||: : 5457it [01:49,  8.37it/s] \u001b[A\n",
      "bce: 0.0015, loss: 0.1380 ||: : 5470it [01:49, 11.63it/s]\u001b[A\n",
      "bce: 0.0081, loss: 0.1378 ||: : 5483it [01:49, 15.97it/s]\u001b[A\n",
      "bce: 0.0066, loss: 0.1376 ||: : 5496it [01:49, 21.67it/s]\u001b[A\n",
      "bce: 0.2368, loss: 0.1374 ||: : 5508it [01:49, 28.58it/s]\u001b[A\n",
      "bce: 0.0073, loss: 0.1371 ||: : 5522it [01:49, 37.24it/s]\u001b[A\n",
      "bce: 0.0642, loss: 0.1369 ||: : 5534it [01:50, 46.83it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.1366 ||: : 5547it [01:50, 57.48it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.1363 ||: : 5561it [01:50, 69.09it/s]\u001b[A\n",
      "bce: 0.0260, loss: 0.1361 ||: : 5574it [01:50, 78.76it/s]\u001b[A\n",
      "bce: 0.0114, loss: 0.1358 ||: : 5587it [01:50, 86.33it/s]\u001b[A\n",
      "bce: 0.0132, loss: 0.1355 ||: : 5599it [01:50, 94.12it/s]\u001b[A\n",
      "bce: 0.0063, loss: 0.1353 ||: : 5611it [01:50, 98.62it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.1350 ||: : 5623it [01:50, 102.19it/s]\u001b[A\n",
      "bce: 0.2220, loss: 0.1349 ||: : 5636it [01:50, 106.76it/s]\u001b[A\n",
      "bce: 0.0193, loss: 0.1347 ||: : 5648it [01:51, 110.41it/s]\u001b[A\n",
      "bce: 0.0169, loss: 0.1346 ||: : 5660it [01:51, 110.70it/s]\u001b[A\n",
      "bce: 0.0744, loss: 0.1343 ||: : 5673it [01:51, 114.85it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.1342 ||: : 5685it [01:51, 116.23it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1340 ||: : 5698it [01:51, 118.28it/s]\u001b[A\n",
      "bce: 0.0052, loss: 0.1338 ||: : 5711it [01:51, 115.26it/s]\u001b[A\n",
      "bce: 0.0124, loss: 0.1335 ||: : 5723it [01:51, 115.07it/s]\u001b[A\n",
      "bce: 0.0066, loss: 0.1333 ||: : 5736it [01:51, 118.89it/s]\u001b[A\n",
      "bce: 0.0156, loss: 0.1335 ||: : 5748it [01:51, 116.40it/s]\u001b[A\n",
      "bce: 0.0201, loss: 0.1335 ||: : 5760it [01:57,  7.26it/s] \u001b[A\n",
      "bce: 0.0160, loss: 0.1333 ||: : 5772it [01:57, 10.10it/s]\u001b[A\n",
      "bce: 0.0093, loss: 0.1332 ||: : 5784it [01:57, 13.88it/s]\u001b[A\n",
      "bce: 0.0120, loss: 0.1330 ||: : 5795it [01:57, 18.81it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.1328 ||: : 5806it [01:57, 24.95it/s]\u001b[A\n",
      "bce: 0.0069, loss: 0.1325 ||: : 5819it [01:57, 32.65it/s]\u001b[A\n",
      "bce: 0.0282, loss: 0.1323 ||: : 5830it [01:57, 41.37it/s]\u001b[A\n",
      "bce: 0.0064, loss: 0.1321 ||: : 5842it [01:57, 50.99it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.1319 ||: : 5855it [01:58, 61.62it/s]\u001b[A\n",
      "bce: 0.0178, loss: 0.1318 ||: : 5868it [01:58, 72.29it/s]\u001b[A\n",
      "bce: 0.1423, loss: 0.1318 ||: : 5880it [01:58, 81.90it/s]\u001b[A\n",
      "bce: 0.0089, loss: 0.1317 ||: : 5893it [01:58, 91.55it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.1314 ||: : 5905it [01:58, 94.84it/s]\u001b[A\n",
      "bce: 0.0028, loss: 0.1312 ||: : 5918it [01:58, 102.59it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.1309 ||: : 5931it [01:58, 107.55it/s]\u001b[A\n",
      "bce: 0.0133, loss: 0.1307 ||: : 5944it [01:58, 113.09it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.1305 ||: : 5957it [01:58, 117.39it/s]\u001b[A\n",
      "bce: 0.2364, loss: 0.1303 ||: : 5970it [01:58, 118.64it/s]\u001b[A\n",
      "bce: 0.0088, loss: 0.1300 ||: : 5983it [01:59, 121.62it/s]\u001b[A\n",
      "bce: 0.0603, loss: 0.1298 ||: : 5996it [01:59, 120.94it/s]\u001b[A\n",
      "bce: 0.0139, loss: 0.1297 ||: : 6009it [01:59, 120.81it/s]\u001b[A\n",
      "bce: 0.0106, loss: 0.1294 ||: : 6022it [01:59, 122.99it/s]\u001b[A\n",
      "bce: 0.0058, loss: 0.1292 ||: : 6035it [01:59, 120.19it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.1290 ||: : 6048it [01:59, 115.77it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.1288 ||: : 6060it [01:59, 116.33it/s]\u001b[A\n",
      "bce: 0.0097, loss: 0.1286 ||: : 6072it [02:04,  7.72it/s] \u001b[A\n",
      "bce: 0.0057, loss: 0.1283 ||: : 6085it [02:04, 10.76it/s]\u001b[A\n",
      "bce: 0.0168, loss: 0.1281 ||: : 6098it [02:04, 14.81it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.1279 ||: : 6109it [02:04, 20.00it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.1279 ||: : 6122it [02:05, 26.79it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.1277 ||: : 6134it [02:05, 34.90it/s]\u001b[A\n",
      "bce: 0.1404, loss: 0.1275 ||: : 6147it [02:05, 44.52it/s]\u001b[A\n",
      "bce: 0.0317, loss: 0.1273 ||: : 6160it [02:05, 55.35it/s]\u001b[A\n",
      "bce: 0.0774, loss: 0.1272 ||: : 6173it [02:05, 66.57it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.1270 ||: : 6186it [02:05, 77.29it/s]\u001b[A\n",
      "bce: 0.0059, loss: 0.1268 ||: : 6199it [02:05, 87.55it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.1267 ||: : 6212it [02:05, 94.32it/s]\u001b[A\n",
      "bce: 0.1877, loss: 0.1265 ||: : 6225it [02:05, 99.60it/s]\u001b[A\n",
      "bce: 0.0307, loss: 0.1263 ||: : 6238it [02:06, 105.22it/s]\u001b[A\n",
      "bce: 0.0036, loss: 0.1262 ||: : 6251it [02:06, 111.34it/s]\u001b[A\n",
      "bce: 0.0109, loss: 0.1262 ||: : 6264it [02:06, 116.15it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.1259 ||: : 6277it [02:06, 115.28it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.1257 ||: : 6290it [02:06, 117.32it/s]\u001b[A\n",
      "bce: 0.5351, loss: 0.1256 ||: : 6303it [02:06, 116.66it/s]\u001b[A\n",
      "bce: 0.0253, loss: 0.1257 ||: : 6316it [02:06, 118.22it/s]\u001b[A\n",
      "bce: 0.0035, loss: 0.1254 ||: : 6329it [02:06, 118.56it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.1252 ||: : 6342it [02:06, 120.03it/s]\u001b[A\n",
      "bce: 0.0026, loss: 0.1250 ||: : 6355it [02:07, 118.94it/s]\u001b[A\n",
      "bce: 0.0056, loss: 0.1248 ||: : 6368it [02:07, 120.65it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1246 ||: : 6381it [02:12,  8.39it/s] \u001b[A\n",
      "bce: 0.0004, loss: 0.1244 ||: : 6393it [02:12, 11.62it/s]\u001b[A\n",
      "bce: 0.0028, loss: 0.1242 ||: : 6405it [02:12, 15.94it/s]\u001b[A\n",
      "bce: 0.0520, loss: 0.1240 ||: : 6418it [02:12, 21.57it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1241 ||: : 6431it [02:12, 28.74it/s]\u001b[A\n",
      "bce: 0.0096, loss: 0.1239 ||: : 6444it [02:12, 37.38it/s]\u001b[A\n",
      "bce: 0.0056, loss: 0.1236 ||: : 6457it [02:12, 47.36it/s]\u001b[A\n",
      "bce: 0.0032, loss: 0.1234 ||: : 6469it [02:12, 55.07it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.1234 ||: : 6481it [02:12, 65.40it/s]\u001b[A\n",
      "bce: 0.0117, loss: 0.1232 ||: : 6494it [02:12, 76.48it/s]\u001b[A\n",
      "bce: 0.0380, loss: 0.1230 ||: : 6506it [02:13, 84.86it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1229 ||: : 6518it [02:13, 92.98it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.1226 ||: : 6531it [02:13, 101.06it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1225 ||: : 6544it [02:13, 108.04it/s]\u001b[A\n",
      "bce: 0.0959, loss: 0.1224 ||: : 6557it [02:13, 108.31it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1222 ||: : 6570it [02:13, 113.09it/s]\u001b[A\n",
      "bce: 0.3787, loss: 0.1220 ||: : 6583it [02:13, 117.16it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0191, loss: 0.1227 ||: : 6596it [02:13, 113.38it/s]\u001b[A\n",
      "bce: 0.0078, loss: 0.1225 ||: : 6608it [02:13, 109.28it/s]\u001b[A\n",
      "bce: 0.0054, loss: 0.1223 ||: : 6621it [02:14, 113.78it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1221 ||: : 6633it [02:14, 104.95it/s]\u001b[A\n",
      "bce: 0.0891, loss: 0.1220 ||: : 6645it [02:14, 108.77it/s]\u001b[A\n",
      "bce: 0.0108, loss: 0.1218 ||: : 6658it [02:14, 113.03it/s]\u001b[A\n",
      "bce: 0.0084, loss: 0.1216 ||: : 6670it [02:14, 114.83it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.1213 ||: : 6683it [02:14, 118.13it/s]\u001b[A\n",
      "bce: 0.0193, loss: 0.1212 ||: : 6695it [02:19,  7.50it/s] \u001b[A\n",
      "bce: 0.0500, loss: 0.1210 ||: : 6704it [02:19, 10.34it/s]\u001b[A\n",
      "bce: 0.0111, loss: 0.1208 ||: : 6717it [02:19, 14.27it/s]\u001b[A\n",
      "bce: 0.0039, loss: 0.1206 ||: : 6730it [02:20, 19.42it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1204 ||: : 6741it [02:20, 25.48it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1202 ||: : 6754it [02:20, 33.55it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1200 ||: : 6766it [02:20, 42.26it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1200 ||: : 6777it [02:20, 51.14it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.1201 ||: : 6790it [02:20, 61.91it/s]\u001b[A\n",
      "bce: 0.0050, loss: 0.1199 ||: : 6802it [02:20, 70.96it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1200 ||: : 6814it [02:20, 80.76it/s]\u001b[A\n",
      "bce: 0.0936, loss: 0.1198 ||: : 6827it [02:20, 90.80it/s]\u001b[A\n",
      "bce: 0.0694, loss: 0.1196 ||: : 6840it [02:20, 99.42it/s]\u001b[A\n",
      "bce: 0.0282, loss: 0.1195 ||: : 6853it [02:21, 95.99it/s]\u001b[A\n",
      "bce: 0.0055, loss: 0.1194 ||: : 6866it [02:21, 102.41it/s]\u001b[A\n",
      "bce: 0.0791, loss: 0.1192 ||: : 6879it [02:21, 109.03it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1190 ||: : 6892it [02:21, 114.50it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1189 ||: : 6905it [02:21, 114.13it/s]\u001b[A\n",
      "bce: 0.0396, loss: 0.1190 ||: : 6918it [02:21, 117.90it/s]\u001b[A\n",
      "bce: 0.0102, loss: 0.1189 ||: : 6931it [02:21, 119.11it/s]\u001b[A\n",
      "bce: 0.1584, loss: 0.1187 ||: : 6944it [02:21, 120.18it/s]\u001b[A\n",
      "bce: 0.0285, loss: 0.1186 ||: : 6957it [02:21, 120.91it/s]\u001b[A\n",
      "bce: 0.0026, loss: 0.1183 ||: : 6970it [02:22, 123.22it/s]\u001b[A\n",
      "bce: 0.0185, loss: 0.1183 ||: : 6983it [02:22, 124.29it/s]\u001b[A\n",
      "bce: 1.4205, loss: 0.1183 ||: : 6996it [02:22, 124.82it/s]\u001b[A\n",
      "bce: 0.0070, loss: 0.1182 ||: : 7009it [02:27,  8.68it/s] \u001b[A\n",
      "bce: 0.1201, loss: 0.1180 ||: : 7022it [02:27, 12.04it/s]\u001b[A\n",
      "bce: 0.0592, loss: 0.1178 ||: : 7035it [02:27, 16.55it/s]\u001b[A\n",
      "bce: 0.0109, loss: 0.1177 ||: : 7048it [02:27, 22.40it/s]\u001b[A\n",
      "bce: 0.0080, loss: 0.1175 ||: : 7061it [02:27, 29.78it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.1173 ||: : 7074it [02:27, 38.62it/s]\u001b[A\n",
      "bce: 0.0364, loss: 0.1172 ||: : 7087it [02:27, 48.54it/s]\u001b[A\n",
      "bce: 0.0013, loss: 0.1170 ||: : 7100it [02:27, 59.11it/s]\u001b[A\n",
      "bce: 0.0039, loss: 0.1168 ||: : 7113it [02:27, 70.57it/s]\u001b[A\n",
      "bce: 0.0035, loss: 0.1166 ||: : 7126it [02:27, 81.17it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.1164 ||: : 7139it [02:28, 91.21it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.1163 ||: : 7152it [02:28, 99.27it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1161 ||: : 7165it [02:28, 106.46it/s]\u001b[A\n",
      "bce: 0.0065, loss: 0.1159 ||: : 7178it [02:28, 111.28it/s]\u001b[A\n",
      "bce: 0.0047, loss: 0.1157 ||: : 7191it [02:28, 116.07it/s]\u001b[A\n",
      "bce: 0.0131, loss: 0.1155 ||: : 7204it [02:28, 117.35it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.1154 ||: : 7217it [02:28, 120.03it/s]\u001b[A\n",
      "bce: 0.0211, loss: 0.1152 ||: : 7230it [02:28, 122.43it/s]\u001b[A\n",
      "bce: 0.0125, loss: 0.1151 ||: : 7243it [02:28, 124.24it/s]\u001b[A\n",
      "bce: 0.0136, loss: 0.1149 ||: : 7256it [02:28, 122.42it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.1148 ||: : 7269it [02:29, 122.15it/s]\u001b[A\n",
      "bce: 0.0300, loss: 0.1147 ||: : 7282it [02:29, 121.72it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1145 ||: : 7295it [02:29, 121.76it/s]\u001b[A\n",
      "bce: 0.0051, loss: 0.1143 ||: : 7308it [02:29, 119.48it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1142 ||: : 7321it [02:34,  7.99it/s] \u001b[A\n",
      "bce: 0.0286, loss: 0.1141 ||: : 7334it [02:34, 11.11it/s]\u001b[A\n",
      "bce: 0.0975, loss: 0.1139 ||: : 7347it [02:34, 15.31it/s]\u001b[A\n",
      "bce: 0.0019, loss: 0.1138 ||: : 7360it [02:34, 20.72it/s]\u001b[A\n",
      "bce: 0.0056, loss: 0.1136 ||: : 7373it [02:35, 27.67it/s]\u001b[A\n",
      "bce: 0.0123, loss: 0.1134 ||: : 7386it [02:35, 36.04it/s]\u001b[A\n",
      "bce: 0.0127, loss: 0.1133 ||: : 7399it [02:35, 45.93it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1132 ||: : 7412it [02:35, 56.37it/s]\u001b[A\n",
      "bce: 0.0330, loss: 0.1130 ||: : 7425it [02:35, 67.55it/s]\u001b[A\n",
      "bce: 0.0060, loss: 0.1129 ||: : 7438it [02:35, 78.33it/s]\u001b[A\n",
      "bce: 0.0054, loss: 0.1127 ||: : 7451it [02:35, 88.71it/s]\u001b[A\n",
      "bce: 0.0220, loss: 0.1126 ||: : 7464it [02:35, 97.68it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1125 ||: : 7477it [02:35, 105.25it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.1125 ||: : 7490it [02:35, 109.44it/s]\u001b[A\n",
      "bce: 0.0162, loss: 0.1124 ||: : 7503it [02:36, 112.28it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.1122 ||: : 7516it [02:36, 116.75it/s]\u001b[A\n",
      "bce: 0.0843, loss: 0.1120 ||: : 7529it [02:36, 119.99it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1118 ||: : 7542it [02:36, 119.76it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.1117 ||: : 7555it [02:36, 120.08it/s]\u001b[A\n",
      "bce: 0.0072, loss: 0.1115 ||: : 7568it [02:36, 122.69it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.1113 ||: : 7581it [02:36, 124.25it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.1111 ||: : 7594it [02:36, 125.42it/s]\u001b[A\n",
      "bce: 0.0055, loss: 0.1110 ||: : 7607it [02:36, 123.66it/s]\u001b[A\n",
      "bce: 0.0034, loss: 0.1108 ||: : 7620it [02:36, 122.57it/s]\u001b[A\n",
      "bce: 0.0078, loss: 0.1107 ||: : 7633it [02:42,  8.10it/s] \u001b[A\n",
      "bce: 0.0031, loss: 0.1105 ||: : 7644it [02:42, 11.21it/s]\u001b[A\n",
      "bce: 0.0019, loss: 0.1104 ||: : 7656it [02:42, 15.38it/s]\u001b[A\n",
      "bce: 0.0251, loss: 0.1103 ||: : 7667it [02:42, 20.71it/s]\u001b[A\n",
      "bce: 0.0062, loss: 0.1101 ||: : 7680it [02:42, 27.69it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.1100 ||: : 7693it [02:42, 36.22it/s]\u001b[A\n",
      "bce: 0.0751, loss: 0.1100 ||: : 7705it [02:42, 43.97it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1099 ||: : 7716it [02:42, 53.60it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.1098 ||: : 7728it [02:42, 63.54it/s]\u001b[A\n",
      "bce: 0.0177, loss: 0.1096 ||: : 7741it [02:43, 73.94it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1095 ||: : 7754it [02:43, 83.73it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.1094 ||: : 7767it [02:43, 92.81it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.1092 ||: : 7779it [02:43, 98.31it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1091 ||: : 7791it [02:43, 102.18it/s]\u001b[A\n",
      "bce: 0.0209, loss: 0.1089 ||: : 7803it [02:43, 99.59it/s] \u001b[A\n",
      "bce: 0.0006, loss: 0.1088 ||: : 7814it [02:43, 100.61it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1086 ||: : 7826it [02:43, 105.29it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.1085 ||: : 7838it [02:43, 104.96it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.1084 ||: : 7849it [02:44, 100.95it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1082 ||: : 7861it [02:44, 104.21it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1081 ||: : 7872it [02:44, 104.03it/s]\u001b[A\n",
      "bce: 0.0208, loss: 0.1080 ||: : 7885it [02:44, 109.80it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.1078 ||: : 7898it [02:44, 114.40it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.1077 ||: : 7910it [02:44, 115.12it/s]\u001b[A\n",
      "bce: 0.0295, loss: 0.1076 ||: : 7922it [02:44, 112.93it/s]\u001b[A\n",
      "bce: 0.0078, loss: 0.1074 ||: : 7934it [02:49,  7.29it/s] \u001b[A\n",
      "bce: 0.0066, loss: 0.1073 ||: : 7947it [02:50, 10.16it/s]\u001b[A\n",
      "bce: 0.0345, loss: 0.1072 ||: : 7960it [02:50, 14.04it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.1071 ||: : 7973it [02:50, 19.09it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.1069 ||: : 7984it [02:50, 25.09it/s]\u001b[A\n",
      "bce: 0.0069, loss: 0.1068 ||: : 7997it [02:50, 32.92it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.1067 ||: : 8009it [02:50, 41.77it/s]\u001b[A\n",
      "bce: 0.0077, loss: 0.1065 ||: : 8021it [02:50, 50.73it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.1064 ||: : 8034it [02:50, 61.67it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.1062 ||: : 8046it [02:50, 71.47it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.1061 ||: : 8059it [02:50, 82.36it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1059 ||: : 8072it [02:51, 91.01it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.1058 ||: : 8085it [02:51, 99.69it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1057 ||: : 8098it [02:51, 105.70it/s]\u001b[A\n",
      "bce: 0.0747, loss: 0.1056 ||: : 8111it [02:51, 109.91it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1054 ||: : 8124it [02:51, 115.08it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1053 ||: : 8137it [02:51, 119.17it/s]\u001b[A\n",
      "bce: 0.0117, loss: 0.1052 ||: : 8150it [02:51, 121.87it/s]\u001b[A\n",
      "bce: 0.0203, loss: 0.1051 ||: : 8163it [02:51, 123.33it/s]\u001b[A\n",
      "bce: 0.0144, loss: 0.1050 ||: : 8176it [02:51, 121.22it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1048 ||: : 8189it [02:52, 123.31it/s]\u001b[A\n",
      "bce: 0.0067, loss: 0.1047 ||: : 8202it [02:52, 122.49it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.1046 ||: : 8215it [02:52, 124.31it/s]\u001b[A\n",
      "bce: 0.0030, loss: 0.1044 ||: : 8228it [02:52, 125.62it/s]\u001b[A\n",
      "bce: 0.0084, loss: 0.1043 ||: : 8241it [02:52, 123.54it/s]\u001b[A\n",
      "bce: 0.0310, loss: 0.1041 ||: : 8254it [02:57,  8.10it/s] \u001b[A\n",
      "bce: 0.0009, loss: 0.1040 ||: : 8267it [02:57, 11.26it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0003, loss: 0.1038 ||: : 8280it [02:57, 15.49it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.1037 ||: : 8293it [02:57, 20.99it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1035 ||: : 8305it [02:57, 27.56it/s]\u001b[A\n",
      "bce: 0.0649, loss: 0.1034 ||: : 8318it [02:58, 35.87it/s]\u001b[A\n",
      "bce: 0.0203, loss: 0.1033 ||: : 8330it [02:58, 45.22it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.1031 ||: : 8343it [02:58, 55.98it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1031 ||: : 8356it [02:58, 67.10it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.1029 ||: : 8368it [02:58, 76.95it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.1028 ||: : 8381it [02:58, 87.30it/s]\u001b[A\n",
      "bce: 0.0068, loss: 0.1027 ||: : 8394it [02:58, 90.31it/s]\u001b[A\n",
      "bce: 0.0117, loss: 0.1026 ||: : 8406it [02:58, 97.06it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.1024 ||: : 8418it [02:58, 102.84it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.1023 ||: : 8431it [02:59, 107.44it/s]\u001b[A\n",
      "bce: 0.0057, loss: 0.1021 ||: : 8443it [02:59, 109.39it/s]\u001b[A\n",
      "bce: 0.0027, loss: 0.1020 ||: : 8456it [02:59, 113.98it/s]\u001b[A\n",
      "bce: 0.0058, loss: 0.1019 ||: : 8469it [02:59, 117.76it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.1018 ||: : 8482it [02:59, 120.63it/s]\u001b[A\n",
      "bce: 0.0458, loss: 0.1017 ||: : 8495it [02:59, 109.25it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.1015 ||: : 8507it [02:59, 111.78it/s]\u001b[A\n",
      "bce: 0.0032, loss: 0.1014 ||: : 8519it [02:59, 106.62it/s]\u001b[A\n",
      "bce: 0.0013, loss: 0.1013 ||: : 8531it [02:59, 109.14it/s]\u001b[A\n",
      "bce: 0.0256, loss: 0.1011 ||: : 8544it [03:00, 113.46it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.1010 ||: : 8556it [03:05,  7.65it/s] \u001b[A\n",
      "bce: 0.0013, loss: 0.1009 ||: : 8569it [03:05, 10.65it/s]\u001b[A\n",
      "bce: 0.0047, loss: 0.1007 ||: : 8581it [03:05, 14.63it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.1006 ||: : 8594it [03:05, 19.86it/s]\u001b[A\n",
      "bce: 0.0074, loss: 0.1005 ||: : 8606it [03:05, 26.34it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.1003 ||: : 8618it [03:05, 34.38it/s]\u001b[A\n",
      "bce: 0.0137, loss: 0.1002 ||: : 8631it [03:05, 44.05it/s]\u001b[A\n",
      "bce: 0.0027, loss: 0.1001 ||: : 8644it [03:05, 54.85it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0999 ||: : 8657it [03:05, 65.68it/s]\u001b[A\n",
      "bce: 0.0059, loss: 0.0998 ||: : 8670it [03:05, 76.78it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.0997 ||: : 8683it [03:06, 86.25it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0995 ||: : 8696it [03:06, 93.62it/s]\u001b[A\n",
      "bce: 0.0032, loss: 0.0994 ||: : 8709it [03:06, 97.92it/s]\u001b[A\n",
      "bce: 1.4254, loss: 0.0995 ||: : 8722it [03:06, 104.67it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.0993 ||: : 8735it [03:06, 110.53it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0992 ||: : 8748it [03:06, 115.37it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0990 ||: : 8761it [03:06, 117.17it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0989 ||: : 8774it [03:06, 120.06it/s]\u001b[A\n",
      "bce: 0.0137, loss: 0.0988 ||: : 8787it [03:06, 120.89it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.0987 ||: : 8800it [03:07, 120.21it/s]\u001b[A\n",
      "bce: 0.1184, loss: 0.0985 ||: : 8813it [03:07, 122.26it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.0984 ||: : 8826it [03:07, 120.69it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.0983 ||: : 8839it [03:07, 121.34it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0982 ||: : 8852it [03:07, 122.05it/s]\u001b[A\n",
      "bce: 0.0116, loss: 0.0981 ||: : 8865it [03:07, 120.55it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0979 ||: : 8878it [03:12,  8.02it/s] \u001b[A\n",
      "bce: 0.0000, loss: 0.0979 ||: : 8891it [03:12, 11.16it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.0978 ||: : 8902it [03:12, 15.27it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0976 ||: : 8915it [03:13, 20.75it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0975 ||: : 8928it [03:13, 27.65it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.0974 ||: : 8940it [03:13, 35.44it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0973 ||: : 8952it [03:13, 44.61it/s]\u001b[A\n",
      "bce: 0.0087, loss: 0.0972 ||: : 8964it [03:13, 52.92it/s]\u001b[A\n",
      "bce: 0.0236, loss: 0.0971 ||: : 8976it [03:13, 63.39it/s]\u001b[A\n",
      "bce: 0.0132, loss: 0.0970 ||: : 8987it [03:13, 72.11it/s]\u001b[A\n",
      "bce: 0.0037, loss: 0.0969 ||: : 8998it [03:13, 79.06it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0967 ||: : 9010it [03:13, 87.61it/s]\u001b[A\n",
      "bce: 0.0053, loss: 0.0966 ||: : 9021it [03:14, 88.03it/s]\u001b[A\n",
      "bce: 0.0077, loss: 0.0965 ||: : 9034it [03:14, 96.40it/s]\u001b[A\n",
      "bce: 0.0026, loss: 0.0964 ||: : 9047it [03:14, 102.91it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.0963 ||: : 9060it [03:14, 108.66it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0962 ||: : 9073it [03:14, 114.02it/s]\u001b[A\n",
      "bce: 0.1741, loss: 0.0961 ||: : 9086it [03:14, 116.86it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0960 ||: : 9099it [03:14, 119.99it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.0959 ||: : 9112it [03:14, 114.53it/s]\u001b[A\n",
      "bce: 0.0051, loss: 0.0957 ||: : 9125it [03:14, 117.74it/s]\u001b[A\n",
      "bce: 0.0026, loss: 0.0956 ||: : 9138it [03:14, 119.42it/s]\u001b[A\n",
      "bce: 0.0085, loss: 0.0958 ||: : 9151it [03:15, 120.17it/s]\u001b[A\n",
      "bce: 0.0027, loss: 0.0957 ||: : 9164it [03:15, 122.02it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0955 ||: : 9177it [03:20,  8.36it/s] \u001b[A\n",
      "bce: 0.0001, loss: 0.0954 ||: : 9190it [03:20, 11.62it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0953 ||: : 9202it [03:20, 15.93it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0951 ||: : 9215it [03:20, 21.60it/s]\u001b[A\n",
      "bce: 0.0429, loss: 0.0950 ||: : 9228it [03:20, 28.77it/s]\u001b[A\n",
      "bce: 0.0113, loss: 0.0949 ||: : 9240it [03:20, 37.26it/s]\u001b[A\n",
      "bce: 0.0086, loss: 0.0948 ||: : 9253it [03:20, 47.31it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0947 ||: : 9266it [03:20, 58.34it/s]\u001b[A\n",
      "bce: 0.0990, loss: 0.0946 ||: : 9279it [03:20, 69.68it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0945 ||: : 9292it [03:21, 80.67it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.0944 ||: : 9305it [03:21, 87.97it/s]\u001b[A\n",
      "bce: 0.0982, loss: 0.0943 ||: : 9318it [03:21, 95.75it/s]\u001b[A\n",
      "bce: 0.0055, loss: 0.0941 ||: : 9331it [03:21, 103.54it/s]\u001b[A\n",
      "bce: 0.0022, loss: 0.0940 ||: : 9344it [03:21, 109.62it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0940 ||: : 9357it [03:21, 112.53it/s]\u001b[A\n",
      "bce: 0.1310, loss: 0.0939 ||: : 9370it [03:21, 115.51it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0938 ||: : 9383it [03:21, 119.06it/s]\u001b[A\n",
      "bce: 0.0169, loss: 0.0937 ||: : 9396it [03:21, 121.67it/s]\u001b[A\n",
      "bce: 0.0128, loss: 0.0935 ||: : 9409it [03:21, 115.77it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0934 ||: : 9421it [03:22, 113.48it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0933 ||: : 9433it [03:22, 110.82it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0932 ||: : 9445it [03:22, 110.59it/s]\u001b[A\n",
      "bce: 0.0298, loss: 0.0932 ||: : 9457it [03:22, 108.68it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0931 ||: : 9468it [03:22, 108.75it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.0932 ||: : 9479it [03:22, 108.11it/s]\u001b[A\n",
      "bce: 0.0022, loss: 0.0931 ||: : 9490it [03:27,  7.27it/s] \u001b[A\n",
      "bce: 0.0002, loss: 0.0930 ||: : 9502it [03:27, 10.12it/s]\u001b[A\n",
      "bce: 0.0243, loss: 0.0929 ||: : 9514it [03:27, 13.94it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0928 ||: : 9527it [03:27, 19.01it/s]\u001b[A\n",
      "bce: 0.3829, loss: 0.0927 ||: : 9540it [03:27, 25.55it/s]\u001b[A\n",
      "bce: 0.0069, loss: 0.0926 ||: : 9552it [03:27, 33.06it/s]\u001b[A\n",
      "bce: 0.0053, loss: 0.0925 ||: : 9564it [03:28, 41.93it/s]\u001b[A\n",
      "bce: 0.0044, loss: 0.0924 ||: : 9576it [03:28, 50.23it/s]\u001b[A\n",
      "bce: 0.0062, loss: 0.0923 ||: : 9588it [03:28, 60.62it/s]\u001b[A\n",
      "bce: 0.0211, loss: 0.0922 ||: : 9600it [03:28, 70.65it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0921 ||: : 9613it [03:28, 81.04it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.0920 ||: : 9626it [03:28, 90.91it/s]\u001b[A\n",
      "bce: 0.0062, loss: 0.0919 ||: : 9638it [03:28, 97.72it/s]\u001b[A\n",
      "bce: 0.0052, loss: 0.0918 ||: : 9651it [03:28, 103.71it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.0917 ||: : 9663it [03:28, 107.47it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.0916 ||: : 9675it [03:29, 110.08it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.0915 ||: : 9688it [03:29, 114.04it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.0914 ||: : 9700it [03:29, 115.65it/s]\u001b[A\n",
      "bce: 0.0050, loss: 0.0913 ||: : 9713it [03:29, 119.01it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.0912 ||: : 9726it [03:29, 112.73it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0911 ||: : 9738it [03:29, 112.92it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0910 ||: : 9751it [03:29, 116.10it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0909 ||: : 9764it [03:29, 119.25it/s]\u001b[A\n",
      "bce: 0.0275, loss: 0.0908 ||: : 9777it [03:29, 121.63it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0907 ||: : 9790it [03:30, 122.11it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.0906 ||: : 9803it [03:35,  8.05it/s] \u001b[A\n",
      "bce: 0.0028, loss: 0.0905 ||: : 9816it [03:35, 11.18it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0903 ||: : 9829it [03:35, 15.40it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0902 ||: : 9842it [03:35, 20.91it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0901 ||: : 9855it [03:35, 27.92it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0900 ||: : 9868it [03:35, 36.35it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0899 ||: : 9880it [03:35, 45.86it/s]\u001b[A\n",
      "bce: 0.0061, loss: 0.0898 ||: : 9893it [03:35, 56.75it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0898 ||: : 9906it [03:35, 67.34it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0898 ||: : 9919it [03:36, 78.35it/s]\u001b[A\n",
      "bce: 0.0084, loss: 0.0896 ||: : 9932it [03:36, 88.61it/s]\u001b[A\n",
      "bce: 0.0053, loss: 0.0895 ||: : 9945it [03:36, 95.89it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0170, loss: 0.0894 ||: : 9958it [03:36, 103.50it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0893 ||: : 9971it [03:36, 109.74it/s]\u001b[A\n",
      "bce: 0.0022, loss: 0.0892 ||: : 9984it [03:36, 113.98it/s]\u001b[A\n",
      "bce: 0.0067, loss: 0.0891 ||: : 9997it [03:36, 117.60it/s]\u001b[A\n",
      "bce: 0.0200, loss: 0.0890 ||: : 10010it [03:36, 116.70it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0889 ||: : 10023it [03:36, 118.30it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.0888 ||: : 10036it [03:37, 118.88it/s]\u001b[A\n",
      "bce: 0.0065, loss: 0.0887 ||: : 10049it [03:37, 120.55it/s]\u001b[A\n",
      "bce: 0.0022, loss: 0.0886 ||: : 10062it [03:37, 122.61it/s]\u001b[A\n",
      "bce: 0.0107, loss: 0.0885 ||: : 10075it [03:37, 122.02it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0884 ||: : 10088it [03:37, 121.84it/s]\u001b[A\n",
      "bce: 0.0114, loss: 0.0883 ||: : 10101it [03:37, 121.10it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0882 ||: : 10114it [03:42,  8.37it/s] \u001b[A\n",
      "bce: 0.0001, loss: 0.0881 ||: : 10127it [03:42, 11.62it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0880 ||: : 10138it [03:42, 15.85it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0879 ||: : 10151it [03:42, 21.46it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0878 ||: : 10163it [03:42, 28.35it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.0877 ||: : 10174it [03:42, 36.47it/s]\u001b[A\n",
      "bce: 0.0468, loss: 0.0876 ||: : 10187it [03:43, 46.31it/s]\u001b[A\n",
      "bce: 0.0527, loss: 0.0875 ||: : 10200it [03:43, 56.53it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.0875 ||: : 10212it [03:43, 66.39it/s]\u001b[A\n",
      "bce: 0.2192, loss: 0.0874 ||: : 10225it [03:43, 77.26it/s]\u001b[A\n",
      "bce: 0.0140, loss: 0.0873 ||: : 10238it [03:43, 86.58it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0872 ||: : 10250it [03:43, 92.85it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0871 ||: : 10262it [03:43, 97.64it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0870 ||: : 10274it [03:43, 99.60it/s]\u001b[A\n",
      "bce: 3.4617, loss: 0.0873 ||: : 10287it [03:43, 105.91it/s]\u001b[A\n",
      "bce: 0.0034, loss: 0.0872 ||: : 10300it [03:44, 108.97it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.0871 ||: : 10312it [03:44, 102.63it/s]\u001b[A\n",
      "bce: 0.0175, loss: 0.0870 ||: : 10325it [03:44, 107.88it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0869 ||: : 10337it [03:44, 106.74it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0868 ||: : 10349it [03:44, 108.52it/s]\u001b[A\n",
      "bce: 0.0372, loss: 0.0867 ||: : 10361it [03:44, 110.01it/s]\u001b[A\n",
      "bce: 0.0349, loss: 0.0866 ||: : 10373it [03:44, 111.76it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0865 ||: : 10385it [03:44, 107.05it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.0864 ||: : 10398it [03:44, 111.36it/s]\u001b[A\n",
      "bce: 0.0107, loss: 0.0863 ||: : 10410it [03:45, 113.56it/s]\u001b[A\n",
      "bce: 0.2450, loss: 0.0863 ||: : 10422it [03:49,  7.86it/s] \u001b[A\n",
      "bce: 0.0199, loss: 0.0862 ||: : 10434it [03:50, 10.90it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0861 ||: : 10447it [03:50, 14.99it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0860 ||: : 10458it [03:50, 20.21it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0859 ||: : 10471it [03:50, 26.92it/s]\u001b[A\n",
      "bce: 0.0106, loss: 0.0859 ||: : 10484it [03:50, 35.24it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0858 ||: : 10496it [03:50, 43.59it/s]\u001b[A\n",
      "bce: 0.0030, loss: 0.0857 ||: : 10507it [03:50, 52.68it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0856 ||: : 10518it [03:50, 62.33it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0855 ||: : 10530it [03:50, 71.70it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0854 ||: : 10542it [03:51, 79.32it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0854 ||: : 10553it [03:51, 76.59it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0853 ||: : 10564it [03:51, 82.77it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0852 ||: : 10576it [03:51, 90.65it/s]\u001b[A\n",
      "bce: 0.2531, loss: 0.0851 ||: : 10588it [03:51, 97.01it/s]\u001b[A\n",
      "bce: 0.0462, loss: 0.0851 ||: : 10600it [03:51, 99.22it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0850 ||: : 10611it [03:51, 102.13it/s]\u001b[A\n",
      "bce: 0.0108, loss: 0.0849 ||: : 10624it [03:51, 107.28it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.0848 ||: : 10637it [03:51, 112.40it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0847 ||: : 10649it [03:52, 112.75it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.0846 ||: : 10662it [03:52, 116.43it/s]\u001b[A\n",
      "bce: 0.0073, loss: 0.0845 ||: : 10674it [03:52, 114.61it/s]\u001b[A\n",
      "bce: 0.0130, loss: 0.0844 ||: : 10686it [03:52, 114.02it/s]\u001b[A\n",
      "bce: 0.0025, loss: 0.0843 ||: : 10698it [03:52, 112.95it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0843 ||: : 10710it [03:52, 109.83it/s]\u001b[A\n",
      "bce: 0.0019, loss: 0.0844 ||: : 10722it [03:52, 110.12it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0843 ||: : 10734it [03:57,  7.48it/s] \u001b[A\n",
      "bce: 0.0006, loss: 0.0842 ||: : 10747it [03:57, 10.42it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.0841 ||: : 10760it [03:57, 14.39it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0840 ||: : 10771it [03:58, 19.26it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0840 ||: : 10783it [03:58, 25.74it/s]\u001b[A\n",
      "bce: 0.0426, loss: 0.0839 ||: : 10796it [03:58, 33.79it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0838 ||: : 10808it [03:58, 41.46it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0837 ||: : 10821it [03:58, 51.92it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0836 ||: : 10834it [03:58, 62.69it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0835 ||: : 10847it [03:58, 73.21it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0834 ||: : 10859it [03:58, 81.76it/s]\u001b[A\n",
      "bce: 0.0080, loss: 0.0834 ||: : 10871it [03:58, 88.99it/s]\u001b[A\n",
      "bce: 0.0348, loss: 0.0833 ||: : 10883it [03:59, 90.36it/s]\u001b[A\n",
      "bce: 0.0039, loss: 0.0833 ||: : 10894it [03:59, 92.29it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0832 ||: : 10905it [03:59, 92.07it/s]\u001b[A\n",
      "bce: 0.0235, loss: 0.0831 ||: : 10917it [03:59, 97.03it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0831 ||: : 10928it [03:59, 94.29it/s]\u001b[A\n",
      "bce: 0.0029, loss: 0.0830 ||: : 10938it [03:59, 94.89it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0829 ||: : 10950it [03:59, 100.58it/s]\u001b[A\n",
      "bce: 0.0030, loss: 0.0829 ||: : 10963it [03:59, 107.81it/s]\u001b[A\n",
      "bce: 0.0071, loss: 0.0828 ||: : 10976it [03:59, 112.02it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0827 ||: : 10989it [04:00, 116.70it/s]\u001b[A\n",
      "bce: 0.0013, loss: 0.0826 ||: : 11001it [04:00, 116.06it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0825 ||: : 11014it [04:00, 119.82it/s]\u001b[A\n",
      "bce: 0.0034, loss: 0.0825 ||: : 11028it [04:00, 123.00it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0824 ||: : 11041it [04:00, 109.14it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0823 ||: : 11053it [04:05,  8.08it/s] \u001b[A\n",
      "bce: 0.0001, loss: 0.0822 ||: : 11064it [04:05, 11.17it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0821 ||: : 11077it [04:05, 15.36it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0821 ||: : 11091it [04:05, 20.89it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0820 ||: : 11104it [04:05, 27.82it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0819 ||: : 11118it [04:05, 36.43it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.0818 ||: : 11132it [04:05, 46.57it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0817 ||: : 11145it [04:05, 56.85it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0816 ||: : 11158it [04:06, 68.31it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0815 ||: : 11171it [04:06, 77.56it/s]\u001b[A\n",
      "bce: 0.0258, loss: 0.0815 ||: : 11184it [04:06, 84.65it/s]\u001b[A\n",
      "bce: 0.0702, loss: 0.0814 ||: : 11196it [04:06, 92.00it/s]\u001b[A\n",
      "bce: 0.0230, loss: 0.0813 ||: : 11208it [04:06, 98.13it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0812 ||: : 11220it [04:06, 103.80it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0811 ||: : 11234it [04:06, 110.70it/s]\u001b[A\n",
      "bce: 0.0037, loss: 0.0811 ||: : 11247it [04:06, 113.50it/s]\u001b[A\n",
      "bce: 0.0112, loss: 0.0810 ||: : 11260it [04:06, 111.58it/s]\u001b[A\n",
      "bce: 0.0030, loss: 0.0809 ||: : 11273it [04:07, 115.41it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0808 ||: : 11286it [04:07, 117.49it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0807 ||: : 11300it [04:07, 119.27it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0806 ||: : 11313it [04:07, 119.88it/s]\u001b[A\n",
      "bce: 0.0076, loss: 0.0805 ||: : 11327it [04:07, 122.99it/s]\u001b[A\n",
      "bce: 0.0065, loss: 0.0804 ||: : 11341it [04:07, 125.84it/s]\u001b[A\n",
      "bce: 0.0174, loss: 0.0804 ||: : 11355it [04:12,  8.98it/s] \u001b[A\n",
      "bce: 0.0009, loss: 0.0803 ||: : 11367it [04:12, 12.40it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0802 ||: : 11377it [04:12, 16.63it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.0801 ||: : 11387it [04:12, 21.65it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0801 ||: : 11396it [04:13, 27.43it/s]\u001b[A\n",
      "bce: 0.0093, loss: 0.0800 ||: : 11405it [04:13, 33.26it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0800 ||: : 11413it [04:13, 39.99it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0799 ||: : 11425it [04:13, 49.63it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.0798 ||: : 11438it [04:13, 60.27it/s]\u001b[A\n",
      "bce: 0.0052, loss: 0.0797 ||: : 11450it [04:13, 70.82it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0797 ||: : 11461it [04:13, 77.64it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0796 ||: : 11472it [04:13, 76.09it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0796 ||: : 11482it [04:13, 75.35it/s]\u001b[A\n",
      "bce: 0.0061, loss: 0.0795 ||: : 11491it [04:14, 74.72it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0794 ||: : 11500it [04:14, 71.21it/s]\u001b[A\n",
      "bce: 0.0272, loss: 0.0794 ||: : 11508it [04:14, 71.33it/s]\u001b[A\n",
      "bce: 0.0023, loss: 0.0794 ||: : 11516it [04:14, 71.93it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0793 ||: : 11524it [04:14, 72.36it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0792 ||: : 11534it [04:14, 77.01it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0003, loss: 0.0792 ||: : 11547it [04:14, 86.97it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0791 ||: : 11560it [04:14, 95.55it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0790 ||: : 11571it [04:15, 87.45it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0789 ||: : 11581it [04:15, 82.48it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0789 ||: : 11590it [04:15, 79.71it/s]\u001b[A\n",
      "bce: 0.0190, loss: 0.0790 ||: : 11599it [04:15, 77.70it/s]\u001b[A\n",
      "bce: 0.0013, loss: 0.0789 ||: : 11608it [04:15, 74.55it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0789 ||: : 11616it [04:15, 74.30it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0788 ||: : 11624it [04:15, 73.79it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0787 ||: : 11632it [04:15, 73.42it/s]\u001b[A\n",
      "bce: 0.0162, loss: 0.0787 ||: : 11640it [04:15, 73.51it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0786 ||: : 11648it [04:16, 73.25it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0786 ||: : 11656it [04:16, 73.50it/s]\u001b[A\n",
      "bce: 0.0056, loss: 0.0785 ||: : 11664it [04:16, 73.56it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.0785 ||: : 11672it [04:21,  5.26it/s]\u001b[A\n",
      "bce: 0.0032, loss: 0.0784 ||: : 11680it [04:21,  7.29it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.0784 ||: : 11688it [04:21,  9.99it/s]\u001b[A\n",
      "bce: 0.0551, loss: 0.0783 ||: : 11696it [04:21, 13.48it/s]\u001b[A\n",
      "bce: 0.0058, loss: 0.0783 ||: : 11703it [04:21, 17.66it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0783 ||: : 11713it [04:21, 23.40it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.0782 ||: : 11726it [04:21, 30.93it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0782 ||: : 11739it [04:21, 39.87it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0781 ||: : 11751it [04:21, 49.84it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0780 ||: : 11764it [04:22, 60.84it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0779 ||: : 11777it [04:22, 71.75it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0779 ||: : 11790it [04:22, 82.59it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.0778 ||: : 11802it [04:22, 90.36it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0777 ||: : 11815it [04:22, 98.83it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.0776 ||: : 11828it [04:22, 104.15it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0775 ||: : 11841it [04:22, 109.18it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0774 ||: : 11854it [04:22, 112.57it/s]\u001b[A\n",
      "bce: 0.0038, loss: 0.0774 ||: : 11867it [04:22, 115.96it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0773 ||: : 11880it [04:23, 117.53it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0772 ||: : 11893it [04:23, 117.77it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0771 ||: : 11906it [04:23, 116.67it/s]\u001b[A\n",
      "bce: 0.0061, loss: 0.0772 ||: : 11919it [04:23, 119.14it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0772 ||: : 11932it [04:23, 120.26it/s]\u001b[A\n",
      "bce: 0.1207, loss: 0.0771 ||: : 11945it [04:23, 119.79it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0770 ||: : 11958it [04:23, 120.73it/s]\u001b[A\n",
      "bce: 0.0127, loss: 0.0769 ||: : 11971it [04:23, 121.08it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0769 ||: : 11984it [04:28,  8.61it/s] \u001b[A\n",
      "bce: 0.0001, loss: 0.0768 ||: : 11996it [04:28, 11.91it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0767 ||: : 12007it [04:28, 16.23it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0767 ||: : 12017it [04:28, 21.57it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.0768 ||: : 12027it [04:28, 28.06it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0767 ||: : 12038it [04:29, 36.09it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0767 ||: : 12050it [04:29, 45.43it/s]\u001b[A\n",
      "bce: 0.0073, loss: 0.0766 ||: : 12063it [04:29, 56.02it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0766 ||: : 12075it [04:29, 65.97it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0765 ||: : 12087it [04:29, 75.18it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0764 ||: : 12100it [04:29, 84.46it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0763 ||: : 12113it [04:29, 93.62it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.0763 ||: : 12126it [04:29, 100.63it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0762 ||: : 12138it [04:29, 105.44it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0762 ||: : 12151it [04:30, 109.75it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0761 ||: : 12164it [04:30, 114.52it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0760 ||: : 12177it [04:30, 116.72it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0759 ||: : 12190it [04:30, 117.54it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.0759 ||: : 12203it [04:30, 116.84it/s]\u001b[A\n",
      "bce: 0.4996, loss: 0.0758 ||: : 12216it [04:30, 119.06it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0758 ||: : 12229it [04:30, 120.00it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0757 ||: : 12242it [04:30, 119.46it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0756 ||: : 12255it [04:30, 119.48it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0755 ||: : 12268it [04:30, 121.84it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.0755 ||: : 12281it [04:31, 122.32it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0754 ||: : 12294it [04:35,  8.44it/s] \u001b[A\n",
      "bce: 0.0000, loss: 0.0754 ||: : 12306it [04:36, 11.68it/s]\u001b[A\n",
      "bce: 0.0236, loss: 0.0753 ||: : 12319it [04:36, 16.03it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0752 ||: : 12332it [04:36, 21.67it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0752 ||: : 12345it [04:36, 28.86it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0751 ||: : 12358it [04:36, 37.40it/s]\u001b[A\n",
      "bce: 0.0066, loss: 0.0750 ||: : 12371it [04:36, 47.24it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.0750 ||: : 12384it [04:36, 58.16it/s]\u001b[A\n",
      "bce: 0.0015, loss: 0.0749 ||: : 12397it [04:36, 68.99it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0748 ||: : 12410it [04:36, 78.63it/s]\u001b[A\n",
      "bce: 0.0053, loss: 0.0748 ||: : 12423it [04:37, 88.09it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0747 ||: : 12436it [04:37, 96.13it/s]\u001b[A\n",
      "bce: 0.3786, loss: 0.0747 ||: : 12449it [04:37, 103.52it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0746 ||: : 12462it [04:37, 108.45it/s]\u001b[A\n",
      "bce: 0.0120, loss: 0.0746 ||: : 12475it [04:37, 113.52it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0745 ||: : 12488it [04:37, 115.77it/s]\u001b[A\n",
      "bce: 0.0057, loss: 0.0744 ||: : 12501it [04:37, 116.14it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0743 ||: : 12514it [04:37, 117.50it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.0743 ||: : 12527it [04:37, 119.01it/s]\u001b[A\n",
      "bce: 0.0478, loss: 0.0742 ||: : 12540it [04:38, 119.71it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0742 ||: : 12553it [04:38, 121.93it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0741 ||: : 12566it [04:38, 121.77it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0740 ||: : 12579it [04:38, 121.91it/s]\u001b[A\n",
      "bce: 0.0153, loss: 0.0740 ||: : 12592it [04:38, 122.76it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0739 ||: : 12605it [04:43,  8.20it/s] \u001b[A\n",
      "bce: 0.0030, loss: 0.0738 ||: : 12616it [04:43, 11.35it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0738 ||: : 12628it [04:43, 15.55it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.0737 ||: : 12640it [04:43, 21.04it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0737 ||: : 12651it [04:43, 27.00it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.0736 ||: : 12661it [04:44, 33.40it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0735 ||: : 12672it [04:44, 42.01it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0735 ||: : 12683it [04:44, 51.57it/s]\u001b[A\n",
      "bce: 0.0099, loss: 0.0735 ||: : 12694it [04:44, 61.16it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0734 ||: : 12705it [04:44, 67.80it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0733 ||: : 12716it [04:44, 76.38it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0733 ||: : 12727it [04:44, 84.08it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0732 ||: : 12738it [04:44, 88.32it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0732 ||: : 12749it [04:44, 84.29it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0731 ||: : 12759it [04:45, 85.85it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0731 ||: : 12770it [04:45, 91.71it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0730 ||: : 12782it [04:45, 98.16it/s]\u001b[A\n",
      "bce: 0.0041, loss: 0.0730 ||: : 12794it [04:45, 102.42it/s]\u001b[A\n",
      "bce: 0.0031, loss: 0.0729 ||: : 12805it [04:45, 89.44it/s] \u001b[A\n",
      "bce: 0.0208, loss: 0.0728 ||: : 12815it [04:45, 84.93it/s]\u001b[A\n",
      "bce: 0.0022, loss: 0.0728 ||: : 12825it [04:45, 86.97it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.0727 ||: : 12836it [04:45, 91.68it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0727 ||: : 12847it [04:45, 95.34it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0726 ||: : 12860it [04:46, 101.75it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0726 ||: : 12872it [04:46, 106.07it/s]\u001b[A\n",
      "bce: 0.0013, loss: 0.0725 ||: : 12883it [04:46, 104.95it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0724 ||: : 12894it [04:46, 106.36it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0724 ||: : 12905it [04:46, 105.45it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0724 ||: : 12916it [04:51,  6.51it/s] \u001b[A\n",
      "bce: 0.0002, loss: 0.0723 ||: : 12925it [04:51,  8.98it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0723 ||: : 12936it [04:52, 12.39it/s]\u001b[A\n",
      "bce: 0.0057, loss: 0.0722 ||: : 12946it [04:52, 16.80it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0721 ||: : 12958it [04:52, 22.46it/s]\u001b[A\n",
      "bce: 0.0062, loss: 0.0721 ||: : 12968it [04:52, 28.50it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.0720 ||: : 12979it [04:52, 36.50it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0720 ||: : 12989it [04:52, 44.30it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0719 ||: : 12999it [04:52, 52.42it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0719 ||: : 13009it [04:52, 59.95it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.0718 ||: : 13020it [04:52, 68.89it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0718 ||: : 13030it [04:53, 71.73it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0717 ||: : 13040it [04:53, 75.65it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.0717 ||: : 13049it [04:53, 79.34it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0000, loss: 0.0716 ||: : 13058it [04:53, 79.23it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0716 ||: : 13067it [04:53, 78.35it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0716 ||: : 13077it [04:53, 83.55it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0715 ||: : 13088it [04:53, 89.23it/s]\u001b[A\n",
      "bce: 0.0105, loss: 0.0714 ||: : 13098it [04:53, 81.51it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0714 ||: : 13108it [04:54, 84.14it/s]\u001b[A\n",
      "bce: 0.0460, loss: 0.0713 ||: : 13117it [04:54, 73.66it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0713 ||: : 13125it [04:54, 74.26it/s]\u001b[A\n",
      "bce: 0.0145, loss: 0.0713 ||: : 13133it [04:54, 70.51it/s]\u001b[A\n",
      "bce: 0.0114, loss: 0.0712 ||: : 13141it [04:54, 68.76it/s]\u001b[A\n",
      "bce: 0.0079, loss: 0.0712 ||: : 13149it [04:54, 70.38it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0711 ||: : 13159it [04:54, 76.06it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0711 ||: : 13167it [04:54, 75.59it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0710 ||: : 13178it [04:54, 82.98it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0710 ||: : 13187it [04:55, 82.38it/s]\u001b[A\n",
      "bce: 0.0019, loss: 0.0710 ||: : 13196it [04:55, 78.37it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.0709 ||: : 13205it [04:55, 74.42it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0709 ||: : 13214it [04:55, 76.10it/s]\u001b[A\n",
      "bce: 0.0037, loss: 0.0708 ||: : 13222it [05:02,  3.65it/s]\u001b[A\n",
      "bce: 0.0067, loss: 0.0708 ||: : 13230it [05:02,  5.10it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0707 ||: : 13240it [05:02,  7.13it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0707 ||: : 13252it [05:02,  9.92it/s]\u001b[A\n",
      "bce: 0.0022, loss: 0.0706 ||: : 13261it [05:02, 13.48it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0706 ||: : 13270it [05:03, 17.80it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0705 ||: : 13278it [05:03, 22.95it/s]\u001b[A\n",
      "bce: 0.0009, loss: 0.0705 ||: : 13286it [05:03, 28.76it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0705 ||: : 13294it [05:03, 34.82it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.0704 ||: : 13302it [05:03, 39.14it/s]\u001b[A\n",
      "bce: 0.0016, loss: 0.0704 ||: : 13309it [05:03, 44.17it/s]\u001b[A\n",
      "bce: 0.0554, loss: 0.0704 ||: : 13316it [05:03, 49.31it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0704 ||: : 13323it [05:03, 51.00it/s]\u001b[A\n",
      "bce: 0.0179, loss: 0.0704 ||: : 13330it [05:04, 49.11it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0703 ||: : 13336it [05:04, 49.48it/s]\u001b[A\n",
      "bce: 0.0011, loss: 0.0703 ||: : 13342it [05:04, 48.89it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0703 ||: : 13348it [05:04, 49.98it/s]\u001b[A\n",
      "bce: 0.0055, loss: 0.0702 ||: : 13354it [05:04, 49.35it/s]\u001b[A\n",
      "bce: 0.0012, loss: 0.0702 ||: : 13363it [05:04, 55.84it/s]\u001b[A\n",
      "bce: 0.0052, loss: 0.0702 ||: : 13370it [05:04, 54.07it/s]\u001b[A\n",
      "bce: 0.0312, loss: 0.0701 ||: : 13376it [05:04, 54.97it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.0701 ||: : 13382it [05:04, 54.90it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0701 ||: : 13388it [05:05, 56.12it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0700 ||: : 13394it [05:05, 57.18it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0700 ||: : 13400it [05:05, 51.49it/s]\u001b[A\n",
      "bce: 0.0018, loss: 0.0700 ||: : 13406it [05:05, 50.86it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0700 ||: : 13412it [05:05, 52.50it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0699 ||: : 13418it [05:05, 51.41it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0699 ||: : 13424it [05:05, 50.04it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0699 ||: : 13430it [05:05, 49.82it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0698 ||: : 13436it [05:06, 50.17it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0698 ||: : 13442it [05:06, 50.86it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0698 ||: : 13448it [05:06, 53.19it/s]\u001b[A\n",
      "bce: 0.0031, loss: 0.0698 ||: : 13454it [05:06, 52.21it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0697 ||: : 13460it [05:06, 52.71it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.0697 ||: : 13466it [05:06, 52.89it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0697 ||: : 13472it [05:06, 52.08it/s]\u001b[A\n",
      "bce: 0.0137, loss: 0.0696 ||: : 13478it [05:06, 53.35it/s]\u001b[A\n",
      "bce: 0.0473, loss: 0.0696 ||: : 13484it [05:06, 54.38it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0696 ||: : 13490it [05:07, 53.63it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0695 ||: : 13497it [05:07, 57.55it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0695 ||: : 13503it [05:07, 55.70it/s]\u001b[A\n",
      "bce: 0.0028, loss: 0.0695 ||: : 13509it [05:07, 54.98it/s]\u001b[A\n",
      "bce: 0.1298, loss: 0.0695 ||: : 13515it [05:07, 55.53it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0694 ||: : 13521it [05:07, 54.38it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0694 ||: : 13527it [05:07, 55.02it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0694 ||: : 13533it [05:14,  2.65it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0693 ||: : 13542it [05:15,  3.73it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0693 ||: : 13549it [05:15,  5.21it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0693 ||: : 13557it [05:15,  7.23it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0693 ||: : 13565it [05:15,  9.92it/s]\u001b[A\n",
      "bce: 0.1869, loss: 0.0693 ||: : 13572it [05:15, 13.28it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.0692 ||: : 13579it [05:15, 16.94it/s]\u001b[A\n",
      "bce: 0.0042, loss: 0.0692 ||: : 13586it [05:15, 21.35it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0692 ||: : 13592it [05:15, 26.28it/s]\u001b[A\n",
      "bce: 0.0221, loss: 0.0691 ||: : 13599it [05:16, 31.63it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0691 ||: : 13605it [05:16, 35.80it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0691 ||: : 13611it [05:16, 39.12it/s]\u001b[A\n",
      "bce: 0.0072, loss: 0.0691 ||: : 13617it [05:16, 42.77it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0690 ||: : 13623it [05:16, 46.24it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0690 ||: : 13629it [05:16, 48.91it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0690 ||: : 13635it [05:16, 50.48it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0689 ||: : 13641it [05:16, 52.54it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0689 ||: : 13647it [05:16, 48.44it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0689 ||: : 13653it [05:17, 48.44it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0689 ||: : 13659it [05:17, 50.20it/s]\u001b[A\n",
      "bce: 0.0097, loss: 0.0689 ||: : 13666it [05:17, 53.63it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0688 ||: : 13673it [05:17, 56.82it/s]\u001b[A\n",
      "bce: 0.0010, loss: 0.0688 ||: : 13679it [05:17, 56.20it/s]\u001b[A\n",
      "bce: 0.0034, loss: 0.0688 ||: : 13685it [05:17, 55.96it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.0688 ||: : 13692it [05:17, 58.80it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.0687 ||: : 13699it [05:17, 61.13it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0687 ||: : 13706it [05:17, 61.76it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0686 ||: : 13713it [05:18, 62.98it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0686 ||: : 13720it [05:18, 58.68it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0686 ||: : 13726it [05:18, 58.77it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0686 ||: : 13732it [05:18, 56.23it/s]\u001b[A\n",
      "bce: 0.0140, loss: 0.0685 ||: : 13738it [05:18, 54.42it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0685 ||: : 13744it [05:18, 53.69it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0685 ||: : 13750it [05:18, 49.83it/s]\u001b[A\n",
      "bce: 0.0041, loss: 0.0684 ||: : 13756it [05:18, 49.45it/s]\u001b[A\n",
      "bce: 0.0249, loss: 0.0684 ||: : 13762it [05:19, 49.76it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0684 ||: : 13769it [05:19, 52.08it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0684 ||: : 13775it [05:19, 51.64it/s]\u001b[A\n",
      "bce: 0.0220, loss: 0.0683 ||: : 13781it [05:19, 51.88it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0683 ||: : 13788it [05:19, 55.21it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0683 ||: : 13797it [05:19, 61.93it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0682 ||: : 13804it [05:19, 62.60it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0682 ||: : 13813it [05:19, 68.43it/s]\u001b[A\n",
      "bce: 0.0344, loss: 0.0681 ||: : 13823it [05:19, 74.58it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0681 ||: : 13831it [05:20, 71.08it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0681 ||: : 13840it [05:20, 75.43it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.0680 ||: : 13848it [05:26,  4.06it/s]\u001b[A\n",
      "bce: 0.0031, loss: 0.0680 ||: : 13856it [05:26,  5.67it/s]\u001b[A\n",
      "bce: 0.0305, loss: 0.0679 ||: : 13865it [05:26,  7.86it/s]\u001b[A\n",
      "bce: 0.0097, loss: 0.0679 ||: : 13875it [05:26, 10.85it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0679 ||: : 13885it [05:26, 14.73it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0678 ||: : 13894it [05:26, 19.55it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.0678 ||: : 13903it [05:27, 25.04it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0677 ||: : 13911it [05:27, 31.41it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0677 ||: : 13920it [05:27, 38.59it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0677 ||: : 13930it [05:27, 46.87it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0676 ||: : 13941it [05:27, 56.04it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0676 ||: : 13950it [05:27, 60.10it/s]\u001b[A\n",
      "bce: 0.0067, loss: 0.0675 ||: : 13959it [05:27, 65.54it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0675 ||: : 13969it [05:27, 70.93it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0674 ||: : 13978it [05:28, 71.13it/s]\u001b[A\n",
      "bce: 0.0004, loss: 0.0674 ||: : 13987it [05:28, 69.56it/s]\u001b[A\n",
      "bce: 0.0105, loss: 0.0673 ||: : 13995it [05:28, 67.02it/s]\u001b[A\n",
      "bce: 0.0040, loss: 0.0673 ||: : 14003it [05:28, 69.76it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0673 ||: : 14012it [05:28, 74.07it/s]\u001b[A\n",
      "bce: 0.1348, loss: 0.0673 ||: : 14020it [05:28, 70.00it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0672 ||: : 14028it [05:28, 66.01it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.0672 ||: : 14035it [05:28, 66.54it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bce: 0.0012, loss: 0.0672 ||: : 14042it [05:28, 66.51it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0671 ||: : 14052it [05:29, 73.31it/s]\u001b[A\n",
      "bce: 0.0175, loss: 0.0671 ||: : 14060it [05:29, 73.22it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0671 ||: : 14069it [05:29, 77.51it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0670 ||: : 14078it [05:29, 79.84it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0670 ||: : 14089it [05:29, 85.45it/s]\u001b[A\n",
      "bce: 0.0014, loss: 0.0669 ||: : 14098it [05:29, 84.31it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0669 ||: : 14107it [05:29, 82.79it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0669 ||: : 14118it [05:29, 87.96it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0668 ||: : 14130it [05:29, 93.73it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0668 ||: : 14141it [05:30, 95.55it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0667 ||: : 14151it [05:30, 87.99it/s]\u001b[A\n",
      "bce: 0.0054, loss: 0.0667 ||: : 14161it [05:36,  5.14it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0666 ||: : 14168it [05:36,  7.07it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0666 ||: : 14175it [05:36,  9.65it/s]\u001b[A\n",
      "bce: 0.0425, loss: 0.0666 ||: : 14182it [05:36, 12.99it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0666 ||: : 14189it [05:36, 17.13it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0665 ||: : 14196it [05:36, 22.00it/s]\u001b[A\n",
      "bce: 0.0024, loss: 0.0665 ||: : 14203it [05:37, 26.00it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0665 ||: : 14209it [05:37, 31.07it/s]\u001b[A\n",
      "bce: 0.0008, loss: 0.0665 ||: : 14216it [05:37, 37.01it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0664 ||: : 14223it [05:37, 42.66it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0664 ||: : 14230it [05:37, 46.10it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0664 ||: : 14237it [05:37, 50.46it/s]\u001b[A\n",
      "bce: 0.0005, loss: 0.0663 ||: : 14244it [05:37, 54.51it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0663 ||: : 14251it [05:37, 53.36it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.0663 ||: : 14258it [05:37, 57.12it/s]\u001b[A\n",
      "bce: 0.0017, loss: 0.0662 ||: : 14265it [05:38, 59.68it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0662 ||: : 14272it [05:38, 62.39it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0662 ||: : 14279it [05:38, 64.23it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0662 ||: : 14286it [05:38, 64.24it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0661 ||: : 14293it [05:38, 62.01it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0661 ||: : 14300it [05:38, 55.28it/s]\u001b[A\n",
      "bce: 0.0003, loss: 0.0661 ||: : 14307it [05:38, 58.84it/s]\u001b[A\n",
      "bce: 0.0006, loss: 0.0660 ||: : 14314it [05:38, 61.53it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0660 ||: : 14321it [05:38, 62.96it/s]\u001b[A\n",
      "bce: 0.0044, loss: 0.0660 ||: : 14328it [05:39, 64.25it/s]\u001b[A\n",
      "bce: 0.0067, loss: 0.0660 ||: : 14335it [05:39, 65.24it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0659 ||: : 14342it [05:39, 61.94it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0659 ||: : 14349it [05:39, 58.21it/s]\u001b[A\n",
      "bce: 0.0007, loss: 0.0659 ||: : 14359it [05:39, 65.86it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0658 ||: : 14370it [05:39, 74.62it/s]\u001b[A\n",
      "bce: 0.0111, loss: 0.0658 ||: : 14381it [05:39, 81.76it/s]\u001b[A\n",
      "bce: 0.0020, loss: 0.0657 ||: : 14391it [05:39, 84.74it/s]\u001b[A\n",
      "bce: 0.0021, loss: 0.0657 ||: : 14400it [05:39, 83.34it/s]\u001b[A\n",
      "bce: 0.0253, loss: 0.0657 ||: : 14411it [05:40, 89.68it/s]\u001b[A\n",
      "bce: 0.0001, loss: 0.0656 ||: : 14423it [05:40, 95.09it/s]\u001b[A\n",
      "bce: 0.0041, loss: 0.0656 ||: : 14435it [05:40, 99.68it/s]\u001b[A\n",
      "bce: 0.0002, loss: 0.0655 ||: : 14446it [05:40, 93.15it/s]\u001b[A\n",
      "bce: 0.0000, loss: 0.0655 ||: : 14457it [05:40, 97.58it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "iterator = BucketIterator(batch_size=batch_size, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab_new)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=word2vec_dataset,\n",
    "                  # validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=2,\n",
    "                  cuda_device=0)\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader=ds)\n",
    "\n",
    "vec = predictor.predict(\"Cartman\")['vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_word(vec, model, vocab):\n",
    "    score = vec.view(1, -1).expand_as(model.word_embedding.weight)*model.word_embedding.weight\n",
    "    score = score.sum(dim=-1)\n",
    "    norm = vec.view(1, -1).expand_as(model.word_embedding.weight).norm(dim=1)*model.word_embedding.weight.norm(dim=1)\n",
    "    word_idx = (score / norm).argmax().numpy()\n",
    "    print(word_idx)\n",
    "    word = vocab.get_token_from_index(int(word_idx))\n",
    "    return word\n",
    "\n",
    "out = predictor.predict(\"cartman asshole girl\")['vec']\n",
    "print(out)\n",
    "vec = torch.tensor(out[0]) - torch.tensor(out[1]) #+ torch.tensor(out[2])\n",
    "print(out[0], vec)\n",
    "#vec = (out[0] + out[1]).unsqueeze(0)\n",
    "print(get_closest_word(vec, model, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "\n",
    "tag_logits = predictor.predict(\"The dog ate the apple\")['tag_logits']\n",
    "\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/home/jendrik/git/DLWorkshop/data/all-seasons.csv'\n",
    "print_lines(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 2\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "ls = tokenizer.tokenize(corpus.iloc[0]['Line'])\n",
    "print(ls)\n",
    "for sentence in ls:\n",
    "    print([lmtzr.lemmatize(word) for word in nltk.word_tokenize(sentence) if word.isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "vocab = {' ': 1}\n",
    "corpus_len = 0\n",
    "for i, row in corpus.iterrows():\n",
    "    ls = tokenizer.tokenize(row['Line'])\n",
    "    for sentence in ls:\n",
    "        stemmed_sentence = [lmtzr.lemmatize(word.lower()) for word in nltk.word_tokenize(sentence) if word.isalpha()]\n",
    "        if len(stemmed_sentence) > 2:\n",
    "            corpus_len += len(stemmed_sentence)\n",
    "            for word in stemmed_sentence:\n",
    "                try:\n",
    "                    vocab[word] += 1\n",
    "                except KeyError:\n",
    "                    vocab[word] = 1\n",
    "            sentences.append(stemmed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab.keys()))\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dict = {}\n",
    "sample_dict = {}\n",
    "for k, v in vocab.items():\n",
    "    z = v/corpus_len\n",
    "    drop_dict[word_to_ix[k]] = (np.sqrt(z*1e3) + 1)*1e-3/z\n",
    "    sample_dict[word_to_ix[k]] = (v/corpus_len)**0.75\n",
    "sample_dict_sum = sum(sample_dict.values())\n",
    "for k in sample_dict:\n",
    "    sample_dict[k] /= sample_dict_sum\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs).mean(1)#.view((inputs.shape[0], -1))\n",
    "        #out = self.relu(self.linear1(embeds))\n",
    "        return out\n",
    "    \n",
    "    def log_probs(self, out):\n",
    "        out = self.linear(out)\n",
    "        print(out.shape)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "class SkipGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear1 = nn.Linear(embedding_dim, vec_size)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        return embeds\n",
    "    \n",
    "    def log_probs(self, out):\n",
    "        out = self.linear(out)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "class NGramsLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, vec_size)\n",
    "        self.linear2 = nn.Linear(vec_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))\n",
    "        out = self.relu(self.linear1(embeds))\n",
    "        return out\n",
    "    \n",
    "    def log_probs(self, out):\n",
    "        out = self.linear2(out)\n",
    "        print(out.shape)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "class SkipGramLanguageModelerSparse(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size, embedding_freq, negative_sampling_size):\n",
    "        super().__init__()\n",
    "        self.embedding_freq = torch.autograd.Variable(torch.Tensor(embedding_freq)**(0.75))\n",
    "        self.negative_sampling_size = negative_sampling_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        #self.linear1 = nn.Linear(embedding_dim, vec_size)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #return embedding\n",
    "        return self.word_embedding(inputs) + self.context_embedding(inputs)\n",
    "    \n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def calculate_loss(self, word, context,alpha=0.1):\n",
    "        #word.size() batch_size\n",
    "        #context.size() batch_size*window_size\n",
    "        word_pos = word.view(-1,1).expand_as(context)\n",
    "        word_pos = self.word_embedding(word_pos)\n",
    "        context = self.context_embedding(context)\n",
    "        product_pos = (word_pos*context).sum(dim=-1).mean()\n",
    "        target_pos = torch.autograd.Variable(torch.ones(product_pos.size()))\n",
    "        loss_positive = nn.functional.binary_cross_entropy_with_logits(product_pos, target_pos)\n",
    "        negative_context = self.embedding_freq.multinomial(\n",
    "            self.negative_sampling_size*context.size(0)*context.size(1), replacement=True).to(self.device())\n",
    "        negative_context = negative_context.view(-1,self.negative_sampling_size*context.size(1))\n",
    "        word_neg = word.view(-1,1).expand_as(negative_context)\n",
    "        word_neg = self.word_embedding(word_neg)\n",
    "        negative_context = self.context_embedding(negative_context)\n",
    "        product_neg = (word_neg*negative_context).sum(dim=-1).mean()\n",
    "        target_neg = torch.autograd.Variable(torch.zeros(product_neg.size()))\n",
    "        loss_negative = nn.functional.binary_cross_entropy_with_logits(product_neg,target_neg)\n",
    "        loss = loss_positive + loss_negative\n",
    "        return loss\n",
    "    \n",
    "    def log_probs(self, inp, out, neg):\n",
    "        losses = []\n",
    "        inp = self.embeddings(inp)\n",
    "        out = self.embeddings(out)\n",
    "        neg = self.embeddings(neg)\n",
    "        score = torch.mul(inp, out)\n",
    "        print(score.shape)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        print(score.shape)\n",
    "        score = nn.functional.logsigmoid(score)\n",
    "        print(score.shape)\n",
    "        neg_score = torch.bmm(neg, inp.unsqueeze(2)).squeeze()\n",
    "        print(neg_score.shape)\n",
    "        neg_score = torch.sum(neg_score, dim=0)\n",
    "        print(neg_score.shape)\n",
    "        neg_score = nn.functional.logsigmoid(-1 * neg_score)\n",
    "        print(neg_score.shape)\n",
    "        losses.append(sum(neg_score))\n",
    "        losses.append(sum(score))\n",
    "        input()\n",
    "        return -1 * sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(sentences, context_size, word_to_idx, batch_size, mode):\n",
    "    selected_sentences = np.random.choice(sentences, batch_size)\n",
    "    inp_tokens = []\n",
    "    out_tokens = []\n",
    "    for sentence in selected_sentences:\n",
    "        if mode == 'cbow':\n",
    "            selected_word_idx = np.random.randint(context_size, len(sentence)-context_size)\n",
    "        else:\n",
    "            selected_word_idx = np.random.randint(context_size, len(sentence))\n",
    "        context = [word_to_idx[word] for word in sentence[selected_word_idx-context_size:selected_word_idx]]\n",
    "        while(len(context) < context_size):\n",
    "            context.insert(0, word_to_idx[' '])\n",
    "        if mode == 'cbow':\n",
    "            post_context = [word_to_idx[word] for word in sentence[selected_word_idx+1:selected_word_idx+1+context_size]]\n",
    "            while(len(post_context) < context_size):\n",
    "                post_context.append(word_to_idx[' '])\n",
    "            context.extend(post_context)\n",
    "        inp_tokens.append(np.array(context))\n",
    "        out_tokens.append(np.array(word_to_idx[sentence[selected_word_idx]]))\n",
    "    return np.array(inp_tokens), np.array(out_tokens)\n",
    "\n",
    "def get_batch_w2v(sentences, context_size, word_to_idx, batch_size, drop_dict, neg_samples):\n",
    "    selected_sentences = np.random.choice(sentences, batch_size)\n",
    "    vec = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    out_tokens = []\n",
    "    len_longest_sentence = 0\n",
    "    for sentence in selected_sentences:\n",
    "        selected_word_idx = np.random.randint(0, len(sentence))\n",
    "        drop = drop_dict[word_to_idx[sentence[selected_word_idx]]] < np.random.rand()\n",
    "        while drop:\n",
    "            sentence = np.random.choice(sentences)\n",
    "            selected_word_idx = np.random.randint(0, len(sentence))\n",
    "            drop = drop_dict[word_to_idx[sentence[selected_word_idx]]] < np.random.rand()\n",
    "        pos_words = sentence[0:selected_word_idx]\n",
    "        pos_words.extend(sentence[selected_word_idx+1:])\n",
    "        len_longest_sentence = len(pos_words) if len_longest_sentence < len(pos_words) else len_longest_sentence\n",
    "        pos_indices = [word_to_idx[word] for word in pos_words]\n",
    "        vec.append(selected_word_idx)\n",
    "        pos.append(pos_indices)\n",
    "    pos_padded = []\n",
    "    for pos_indices in pos:\n",
    "        padded = [word_to_idx[' '] for i in range(len_longest_sentence-len(pos_indices))]\n",
    "        padded.extend(pos_indices)\n",
    "        pos_padded.append(padded)\n",
    "    return np.array(vec), np.array(pos_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "batch_size = 128\n",
    "n_batches = 64#len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "neg_samples = 20\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipGramLanguageModelerSparse(len(vocab), embedding_dims, context_size, vec_dims, list(vocab.values()), 10)\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        vec, pos = get_batch_w2v(sentences, context_size, word_to_ix,\n",
    "                                               batch_size, drop_dict, neg_samples=neg_samples)\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        vec = torch.from_numpy(vec).long().to(device)\n",
    "        pos = torch.from_numpy(pos).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        loss = model.calculate_loss(vec, pos)\n",
    "\n",
    "        \n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}', end='\\r')\n",
    "    print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}')\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "batch_size = 128\n",
    "n_batches = len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramsLanguageModeler(len(vocab), embedding_dims, context_size, vec_dims)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        inp_tokens, out_tokens = get_batch(sentences, context_size, word_to_ix, batch_size, mode='ngrams')\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.from_numpy(inp_tokens).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        out = model(context_idxs)\n",
    "        log_probs = model.log_probs(out)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.from_numpy(out_tokens).long().to(device))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}', end='\\r')\n",
    "    print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}')\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cuda available {torch.cuda.is_available()}')\n",
    "embedding_dims = 128\n",
    "batch_size = 128\n",
    "n_batches = len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipGramLanguageModeler(len(vocab), embedding_dims, context_size, vec_dims)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        out_tokens, inp_tokens = get_batch(sentences, context_size, word_to_ix, batch_size, mode='cbow')\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.from_numpy(inp_tokens).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        out = model(context_idxs)\n",
    "        log_probs = model.log_probs(out)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = None\n",
    "        for j in range(out_tokens.shape[1]):\n",
    "            try:\n",
    "                loss += loss_function(log_probs, torch.from_numpy(out_tokens[:, j]).long().to(device))\n",
    "            except TypeError:\n",
    "                loss = loss_function(log_probs, torch.from_numpy(out_tokens[:, j]).long().to(device))\n",
    "        loss = loss/out_tokens.shape[1]\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}', end='\\r')\n",
    "    print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}')\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "context_size = 2\n",
    "batch_size = 128\n",
    "n_batches = len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOWLanguageModeler(len(vocab), embedding_dims, context_size, vec_dims)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        inp_tokens, out_tokens = get_batch(sentences, context_size, word_to_ix, batch_size, mode='cbow')\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.from_numpy(inp_tokens).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        out = model(context_idxs)\n",
    "        log_probs = model.log_probs(out)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.from_numpy(out_tokens).long().to(device))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss/n_batches)\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cosine_distance(id0, id1, model):\n",
    "    out = model(torch.from_numpy(np.array([id0, id1])).long().to(device))\n",
    "    sk_sim = cosine_similarity(out.data.cpu().numpy())\n",
    "    return (out[0] * out[1] / (out[0].norm() * out[1].norm())).sum(), sk_sim\n",
    "    \n",
    "print(cosine_distance(word_to_ix['nice'], word_to_ix['good'], model))\n",
    "print(cosine_distance(word_to_ix['fantastic'], word_to_ix['good'], model))\n",
    "print(cosine_distance(word_to_ix['terrible'], word_to_ix['good'], model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman_id = word_to_ix['man']\n",
    "asshole_id = word_to_ix['boy']\n",
    "poor_id = word_to_ix['girl']\n",
    "print([cartman_id, asshole_id, poor_id])\n",
    "out = model(torch.from_numpy(np.array([cartman_id, asshole_id, poor_id])).long().to(device))\n",
    "print(out.shape)\n",
    "print(out[1] - out[2])\n",
    "vec = out[0] - out[1] + out[2]\n",
    "print(out[0], vec)\n",
    "#vec = (out[0] + out[1]).unsqueeze(0)\n",
    "print(vec.shape)\n",
    "print(model.word_embedding.weight.shape)\n",
    "score = vec.view(1, -1).expand_as(model.word_embedding.weight)*model.word_embedding.weight\n",
    "score = score.sum(dim=-1)\n",
    "norm = vec.view(1, -1).expand_as(model.word_embedding.weight).norm(dim=1)*model.word_embedding.weight.norm(dim=1)\n",
    "print(score.shape)\n",
    "print(norm.shape)\n",
    "print(score)\n",
    "print(norm)\n",
    "word = (score / norm)\n",
    "print(word.shape)\n",
    "print(word)\n",
    "word = word.argmax()\n",
    "print(ix_to_word[int(word.cpu().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_name = \"cornell_movie-dialogs_corpus\"\n",
    "corpus = os.path.join(\"../data\", corpus_name)\n",
    "\n",
    "def print_lines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "print_lines(os.path.join(corpus, \"movie_lines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each line of the file into a dictionary of fields\n",
    "def load_lines(file_name, fields):\n",
    "    lines = []\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            line_dict = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                line_dict[field] = values[i]\n",
    "            lines.append(line_dict)\n",
    "    return pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_lines = [\"line_id\", \"character_id\", \"movie_id\", \"character\", \"text\"]\n",
    "header_conversations = [\"character1_id\", \"character2_id\", \"movie_id\", \"utterance_id\"]\n",
    "\n",
    "# Load lines and process conversations\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = load_lines(os.path.join(corpus, \"movie_lines.txt\"), header_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\n",
    "def load_conversations(file_name, lines, fields):\n",
    "    conversations = []\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            conversation = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                conversation[field] = values[i]\n",
    "            # Convert string to list (conversation[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
    "            line_ids = eval(conversation[\"utteranceIDs\"])\n",
    "            # Reassemble lines\n",
    "            conversation[\"lines\"] = []\n",
    "            for line_id in line_ids:\n",
    "                conversation[\"lines\"].append(lines.loc[lines['line_id'] == line_id])\n",
    "            conversations.append(conversation)\n",
    "    return pd.DataFrame(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_conversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
    "                                  lines, header_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
