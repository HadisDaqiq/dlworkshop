{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "Let us implement a perceptron.\n",
    "As a reminder, our perceptron takes multiple inputs, weights each of them with a certain factor and checks if the sum is bigger than a threshold.\n",
    "![Perceptron](./images/perceptron.png)\n",
    "First we are going to collect the data to run this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ~/data/workshop_data\n",
    "! wget -c --retry-connrefused --tries=0 https://archive.ics.uci.edu/ml/machine-learning-databases/00357/occupancy_data.zip -O ~/data/workshop_data/occupancy_data.zip\n",
    "! unzip ~/data/workshop_data/occupancy_data.zip -d ~/data/workshop_data/occupancy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the relevant packages\n",
    "# matplotlib for plots\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "# pandas to read in some data\n",
    "import pandas as pd\n",
    "# numpy to build our first perceptron\n",
    "import numpy as np\n",
    "# Train test split to do validate our findings from the perceptron training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# MinMaxScaler to normalise the data before inputting them to the perceptron\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "import os\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/occupancy_data/datatraining.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy Detection Dataset\n",
    "For training the perceptron we will utilise the [occupancy detection dataset](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html?task=&area=&type=ts&view=table). It contatins experimental data for binary classification  if a person or not is in a room given temperature, humidity, light and CO$_2$.\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the occupancy data so we have something to predict\n",
    "df = pd.read_csv(data)\n",
    "target = 'Occupancy'\n",
    "# Let us drop the date for the time being as it is no cyclic feature, meaning it will not appear again\n",
    "features = [col for col in df.columns if target not in col and 'date' not in col]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.min(), df.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test\n",
    "First we will split the data to validate what we learned on a dataset that we haven't seen before. We will utilize [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from the sklean package for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[features], df[target].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "We will normalize the data to be in a range from 0 to 1. This makes sure that all weights are in the same order of magnitude. Otherwise, the perceptron would need to learn the range of the data first and then how to separate the data best.\n",
    "\n",
    "Let's have a look at an example for this:\n",
    "- The lights minimum is 0 and its maximum is 1546.33. If we start with an inital weight between 0 and 1 the output would be as well between 0 and 1546.33.\n",
    "- Looking at the Humidity ratio its minimum is 0.00267 and its maximum is 0.00647601. If we start with an inital weight between 0 and 1 the output would be between 0.00267 and its maximum is 0.00647601.\n",
    "- The issue is, that we now sum outputs on each other. In the worst case the initial contribution of the light could be $1546.33/0.00267 \\approx 500000$ higher. \n",
    "- Now, the algorithm would first need to learn to decrease the weight for the light a lot and increase the one for the humidity ratio a lot. \n",
    "- This can be avoided if we scale all of the features to be between 0 and 1. This can be easily done using sklearn's [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "- We have to make sure, that we only fit the scaler on the training data and only transform the test data. Otherwise, the learned weights of the perceptron would have another meaning for the training vs the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the perceptron\n",
    "To build and train a perceptron we have to perform three steps:\n",
    "- Calculate the perceptron's output $\\hat{y} = \\left(\\sum_i w_i X_i \\geq 0\\right)$ (this can be done in numpy using np.dot [docs](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html))\n",
    "- Determine the update for the weights using the error and the learning rate: $\\partial w_i = \\alpha (y-\\hat{y}) X_i$\n",
    "- Calculate new weights as: $w_i \\leftarrow w_i + \\partial w_i$\n",
    "- Repeat the above steps until there occur no more updates (we will iterate once over the dataset instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes weights\n",
    "w = np.random.rand(len(features))\n",
    "print(\"initial weights: {}\".format(w))\n",
    "# set a learning rare\n",
    "alpha = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perceptron_output(w, x):\n",
    "    # Calculate the perceptrons output using \n",
    "    # np.dot(w, x) to calculate the sum and\n",
    "    # thresholding the output using >=0\n",
    "    # Hint: You will need to use .astype(float) to cast \n",
    "    # the output to a float\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_update(alpha, y, y_hat, x):\n",
    "    # Calculate the update of the weights output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, delta_w):\n",
    "    # Add the weight change to the current weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(x_train, y_train.values):\n",
    "    y_hat = calculate_perceptron_output(w, x)\n",
    "    delta_w = calculate_weight_update(alpha, y, y_hat, x)\n",
    "    w = update_weights(w, delta_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "expected = []\n",
    "for x in x_train:\n",
    "    results.append(calculate_perceptron_output(w, x))\n",
    "results = np.array(results)\n",
    "expected = np.array(y_train.values)\n",
    "print(\"final weights: {}\".format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy: {}\".format(np.mean(results == expected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us change to PyTorch\n",
    "\n",
    "To use PyTorch instead of numpy we need to replace all calls to numpy with calls to PyTorch.\n",
    "- We need to import torch\n",
    "- np.dot needs to be replaced with [torch.dot](https://pytorch.org/docs/stable/torch.html#torch.dot)\n",
    "- numpy arrays need to be converted to torch tensors using [torch.from_numpy](https://pytorch.org/docs/stable/torch.html#torch.from_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perceptron_output_torch(w, x):\n",
    "    # Calculate the perceptrons output using \n",
    "    # using torch.dot instead of np.dot.\n",
    "    # Hint do not need to cast the output anymore.\n",
    "    # torch is great isn't it? :) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(len(features))\n",
    "alpha = np.array(alpha)\n",
    "# Convert w, alpha, x_train and y_train to torch tensors\n",
    "w = \n",
    "alpha = \n",
    "x_ttrain = \n",
    "y_ttrain = \n",
    "print(\"initial weights: {}\".format(w))\n",
    "for x, y in zip(x_ttrain, y_ttrain):\n",
    "    # Use the new torch function to calculate the perceptron's output\n",
    "    y_hat = calculate_perceptron_output_torch(w, x)\n",
    "    # The weight update works as before as no specific calls\n",
    "    # to numpy were made\n",
    "    delta_w = calculate_weight_update(alpha, y, y_hat, x)\n",
    "    w = update_weights(w, delta_w)\n",
    "print(\"final weights: {}\".format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "expected = []\n",
    "for x, y in zip(x_ttrain, y_ttrain):\n",
    "    result = calculate_perceptron_output_torch(w, x)\n",
    "    expected.append(y)\n",
    "    results.append(result)\n",
    "results = torch.stack(results)\n",
    "expected = torch.stack(expected)\n",
    "print(\"weights: {}\".format(w))\n",
    "print(\"accuracy: {}\".format((results == expected.byte()).float().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[df.Occupancy==1].plot(x='CO2', y='Light', ls='', marker='o', ms=3, color='r', label='occupied')\n",
    "df[df.Occupancy==0].plot(x='CO2', y='Light', ls='', marker='o', ms=3, color='b', ax=ax, label='empty')\n",
    "ax.set_ylabel('Light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
