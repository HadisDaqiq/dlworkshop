{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the relevant packages\n",
    "# matplotlib for plots\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "# pandas to read in some data\n",
    "import pandas as pd\n",
    "# numpy to build our first perceptron\n",
    "import numpy as np\n",
    "# Train test split to do validate our findings from the perceptron training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# MinMaxScaler to normalise the data before inputting them to the perceptron\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/occupancy_data/datatraining.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>426.0</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714.00</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-04 17:53:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2450</td>\n",
       "      <td>426.0</td>\n",
       "      <td>713.50</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-04 17:54:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-04 17:55:00</td>\n",
       "      <td>23.10</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>704.50</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Temperature  Humidity  Light     CO2  HumidityRatio  \\\n",
       "1  2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   \n",
       "2  2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   \n",
       "3  2015-02-04 17:53:00        23.15   27.2450  426.0  713.50       0.004779   \n",
       "4  2015-02-04 17:54:00        23.15   27.2000  426.0  708.25       0.004772   \n",
       "5  2015-02-04 17:55:00        23.10   27.2000  426.0  704.50       0.004757   \n",
       "\n",
       "   Occupancy  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "5          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the occupancy data so we have something to predict\n",
    "df = pd.read_csv(data)\n",
    "target = 'Occupancy'\n",
    "features = [col for col in df.columns if target not in col and 'date' not in col]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6414\n",
      "1    1729\n",
      "Name: Occupancy, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[target].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[features], df[target], shuffle=False)\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70391174, 0.27549041, 0.42478485, 0.98861629],\n",
       "       [0.99282297, 0.70361083, 0.27775383, 0.41480207, 0.98416176],\n",
       "       [0.99282297, 0.70210632, 0.27549041, 0.4141136 , 0.98230597],\n",
       "       [0.99282297, 0.69909729, 0.27549041, 0.40688468, 0.97859448],\n",
       "       [0.98086124, 0.69909729, 0.27549041, 0.40172117, 0.97182167]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "Name: Occupancy, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the artificial neuron\n",
    "To build and train a neuron we have to perform three steps:\n",
    "- Calculate the perceptron's output $\\hat{y} = \\sigma\\left(\\sum_i w_i X_i)\\right)$\n",
    "- Determine the error: $E(w) = \\frac12 \\sum_{(x,y) \\in D} (y-a)^2$\n",
    "- Calculate the weight gradient with: $\\sum_{(x,y) \\in D} (y-a)$\n",
    "- Repeat the above steps until there occur no more updates (we will iterate once over the dataset instead)\n",
    "\n",
    "PyTorch abstracts neural networks using the nn.Module class. Every neural network has to subclass from it for PyTorch mechanisms to work perfecty. Let us start by using this to build out neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs):\n",
    "        super().__init__()\n",
    "        # Build the neuron using nn.Linear\n",
    "        self.neuron = nn.Linear(number_of_inputs, 1, bias=True)\n",
    "        # use nn.Sigmoid as an activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def logit(self, inp):\n",
    "        return self.neuron(inp)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return self.act(self.logit(inp))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now select a random selection of the training data and calculate the gradients for the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter neuron.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1715,  0.3174,  0.3605, -0.3260,  0.3555]], requires_grad=True)\n",
      "Gradient tensor([[ 1.4013e-01,  2.2882e-01, -6.9648e-03, -1.2235e-04,  1.8078e-01]])\n",
      "Parameter neuron.bias\n",
      "Parameter containing:\n",
      "tensor([0.3760], requires_grad=True)\n",
      "Gradient tensor([0.4765])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.BCEWithLogitsLoss()\n",
    "neuron = Neuron(5)\n",
    "select = np.random.randint(0, len(x_train), 2014)\n",
    "x = torch.from_numpy(x_train[select]).float()\n",
    "y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1)\n",
    "y_logits = neuron.logit(x)\n",
    "err = loss(y_logits, y)\n",
    "err.backward()\n",
    "for name, param in neuron.named_parameters():\n",
    "    print(f'Parameter {name}\\n{param}\\nGradient {param.grad}')\n",
    "    param = param - 5e-2*param.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(neuron.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7944799661636353\n",
      "accuracy 0.8125585913658142\n",
      "accuracy 0.8136743307113647\n",
      "accuracy 0.8149487376213074\n",
      "accuracy 0.8429931402206421\n",
      "accuracy 0.8713037371635437\n",
      "accuracy 0.8879150152206421\n",
      "accuracy 0.9013134837150574\n",
      "accuracy 0.9093286395072937\n",
      "accuracy 0.9180591106414795\n",
      "accuracy 0.9254174828529358\n",
      "accuracy 0.9308105707168579\n",
      "accuracy 0.9352245926856995\n",
      "accuracy 0.9386865496635437\n",
      "accuracy 0.9410717487335205\n",
      "accuracy 0.9428198337554932\n",
      "accuracy 0.943359375\n",
      "accuracy 0.9434033036231995\n",
      "accuracy 0.94366455078125\n",
      "accuracy 0.9439501762390137\n",
      "Training time: 2.1997478008270264\n"
     ]
    }
   ],
   "source": [
    "def fit_batch(optim, loss, neuron, x, y):\n",
    "    optim.zero_grad()\n",
    "    y_pred = neuron.logit(x)\n",
    "    #print(y, y_pred, y.sum())\n",
    "    err = loss(y_pred, y)\n",
    "    #err = err * (y * 3 + 1)\n",
    "    err.mean().backward()\n",
    "    optim.step()\n",
    "    return y_pred\n",
    "\n",
    "start = time.time()  \n",
    "for i in range(20):\n",
    "    acc = None\n",
    "    for i in range(200):\n",
    "        select = np.random.randint(0, len(x_train), 2048)\n",
    "        x = torch.from_numpy(x_train[select]).float()\n",
    "        y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1)\n",
    "        y_pred = fit_batch(optim, loss, neuron, x, y)\n",
    "        if acc is None:\n",
    "            acc = (y==(y_pred > .5).float()).float().mean()\n",
    "        else:\n",
    "            acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        #y_pred = y_pred.argmax(dim=-1)\n",
    "        #acc += (y==y_pred).float().mean()\n",
    "    print(f'accuracy {acc/200}')\n",
    "print(f'Training time: {time.time() - start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we use the logits function instead of calling forward including the sigmoid function?\n",
    "Chaining a Sigmoid and the Cross Entropy Loss can lead to instabilities, if calculated numerically. \n",
    "This can be solved analytically and is done directly in the BCELoss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move the neuron to the GPU\n",
    "PyTorch tensors and modules allow us to call .cuda() on them to move the computations to the GPU.\n",
    "This makes it really easy to perform any calculation on the GPU (which is super handy even if you do not use neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81170166015625\n",
      "0.81325439453125\n",
      "0.81336181640625\n",
      "0.8250390625\n",
      "0.8614404296875\n",
      "0.88064697265625\n",
      "0.896796875\n",
      "0.9060791015625\n",
      "0.91286865234375\n",
      "0.92316650390625\n",
      "0.9283251953125\n",
      "0.9327001953125\n",
      "0.93753662109375\n",
      "0.93989990234375\n",
      "0.94085205078125\n",
      "0.94246337890625\n",
      "0.94280029296875\n",
      "0.9421484375\n",
      "0.9435986328125\n",
      "0.94410400390625\n",
      "Training time: 3.7515981197357178\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    neuron = Neuron(5).cuda()\n",
    "    optim = torch.optim.SGD(neuron.parameters(), lr=5e-2)\n",
    "    start = time.time()\n",
    "    for i in range(20):\n",
    "        acc = None\n",
    "        for i in range(200):\n",
    "            select = np.random.randint(0, len(x_train), 2048)\n",
    "            x = torch.from_numpy(x_train[select]).float().cuda()\n",
    "            y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1).cuda()\n",
    "            y_pred = fit_batch(optim, loss, neuron, x, y)\n",
    "            if acc is None:\n",
    "                acc = (y==(y_pred > .5).float()).float().mean()\n",
    "            else:\n",
    "                acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        print(f'{acc.data.cpu().numpy()/200}')\n",
    "    print(f'Training time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the GPU version slower?\n",
    "\n",
    "Well, we need to move the data to the GPU and back. This costs us time. It normally pays off, as the computations take way longer than moving the data. In our current case the computation is very simple and the amount of data very small. This nothing the GPU is well suited for, because it can not use its advantage of performing a lot of computations in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
