{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let us implement training a neural network using gradient descent in PyTorch.\n",
    "\n",
    "PyTorch utilizes its autograd mechanism [docs](https://pytorch.org/docs/stable/notes/autograd.html) to calculate the gradients for every parameter in a computaton graph automatically, given an error.\n",
    "For this we will:\n",
    "- Calculate the output of a neuron given its current weights\n",
    "- Calculate the error with a given label\n",
    "- Let PyTorch figure out the gradients using .backward()\n",
    "- apply the gradients with $w \\leftarrow w - \\alpha * w.grad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the relevant packages\n",
    "# matplotlib for plots\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "# pandas to read in some data\n",
    "import pandas as pd\n",
    "# numpy to build our first perceptron\n",
    "import numpy as np\n",
    "# Train test split to do validate our findings from the perceptron training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# MinMaxScaler to normalise the data before inputting them to the perceptron\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/occupancy_data/datatraining.txt'\n",
    "# Let us load the data from the previous example\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "target = 'Occupancy'\n",
    "features = [col for col in df.columns if target not in col and 'date' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df[features], df[target])\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the artificial neuron\n",
    "To build and train a neuron we have to perform three steps:\n",
    "- Calculate the perceptron's output $\\hat{y} = \\sigma\\left(\\sum_i w_i X_i)\\right)$\n",
    "- Determine the error: $E(w) = \\frac12 \\sum_{(x,y) \\in D} (y-a)^2$\n",
    "- Calculate the weight gradient with: $\\sum_{(x,y) \\in D} (y-a)$\n",
    "- Repeat the above steps until there occur no more updates (we will iterate once over the dataset instead)\n",
    "\n",
    "PyTorch abstracts neural networks using the nn.Module class. Every neural network has to subclass from it for PyTorch's mechanisms to work perfecty. In addition, every layer has to be a member of the network's class. Otherwise the weights do not appear as parameters of the network. Let us start by building a single neuron within a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs):\n",
    "        super().__init__()\n",
    "        # Build the neuron using nn.Linear\n",
    "        self.neuron = nn.Linear(number_of_inputs, 1, bias=True)\n",
    "        # use nn.Sigmoid as an activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def logit(self, inp):\n",
    "        return self.neuron(inp)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return self.act(self.logit(inp))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now select a random selection of the training data and calculate the gradients for the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCEWithLogitsLoss()\n",
    "neuron = Neuron(5)\n",
    "select = np.random.randint(0, len(x_train), 2014)\n",
    "x = torch.from_numpy(x_train[select]).float()\n",
    "y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1)\n",
    "y_logits = neuron.logit(x)\n",
    "err = loss(y_logits, y)\n",
    "err.backward()\n",
    "for name, param in neuron.named_parameters():\n",
    "    print(f'Parameter {name}\\n{param}\\nGradient {param.grad}')\n",
    "    param = param - 5e-2*param.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(neuron.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(optim, loss, neuron, x, y):\n",
    "    optim.zero_grad()\n",
    "    y_pred = neuron.logit(x)\n",
    "    #print(y, y_pred, y.sum())\n",
    "    err = loss(y_pred, y)\n",
    "    #err = err * (y * 3 + 1)\n",
    "    err.mean().backward()\n",
    "    optim.step()\n",
    "    return y_pred\n",
    "\n",
    "def eval_batch(neuron, x):\n",
    "    y_pred = neuron.logit(x)\n",
    "    return y_pred\n",
    "\n",
    "start = time.time()  \n",
    "for i in range(20):\n",
    "    acc = None\n",
    "    for i in range(200):\n",
    "        select = np.random.randint(0, len(x_train), 2048)\n",
    "        x = torch.from_numpy(x_train[select]).float()\n",
    "        y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1)\n",
    "        y_pred = fit_batch(optim, loss, neuron, x, y)\n",
    "        if acc is None:\n",
    "            acc = (y==(y_pred > .5).float()).float().mean()\n",
    "        else:\n",
    "            acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        #y_pred = y_pred.argmax(dim=-1)\n",
    "        #acc += (y==y_pred).float().mean()\n",
    "    print(f'accuracy {acc/200}')\n",
    "print(f'Training time: {time.time() - start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we use the logits function instead of calling forward including the sigmoid function?\n",
    "Chaining a Sigmoid and the Cross Entropy Loss can lead to instabilities, if calculated numerically. \n",
    "This can be solved analytically and is done directly in the BCELoss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move the neuron to the GPU\n",
    "PyTorch tensors and modules allow us to call .cuda() on them to move the computations to the GPU.\n",
    "This makes it really easy to perform any calculation on the GPU (which is super handy even if you do not use neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    neuron = Neuron(5).cuda()\n",
    "    optim = torch.optim.SGD(neuron.parameters(), lr=5e-2)\n",
    "    start = time.time()\n",
    "    for i in range(20):\n",
    "        acc = None\n",
    "        for i in range(200):\n",
    "            select = np.random.randint(0, len(x_train), 2048)\n",
    "            x = torch.from_numpy(x_train[select]).float().cuda()\n",
    "            y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1).cuda()\n",
    "            y_pred = fit_batch(optim, loss, neuron, x, y)\n",
    "            if acc is None:\n",
    "                acc = (y==(y_pred > .5).float()).float().mean()\n",
    "            else:\n",
    "                acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        print(f'accuracy {acc.data.cpu().numpy()/200}')\n",
    "    print(f'Training time: {time.time() - start}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the GPU version slower?\n",
    "\n",
    "Well, we need to move the data to the GPU and back. This costs us time. It normally pays off, as the computations take way longer than moving the data. In our current case the computation is very simple and the amount of data very small. This nothing the GPU is well suited for, because it can not use its advantage of performing a lot of computations in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
