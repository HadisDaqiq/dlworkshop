{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the relevant packages\n",
    "# matplotlib for plots\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "# pandas to read in some data\n",
    "import pandas as pd\n",
    "# numpy to build our first perceptron\n",
    "import numpy as np\n",
    "# Train test split to do validate our findings from the perceptron training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# MinMaxScaler to normalise the data before inputting them to the perceptron\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/occupancy_data/datatraining.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>426.0</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714.00</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-04 17:53:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2450</td>\n",
       "      <td>426.0</td>\n",
       "      <td>713.50</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-04 17:54:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-04 17:55:00</td>\n",
       "      <td>23.10</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>704.50</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Temperature  Humidity  Light     CO2  HumidityRatio  \\\n",
       "1  2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   \n",
       "2  2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   \n",
       "3  2015-02-04 17:53:00        23.15   27.2450  426.0  713.50       0.004779   \n",
       "4  2015-02-04 17:54:00        23.15   27.2000  426.0  708.25       0.004772   \n",
       "5  2015-02-04 17:55:00        23.10   27.2000  426.0  704.50       0.004757   \n",
       "\n",
       "   Occupancy  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "5          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the occupancy data so we have something to predict\n",
    "df = pd.read_csv(data)\n",
    "target = 'Occupancy'\n",
    "features = [col for col in df.columns if target not in col and 'date' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df[features], df[target], shuffle=False)\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network\n",
    "To extend our previously build neuron to a neural network, we will need to add a second (third, fourth) linear layer.\n",
    "\n",
    "The first layer needs to output as many layers as the second one consumes. Try 10 for the time being.\n",
    "You will need to update the logits and forward function as well to pass through all layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs, hidden_units):\n",
    "        super().__init__()\n",
    "        # Build the neuron using nn.Linear\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_units, bias=True)\n",
    "        self.linear2 = nn.Linear(hidden_units, 1, bias=True)\n",
    "        # use nn.Sigmoid as an activation function\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.act2 = nn.Sigmoid()\n",
    "    \n",
    "    def logit(self, inp):\n",
    "        out_layer1 = self.act1(self.linear1(inp))\n",
    "        return self.linear2(out_layer1)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return self.act2(self.logit(inp))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now select a random selection of the training data and calculate the gradients for the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCEWithLogitsLoss()\n",
    "net = Network(5, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(net.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "train accuracy 0.85386962890625\n",
      "train accuracy 0.96170166015625\n",
      "train accuracy 0.97273681640625\n",
      "train accuracy 0.976767578125\n",
      "train accuracy 0.97940185546875\n",
      "train accuracy 0.980791015625\n",
      "train accuracy 0.98091552734375\n",
      "train accuracy 0.98175537109375\n",
      "train accuracy 0.981826171875\n",
      "train accuracy 0.98212890625\n",
      "train accuracy 0.98299560546875\n",
      "train accuracy 0.98311767578125\n",
      "train accuracy 0.983017578125\n",
      "train accuracy 0.982998046875\n",
      "train accuracy 0.9829833984375\n",
      "train accuracy 0.98308349609375\n",
      "train accuracy 0.98350341796875\n",
      "train accuracy 0.983505859375\n",
      "train accuracy 0.9838427734375\n",
      "train accuracy 0.984228515625\n",
      "val accuracy 0.9228880405426025\n",
      "Training time: 9.295231580734253\n"
     ]
    }
   ],
   "source": [
    "def fit_batch(optim, loss, net, x, y):\n",
    "    net.train()\n",
    "    optim.zero_grad()\n",
    "    y_pred = net.logit(x)\n",
    "    #print(y, y_pred, y.sum())\n",
    "    err = loss(y_pred, y)\n",
    "    #err = err * (y * 3 + 1)\n",
    "    err.mean().backward()\n",
    "    optim.step()\n",
    "    return y_pred\n",
    "\n",
    "def eval_batch(net, x):\n",
    "    net.eval()\n",
    "    y_pred = net(x)\n",
    "    return y_pred\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = Network(5, 10).cuda()\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "    start = time.time()\n",
    "    for i in range(20):\n",
    "        acc = None\n",
    "        for i in range(200):\n",
    "            select = np.random.randint(0, len(x_train), 2048)\n",
    "            x = torch.from_numpy(x_train[select]).float().cuda()\n",
    "            y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1).cuda()\n",
    "            y_pred = fit_batch(optim, loss, net, x, y)\n",
    "            if acc is None:\n",
    "                acc = (y==(y_pred > .5).float()).float().mean()\n",
    "            else:\n",
    "                acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        print(f'train accuracy {acc.data.cpu().numpy()/200}')\n",
    "    \n",
    "    x = torch.from_numpy(x_val).float().cuda()\n",
    "    y = torch.from_numpy(y_val.values).float().unsqueeze(1).cuda()\n",
    "    y_pred = eval_batch(net, x)\n",
    "    acc = (y==(y_pred > .5).float()).float().mean()\n",
    "    print(f'val accuracy {acc.data.cpu().numpy()}')\n",
    "    \n",
    "    print(f'Training time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the result change with a chaning network?\n",
    "\n",
    "Now try using a bigger layersize and try adding dropout.\n",
    "How can we change the training and validation loss?\n",
    "\n",
    "- What happens if we add Dropout? [docs](https://pytorch.org/docs/stable/nn.html#dropout)\n",
    "- What happens if you add momentum or weight decay to SGD? [docs](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)\n",
    "- What happens if you use an Adam optimizer instead of SGD? [docs](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)\n",
    "- What happens if we use other activation functions? [docs](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "Hint You can add dropout using:\n",
    "```\n",
    "    self.dropout = nn.Dropout(how_many_percent_shall_be_dropped)\n",
    "    \n",
    "    def logits(x):\n",
    "        out_layer1 = self.dropout(self.act1(self.linear1(x)))\n",
    "        ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
