{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Pollution\n",
    "\n",
    "Let us use a neural network to predict the air polution at US embassy in Beijing using the following data from UCI MAchine Learning repository: https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -c --retry-connrefused --tries=0 https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv -O ~/data/workshop_data/PRSA_data_2010.1.1-2014.12.31.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import nn\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/PRSA_data_2010.1.1-2014.12.31.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(year, month, day, hour):\n",
    "    return datetime(int(year), int(month), int(day), int(hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data, parse_dates=[['year', 'month', 'day', 'hour']], date_parser=parse_date)\n",
    "df = df.drop('No', axis=1)\n",
    "df.columns = ['date', 'pollution', 'dew_temp', 'temp', 'pressure', 'wind_dir', 'wind_speed', 'snow', 'rain']\n",
    "df.index = df['date']\n",
    "df = df.drop('date', axis=1)\n",
    "df = df[24:]\n",
    "df['pollution'] = df['pollution'].fillna(df['pollution'].median())\n",
    "df['wind_dir'] = df['wind_dir'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,9))\n",
    "for i in range(len(df.columns)):\n",
    "    plt.subplot(len(df.columns), 1, i+1)\n",
    "    plt.plot(df[df.columns[i]])\n",
    "    plt.title(df.columns[i], loc='right', y =.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "We want to predict the olusion in the next our, given the previous context data.\n",
    "So let us first add a target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['pollution'].shift(-1)\n",
    "df = df[:-1]\n",
    "features = [col for col in df.columns if col != 'target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test\n",
    "We can not shuffle the data for random splits anymore, as we will train sequentailly.\n",
    "Therefore, we define a date at which we split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = datetime(2014, 1, 1)\n",
    "#val_end = datetime(2014, 7, 1)\n",
    "train = df[:train_end]\n",
    "#val = df[train_end:val_end]\n",
    "test = df[train_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_nd(a, window, steps = None, axis = None, outlist = False):\n",
    "        \"\"\"\n",
    "        Create a windowed view over `n`-dimensional input that uses an \n",
    "        `m`-dimensional window, with `m <= n`.\n",
    "        \n",
    "    \n",
    "        Parameters\n",
    "        -------------\n",
    "        a : Array-like\n",
    "            The array to create the view on\n",
    "    \n",
    "        window : tuple or int\n",
    "            If int, the size of the window in `axis`, or in all dimensions if \n",
    "            `axis == None`\n",
    "    \n",
    "            If tuple, the shape of the desired window.  `window.size` must be:\n",
    "                equal to `len(axis)` if `axis != None`, else \n",
    "                equal to `len(a.shape)`, or \n",
    "                1\n",
    "    \n",
    "        steps : tuple, int or None\n",
    "            The offset between consecutive windows in desired dimension\n",
    "            If None, offset is one in all dimensions\n",
    "            If int, the offset for all windows over `axis`\n",
    "            If tuple, the steps along each `axis`.  \n",
    "                `len(steps)` must me equal to `len(axis)`\n",
    "    \n",
    "        axis : tuple, int or None\n",
    "            The axes over which to apply the window\n",
    "            If None, apply over all dimensions\n",
    "            if tuple or int, the dimensions over which to apply the window\n",
    "    \n",
    "        outlist : boolean\n",
    "            If output should be as list of windows.  \n",
    "            If False, it will be an array with \n",
    "                `a.nidim + 1 <= a_view.ndim <= a.ndim *2`.  \n",
    "            If True, output is a list of arrays with `a_view[0].ndim = a.ndim`\n",
    "                Warning: this is a memory-intensive copy and not a view\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "    \n",
    "        a_view : ndarray\n",
    "            A windowed view on the input array `a`, or copied list of windows   \n",
    "    \n",
    "        \"\"\"\n",
    "        ashp = np.array(a.shape)\n",
    "    \n",
    "        if axis != None:\n",
    "            axs = np.array(axis, ndmin = 1)\n",
    "            assert np.all(np.in1d(axs, np.arange(ashp.size))), \"Axes out of range\"\n",
    "        else:\n",
    "            axs = np.arange(ashp.size)\n",
    "    \n",
    "        window = np.array(window, ndmin = 1)\n",
    "        assert (window.size == axs.size) | (window.size == 1), \"Window dims and axes don't match\"\n",
    "        wshp = ashp.copy()\n",
    "        wshp[axs] = window\n",
    "        assert np.all(wshp <= ashp), \"Window is bigger than input array in axes\"\n",
    "    \n",
    "        stp = np.ones_like(ashp)\n",
    "        if steps:\n",
    "            steps = np.array(steps, ndmin = 1)\n",
    "            assert np.all(steps > 0), \"Only positive steps allowed\"\n",
    "            assert (steps.size == axs.size) | (steps.size == 1), \"Steps and axes don't match\"\n",
    "            stp[axs] = steps\n",
    "    \n",
    "        astr = np.array(a.strides)\n",
    "        shape = tuple((ashp - wshp) // stp + 1) + tuple(wshp)\n",
    "        strides = tuple(astr * stp) + tuple(astr)\n",
    "    \n",
    "        as_strided = np.lib.stride_tricks.as_strided\n",
    "        a_view = np.squeeze(as_strided(a, \n",
    "                                     shape = shape, \n",
    "                                     strides = strides))\n",
    "        #print(astr, strides, shape, a_view)\n",
    "        if outlist:\n",
    "            return list(a_view.reshape((-1,) + tuple(wshp)))\n",
    "        else:\n",
    "            return a_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale\n",
    "Neural networks work best if the data have the same input scale. To achieve this, we will use sklearn's MinMaxScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(np.float32)\n",
    "scaler = MinMaxScaler()\n",
    "past_samples = 10\n",
    "x_train = window_nd(scaler.fit_transform(train[features]), past_samples, 1, axis=0)\n",
    "x_test = window_nd(scaler.transform(test[features]), past_samples, 1, axis=0)\n",
    "y_train = scaler.fit_transform(train['target'].values.reshape(-1, 1))\n",
    "y_test = scaler.transform(test['target'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by building a [recurrent neural network](https://pytorch.org/docs/stable/nn.html#torch.nn.RNN). \n",
    "We will put the output of the recurrent layer through an activation function into a linear layer to get a single value from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        # Build the recurrent parts using nn.RNN\n",
    "        self.rnn = \n",
    "        # Use a ReLU as an activation\n",
    "        self.act = \n",
    "        # Use a linear output layer\n",
    "        self.out = \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # implement the rest of the forward function\n",
    "        output, x = self.rnn(inp)\n",
    "        x = self.act(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(optim, loss, net, x, y):\n",
    "    optim.zero_grad()\n",
    "    y_pred = net(x)\n",
    "    err = loss(y_pred, y)\n",
    "    err.mean().backward()\n",
    "    optim.step()\n",
    "    return y_pred, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "batch_size = 128\n",
    "batches_per_epoch = 5\n",
    "epochs = 20\n",
    "net = RNNNet(x_train.shape[-1], 25, 1).to(device)\n",
    "loss = nn.L1Loss()\n",
    "optim = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "start = time.time()  \n",
    "for epoch in range(epochs):\n",
    "    train_err = None\n",
    "    for j in range(batches_per_epoch):\n",
    "        select = np.random.randint(0, len(x_train), batch_size)\n",
    "        x = torch.from_numpy(x_train[select]).float().to(device)\n",
    "        y = torch.from_numpy(y_train[select]).float().unsqueeze(1).to(device)\n",
    "        y_pred, err = fit_batch(optim, loss, net, x, y)\n",
    "        if train_err is None:\n",
    "            train_err = err\n",
    "        else:\n",
    "            train_err += err\n",
    "        #y_pred = y_pred.argmax(dim=-1)\n",
    "        #acc += (y==y_pred).float().mean()\n",
    "    x = torch.from_numpy(x_test).float().to(device)\n",
    "    y = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n",
    "    y_pred = net(x)\n",
    "    test_err = loss(y_pred, y)\n",
    "    print(f'Epoch {epoch} train_loss {train_err/batches_per_epoch} test_loss {test_err}')\n",
    "print(f'Training time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use an [LSTM](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM) instead of a simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        # Build the recurrent parts using nn.RNN\n",
    "        self.rnn = \n",
    "        # Use a ReLU as an activation\n",
    "        self.act = \n",
    "        # Use a linear output layer\n",
    "        self.out = \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # implement the rest of the forward function\n",
    "        output, x = self.rnn(inp)\n",
    "        x = self.act(x[0])\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(46)\n",
    "net = LSTMNet(x_train.shape[-1], 25, 1).to(device)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "start = time.time()  \n",
    "for epoch in range(epochs):\n",
    "    train_err = None\n",
    "    for j in range(batches_per_epoch):\n",
    "        select = np.random.randint(0, len(x_train), batch_size)\n",
    "        x = torch.from_numpy(x_train[select]).float().to(device)\n",
    "        y = torch.from_numpy(y_train[select]).float().unsqueeze(1).to(device)\n",
    "        y_pred, err = fit_batch(optim, loss, net, x, y)\n",
    "        if train_err is None:\n",
    "            train_err = err\n",
    "        else:\n",
    "            train_err += err\n",
    "        #y_pred = y_pred.argmax(dim=-1)\n",
    "        #acc += (y==y_pred).float().mean()\n",
    "    x = torch.from_numpy(x_test).float().to(device)\n",
    "    y = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n",
    "    y_pred = net(x)\n",
    "    test_err = loss(y_pred, y)\n",
    "    print(f'Epoch {epoch} train_loss {train_err/batches_per_epoch} test_loss {test_err}')\n",
    "print(f'Training time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are overfitting, let us add [dropout](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) to the LSTM network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetWithDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        # Build the recurrent parts using nn.RNN\n",
    "        self.rnn = \n",
    "        # Use a ReLU as an activation\n",
    "        self.act = \n",
    "        # Use a linear output layer\n",
    "        self.out = \n",
    "        # Build the recurrent parts using nn.RNN\n",
    "        self.dropout = \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # implement the rest of the forward function\n",
    "        output, x = self.rnn(inp)\n",
    "        x = self.act(self.dropout(x[0]))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "net = LSTMNetWithDropout(x_train.shape[-1], 25, 1, .2).to(device)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "start = time.time()  \n",
    "batches_per_epoch_new = batches_per_epoch * 4\n",
    "for epoch in range(epochs):\n",
    "    train_err = None\n",
    "    for j in range(batches_per_epoch_new):\n",
    "        select = np.random.randint(0, len(x_train), batch_size)\n",
    "        x = torch.from_numpy(x_train[select]).float().to(device)\n",
    "        y = torch.from_numpy(y_train[select]).float().unsqueeze(1).to(device)\n",
    "        y_pred, err = fit_batch(optim, loss, net, x, y)\n",
    "        if train_err is None:\n",
    "            train_err = err\n",
    "        else:\n",
    "            train_err += err\n",
    "        #y_pred = y_pred.argmax(dim=-1)\n",
    "        #acc += (y==y_pred).float().mean()\n",
    "    x = torch.from_numpy(x_test).float().to(device)\n",
    "    y = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n",
    "    y_pred = net(x)\n",
    "    test_err = loss(y_pred, y)\n",
    "    print(f'Epoch {epoch} train_loss {train_err/batches_per_epoch_new} test_loss {test_err}')\n",
    "print(f'Training time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
