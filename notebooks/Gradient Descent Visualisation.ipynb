{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "from matplotlib import pyplot as plt\n",
    "torch.random.manual_seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(100) - .5\n",
    "y = x * .2 + .2*(torch.rand(100)-.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss\n",
    "loss = torch.nn.MSELoss()\n",
    "# Generate weight array and their loss for visualization purposes\n",
    "w_arr = torch.from_numpy(np.arange(-.3, 1, .01)).float()\n",
    "r2_arr = ((w_arr.unsqueeze(1)*x - y)**2).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set inital weight\n",
    "w = torch.tensor(.9, requires_grad=True)\n",
    "for i in range(20):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    # select points for this training period\n",
    "    minibatch_indices = np.random.randint(0, 100, 10)\n",
    "    # calculate model's estimate\n",
    "    y_exp = w * x[minibatch_indices]\n",
    "    # calculate the error\n",
    "    err = loss(y_exp, y[minibatch_indices])\n",
    "    # use PyTorch's autograd to get the gradients\n",
    "    err.backward()\n",
    "    # Calculate the loss curve for the current minibatch for visualisation\n",
    "    r2_arr_mini = ((w_arr.unsqueeze(1)*x[minibatch_indices] - y[minibatch_indices])**2).mean(1)\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(x.numpy(), y.numpy(), ls='', marker = 'o', label='all data')\n",
    "    plt.plot(x[minibatch_indices].numpy(), y[minibatch_indices].numpy(), ls='', \n",
    "             marker = 'o', label='current batch')\n",
    "    plt.plot(x.numpy(), (w*x).data.numpy(), ls='-', marker = '', label='model')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(w_arr.numpy(), r2_arr.numpy(), label='global loss')\n",
    "    plt.plot(w_arr.numpy(), r2_arr_mini.numpy(), label='minibatch loss')\n",
    "    plt.plot(w.data.numpy(), err.data.numpy(), ls='', marker = 'o', ms=10, label='current loss')\n",
    "    plt.arrow(w.data.numpy(), err.data.numpy(), -w.grad.numpy(), 0)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    w = (w - w.grad).clone().detach().requires_grad_(True)\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
