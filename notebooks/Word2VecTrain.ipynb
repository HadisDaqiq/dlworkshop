{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import multiprocessing as mp\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import functools\n",
    "from typing import Dict, List, Iterator, Union, Callable\n",
    "# In AllenNLP each training example is represented as an Instance containing Fields\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.models import Model\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.data.tokenizers import WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n",
    "\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "import time\n",
    "import re\n",
    "from os.path import expanduser\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "home = expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset():\n",
    "\n",
    "    def __init__(self, data_series: List[pd.Series], csvs: List[str], \n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, \n",
    "                 tokenizer: WordTokenizer = SpacyWordSplitter(),\n",
    "                 sentence_splitter: SpacySentenceSplitter = SpacySentenceSplitter(rule_based=True),\n",
    "                 context_size: int = 1) -> None:\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentence_splitter = sentence_splitter\n",
    "        self.context_size = context_size\n",
    "        self.data_series = data_series\n",
    "        self.csvs = csvs\n",
    "        self.queue = mp.Queue(maxsize=10000)\n",
    "        self.stop = True\n",
    "        # self.sentences = []\n",
    "        # self.next_sentence_ids = []\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_instance(tokens: List[Token], token_indexers, context: List[Token] = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "        \n",
    "        if context:\n",
    "            label_field = TextField(context, token_indexers)\n",
    "            fields[\"context\"] = label_field\n",
    "        \n",
    "        return Instance(fields)\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Instance]:\n",
    "        select_context_part = functools.partial(self.select_context_from_sentence, \n",
    "                                                context_size = self.context_size)\n",
    "        instances = self.iterate_data(select_context_part, True)\n",
    "        if isinstance(instances, list):\n",
    "            raise ConfigurationError(\"For a lazy dataset reader, word_context() must return a generator\")\n",
    "        return instances\n",
    "         \n",
    "    @staticmethod       \n",
    "    def select_context_from_sentence(sentence: List[Token], context_size: int):\n",
    "        j = random.randint(0, len(sentence))\n",
    "        context1 = [word for word in sentence[j-context_size:j]]\n",
    "        while len(context1) < context_size:\n",
    "            context1.insert(0, Token())\n",
    "        context2 = [word for word in sentence[j+1:j+context_size+1]]\n",
    "        while len(context1) < context_size:\n",
    "            context2.append(Token())\n",
    "        context1.extend(context2)\n",
    "        return [sentence[j]], context1\n",
    "\n",
    "    def it_from_ser(self) -> Iterator[Instance]:\n",
    "        instances = self.iterate_data(None, False)\n",
    "        if isinstance(instances, list):\n",
    "            raise ConfigurationError(\"For a lazy dataset reader, word_context() must return a generator\")\n",
    "            \n",
    "        return instances\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_chunk(tokenizer, sentences: List[str], split_sentences: bool = True, sentence_splitter = None):\n",
    "        print(len(sentences))\n",
    "        print(sentences[0])\n",
    "        print(sentences[-1])\n",
    "        ret_list = []\n",
    "        if split_sentences:\n",
    "            arts = sentence_splitter.batch_split_sentences(sentences)\n",
    "            for sentences in arts: \n",
    "                sentences = tokenizer.batch_split_words(sentences)\n",
    "                for sentence in sentences:\n",
    "                    ret_list.append(sentence)\n",
    "        else:\n",
    "            sentences = tokenizer.batch_split_words(sentences)\n",
    "            for sentence in sentences:\n",
    "                ret_list.append(sentence)\n",
    "        print(len(ret_list))\n",
    "        print(ret_list[0])\n",
    "        print(ret_list[-1])\n",
    "        return ret_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_on_sep(sentence, sep=';'):\n",
    "        return [Token(x) for x in sentence.replace('.', '').split(';')]\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_chunk(sentences):\n",
    "        ret_list = []\n",
    "        sentences = sentences['sentence'].values\n",
    "        for sentence in sentences:\n",
    "            if not len(sentence):\n",
    "                ret_list.append([Token('')])\n",
    "            if not sentence or len(sentence) > 2000:\n",
    "                ret_list.append([Token('')])\n",
    "            words = SentenceDataset.split_on_sep(sentence)\n",
    "            ret_list.append(words)\n",
    "        return ret_list\n",
    "        print('Waiting for Queue....')\n",
    "        queue.put(words)\n",
    "        print('Put to Queue....')\n",
    "        return \n",
    "        return words\n",
    "        #sentences = sentences.apply(lambda x: x.replace('\\uffef', '')).values\n",
    "        return SentenceDataset.process_chunk(tokenizer, sentences, False)\n",
    "    \n",
    "    \n",
    "    def iterate_data(self, apply_to_sentence: Callable,\n",
    "                     split_sentences: bool = True, ) -> Iterator[Union[Instance, Instance]]:\n",
    "        next_sentence_id = 0\n",
    "        for ser in self.data_series:\n",
    "            sentences = ser[pd.notnull(ser)].values\n",
    "            sentence_word_list = SentenceDataset.process_chunk(\n",
    "                self.tokenizer, sentences, split_sentences, \n",
    "                self.sentence_splitter)\n",
    "            for sentence in sentence_word_list:\n",
    "                if len(sentence) < 3:\n",
    "                    continue\n",
    "                if apply_to_sentence is not None:\n",
    "                    sentence, context = apply_to_sentence(sentence)\n",
    "                    yield SentenceDataset.text_to_instance(sentence, self.token_indexers, context)\n",
    "                else:\n",
    "                    yield SentenceDataset.text_to_instance(sentence, self.token_indexers)\n",
    "        for f in self.csvs:\n",
    "            pool = mp.Pool(14)\n",
    "            reader = pd.read_csv(f, chunksize=int(50), dtype=str, sep='\\uffef', engine='python')\n",
    "            for sentences in pool.imap(SentenceDataset.split_chunk, reader):\n",
    "                for sentence in sentences:\n",
    "                    if len(sentence) < 3:\n",
    "                        continue\n",
    "                    if apply_to_sentence is not None:\n",
    "                        sentence, context = apply_to_sentence(sentence)\n",
    "                        yield SentenceDataset.text_to_instance(sentence, self.token_indexers, context)\n",
    "                    else:\n",
    "                        yield SentenceDataset.text_to_instance(sentence, self.token_indexers)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramLanguageModelerSparse(Model):\n",
    "\n",
    "    def __init__(self, vocab: Vocabulary, embedding_dim, context_size, \n",
    "                 embedding_freq, negative_sampling_size):\n",
    "        super().__init__(vocab)\n",
    "        vocab_size = vocab.get_vocab_size('tokens')\n",
    "        print(vocab_size)\n",
    "        self.embedding_freq = torch.autograd.Variable(torch.Tensor(embedding_freq)**(0.75))\n",
    "        self.negative_sampling_size = negative_sampling_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last_loss = None\n",
    "\n",
    "    def forward(self, sentence, context=None):\n",
    "        \n",
    "        word = sentence['tokens']\n",
    "        \n",
    "        # Allen NLP demands a dictionary to be returned\n",
    "        # It has to contain the key loss for training purposes\n",
    "        output = {}\n",
    "        word_emb = self.word_embedding(word)\n",
    "        output['vec'] = word_emb + self.context_embedding(word)\n",
    "\n",
    "        if context is None:\n",
    "            #return embedding\n",
    "            return output\n",
    "        \n",
    "        context = context['tokens']\n",
    "        \n",
    "        context = self.context_embedding(context)\n",
    "        word_pos = word_emb.expand_as(context)\n",
    "        \n",
    "        product_pos = (word_pos*context).sum(dim=-1).mean()\n",
    "        target_pos = torch.autograd.Variable(torch.ones(product_pos.size()))\n",
    "        loss_positive = nn.functional.binary_cross_entropy_with_logits(product_pos, target_pos)\n",
    "        negative_context = self.embedding_freq.multinomial(\n",
    "            self.negative_sampling_size*context.size(0)*context.size(1), replacement=True).to(self.device())\n",
    "        negative_context = negative_context.view(-1,self.negative_sampling_size*context.size(1))\n",
    "        negative_context = self.context_embedding(negative_context)\n",
    "        word_neg = word_emb.expand_as(negative_context)\n",
    "        product_neg = (word_neg*negative_context).sum(dim=-1).mean()\n",
    "        target_neg = torch.autograd.Variable(torch.zeros(product_neg.size()))\n",
    "        loss_negative = nn.functional.binary_cross_entropy_with_logits(product_neg,target_neg)\n",
    "        self.last_loss = loss_positive + loss_negative\n",
    "        output['loss'] = loss_positive + loss_negative\n",
    "        output['vec'] = word_pos + context\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"bce\": self.last_loss}\n",
    "    \n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def log_probs(self, inp, out, neg):\n",
    "        losses = []\n",
    "        inp = self.embeddings(inp)\n",
    "        out = self.embeddings(out)\n",
    "        neg = self.embeddings(neg)\n",
    "        score = torch.mul(inp, out)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = nn.functional.logsigmoid(score)\n",
    "        neg_score = torch.bmm(neg, inp.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(neg_score, dim=0)\n",
    "        neg_score = nn.functional.logsigmoid(-1 * neg_score)\n",
    "        losses.append(sum(neg_score))\n",
    "        losses.append(sum(score))\n",
    "        return -1 * sum(losses)\n",
    "    \n",
    "class SkipGramLanguageModelerSparseNoNeg(Model):\n",
    "\n",
    "    def __init__(self, vocab: Vocabulary, embedding_dim, context_size):\n",
    "        super().__init__(vocab)\n",
    "        vocab_size = vocab.get_vocab_size('tokens')\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last_loss = None\n",
    "\n",
    "    def forward(self, sentence, context=None):\n",
    "        \n",
    "        word = sentence['tokens']\n",
    "        \n",
    "        # Allen NLP demands a dictionary to be returned\n",
    "        # It has to contain the key loss for training purposes\n",
    "        output = {}\n",
    "        word_emb = self.word_embedding(word)\n",
    "        output['vec'] = word_emb + self.context_embedding(word)\n",
    "\n",
    "        if context is None:\n",
    "            #return embedding\n",
    "            return output\n",
    "        \n",
    "        context = context['tokens']\n",
    "        \n",
    "        context = self.context_embedding(context)\n",
    "        word_pos = word_emb.expand_as(context)\n",
    "        \n",
    "        product_pos = (word_pos*context).sum(dim=-1).mean()\n",
    "        target_pos = torch.autograd.Variable(torch.ones(product_pos.size()))\n",
    "        loss_positive = nn.functional.binary_cross_entropy_with_logits(product_pos, target_pos)\n",
    "        self.last_loss = loss_positive\n",
    "        output['loss'] = loss_positive\n",
    "        output['vec'] = word_pos + context\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"bce\": self.last_loss}\n",
    "    \n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def log_probs(self, inp, out, neg):\n",
    "        losses = []\n",
    "        inp = self.embeddings(inp)\n",
    "        out = self.embeddings(out)\n",
    "        neg = self.embeddings(neg)\n",
    "        score = torch.mul(inp, out)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = nn.functional.logsigmoid(score)\n",
    "        neg_score = torch.bmm(neg, inp.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(neg_score, dim=0)\n",
    "        neg_score = nn.functional.logsigmoid(-1 * neg_score)\n",
    "        losses.append(sum(neg_score))\n",
    "        losses.append(sum(score))\n",
    "        return -1 * sum(losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = home + '/data/workshop_data/southpark_full.csv'\n",
    "corpus = pd.read_csv(path, sep='\\t', dtype=str, error_bad_lines=False)\n",
    "wiki_path = home + '/data/workshop_data/wiki_out.csv'\n",
    "ds = SentenceDataset([corpus['spoken']], [wiki_path])\n",
    "train_dataset = ds.it_from_ser()\n",
    "word2vec_dataset = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66413\n",
      "School day, school day, teacher's golden ru...\n",
      " Billy... What have I done?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11572it [00:11,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66413\n",
      "[School, day, ,, school, day, ,, teacher, 's, golden, ru, ...]\n",
      "[Billy, ..., What, have, I, done, ?]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61360it [02:40, 382.66it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save_to_files(home + '/data/workshop_data/vocab.store')\n",
    "with open(home + '/data/workshop_data/vocab.dict', 'w') as f:\n",
    "    for k, v in vocab._retained_counter['tokens'].items():\n",
    "        f.write(f'{k},{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dict = {}\n",
    "with open(home + '/data/workshop_data/vocab.dict', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    # line = f.readline()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            comma_index = line.rfind(',')\n",
    "            token = line[:comma_index]\n",
    "            if (#'|' in token or '/' in token or \n",
    "                #len(token) == 1 or \n",
    "                len(token) > 300 \n",
    "                #or 'style=\"' in token or '<' in token or '!colspan=' in token\n",
    "            ):\n",
    "                    continue\n",
    "            count = int(line[comma_index+1:])\n",
    "            if count < 300:\n",
    "                continue\n",
    "            add_dict[token] = count\n",
    "            # line = f.readline()\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "            \n",
    "    \n",
    "vocab_new = Vocabulary(counter={'tokens': add_dict}, min_count={'tokens': 10}) \n",
    "print(len(list(add_dict.values())))\n",
    "print(vocab_new.get_vocab_size('tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_new.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_new.save_to_files(home + '/data/workshop_data/vocab_new.store')\n",
    "with open(home + '/data/workshop_data/vocab_new.dict', 'w') as f:\n",
    "    for k, v in add_dict.items():\n",
    "        f.write(f'{k},{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_new = Vocabulary.from_files('vocab_new.store')\n",
    "add_dict = {}\n",
    "with open(home + '/data/workshop_data/vocab_new.dict', 'r') as f:\n",
    "    content = f.read()\n",
    "    lines = content.split('\\n')\n",
    "    # line = f.readline()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            comma_index = line.rfind(',')\n",
    "            token = line[:comma_index]\n",
    "            count = int(line[comma_index+1:])\n",
    "            add_dict[token] = count\n",
    "            # line = f.readline()\n",
    "        except ValueError:\n",
    "            print(line)\n",
    "vocab_new = Vocabulary(counter={'tokens': add_dict}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(add_dict.values()))\n",
    "print(vocab_new.get_vocab_size('tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "batch_size = 10240\n",
    "\n",
    "context_size = 3\n",
    "n_batches = 64#len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "neg_samples = 10\n",
    "dict_values = list(add_dict.values())\n",
    "dict_values.insert(0, 0)\n",
    "dict_values.insert(0, 0)\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipGramLanguageModelerSparse(\n",
    "    vocab_new, embedding_dims, context_size, \n",
    "    dict_values, \n",
    "    neg_samples)\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=1e-4)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "iterator = BucketIterator(batch_size=batch_size, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab_new)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=word2vec_dataset,\n",
    "                  # validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=2,\n",
    "                  cuda_device=0)\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader=ds)\n",
    "\n",
    "vec = predictor.predict(\"Cartman\")['vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_word(vec, model, vocab):\n",
    "    score = vec.view(1, -1).expand_as(model.word_embedding.weight)*model.word_embedding.weight\n",
    "    score = score.sum(dim=-1)\n",
    "    norm = vec.view(1, -1).expand_as(model.word_embedding.weight).norm(dim=1)*model.word_embedding.weight.norm(dim=1)\n",
    "    word_idx = (score / norm).argmax().numpy()\n",
    "    print(word_idx)\n",
    "    word = vocab.get_token_from_index(int(word_idx))\n",
    "    return word\n",
    "\n",
    "out = predictor.predict(\"cartman asshole girl\")['vec']\n",
    "print(out)\n",
    "vec = torch.tensor(out[0]) - torch.tensor(out[1]) #+ torch.tensor(out[2])\n",
    "print(out[0], vec)\n",
    "#vec = (out[0] + out[1]).unsqueeze(0)\n",
    "print(get_closest_word(vec, model, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "\n",
    "tag_logits = predictor.predict(\"The dog ate the apple\")['tag_logits']\n",
    "\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/home/jendrik/git/DLWorkshop/data/all-seasons.csv'\n",
    "print_lines(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 2\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "ls = tokenizer.tokenize(corpus.iloc[0]['Line'])\n",
    "print(ls)\n",
    "for sentence in ls:\n",
    "    print([lmtzr.lemmatize(word) for word in nltk.word_tokenize(sentence) if word.isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "vocab = {' ': 1}\n",
    "corpus_len = 0\n",
    "for i, row in corpus.iterrows():\n",
    "    ls = tokenizer.tokenize(row['Line'])\n",
    "    for sentence in ls:\n",
    "        stemmed_sentence = [lmtzr.lemmatize(word.lower()) for word in nltk.word_tokenize(sentence) if word.isalpha()]\n",
    "        if len(stemmed_sentence) > 2:\n",
    "            corpus_len += len(stemmed_sentence)\n",
    "            for word in stemmed_sentence:\n",
    "                try:\n",
    "                    vocab[word] += 1\n",
    "                except KeyError:\n",
    "                    vocab[word] = 1\n",
    "            sentences.append(stemmed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab.keys()))\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dict = {}\n",
    "sample_dict = {}\n",
    "for k, v in vocab.items():\n",
    "    z = v/corpus_len\n",
    "    drop_dict[word_to_ix[k]] = (np.sqrt(z*1e3) + 1)*1e-3/z\n",
    "    sample_dict[word_to_ix[k]] = (v/corpus_len)**0.75\n",
    "sample_dict_sum = sum(sample_dict.values())\n",
    "for k in sample_dict:\n",
    "    sample_dict[k] /= sample_dict_sum\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs).mean(1)#.view((inputs.shape[0], -1))\n",
    "        #out = self.relu(self.linear1(embeds))\n",
    "        return out\n",
    "    \n",
    "    def log_probs(self, out):\n",
    "        out = self.linear(out)\n",
    "        print(out.shape)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "class SkipGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear1 = nn.Linear(embedding_dim, vec_size)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        return embeds\n",
    "    \n",
    "    def log_probs(self, out):\n",
    "        out = self.linear(out)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "class NGramsLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, vec_size)\n",
    "        self.linear2 = nn.Linear(vec_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))\n",
    "        out = self.relu(self.linear1(embeds))\n",
    "        return out\n",
    "    \n",
    "    def log_probs(self, out):\n",
    "        out = self.linear2(out)\n",
    "        print(out.shape)\n",
    "        log_probs = nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "class SkipGramLanguageModelerSparse(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, vec_size, embedding_freq, negative_sampling_size):\n",
    "        super().__init__()\n",
    "        self.embedding_freq = torch.autograd.Variable(torch.Tensor(embedding_freq)**(0.75))\n",
    "        self.negative_sampling_size = negative_sampling_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        #self.linear1 = nn.Linear(embedding_dim, vec_size)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #return embedding\n",
    "        return self.word_embedding(inputs) + self.context_embedding(inputs)\n",
    "    \n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def calculate_loss(self, word, context,alpha=0.1):\n",
    "        #word.size() batch_size\n",
    "        #context.size() batch_size*window_size\n",
    "        word_pos = word.view(-1,1).expand_as(context)\n",
    "        word_pos = self.word_embedding(word_pos)\n",
    "        context = self.context_embedding(context)\n",
    "        product_pos = (word_pos*context).sum(dim=-1).mean()\n",
    "        target_pos = torch.autograd.Variable(torch.ones(product_pos.size()))\n",
    "        loss_positive = nn.functional.binary_cross_entropy_with_logits(product_pos, target_pos)\n",
    "        negative_context = self.embedding_freq.multinomial(\n",
    "            self.negative_sampling_size*context.size(0)*context.size(1), replacement=True).to(self.device())\n",
    "        negative_context = negative_context.view(-1,self.negative_sampling_size*context.size(1))\n",
    "        word_neg = word.view(-1,1).expand_as(negative_context)\n",
    "        word_neg = self.word_embedding(word_neg)\n",
    "        negative_context = self.context_embedding(negative_context)\n",
    "        product_neg = (word_neg*negative_context).sum(dim=-1).mean()\n",
    "        target_neg = torch.autograd.Variable(torch.zeros(product_neg.size()))\n",
    "        loss_negative = nn.functional.binary_cross_entropy_with_logits(product_neg,target_neg)\n",
    "        loss = loss_positive + loss_negative\n",
    "        return loss\n",
    "    \n",
    "    def log_probs(self, inp, out, neg):\n",
    "        losses = []\n",
    "        inp = self.embeddings(inp)\n",
    "        out = self.embeddings(out)\n",
    "        neg = self.embeddings(neg)\n",
    "        score = torch.mul(inp, out)\n",
    "        print(score.shape)\n",
    "        score = torch.sum(score, dim=1)\n",
    "        print(score.shape)\n",
    "        score = nn.functional.logsigmoid(score)\n",
    "        print(score.shape)\n",
    "        neg_score = torch.bmm(neg, inp.unsqueeze(2)).squeeze()\n",
    "        print(neg_score.shape)\n",
    "        neg_score = torch.sum(neg_score, dim=0)\n",
    "        print(neg_score.shape)\n",
    "        neg_score = nn.functional.logsigmoid(-1 * neg_score)\n",
    "        print(neg_score.shape)\n",
    "        losses.append(sum(neg_score))\n",
    "        losses.append(sum(score))\n",
    "        input()\n",
    "        return -1 * sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(sentences, context_size, word_to_idx, batch_size, mode):\n",
    "    selected_sentences = np.random.choice(sentences, batch_size)\n",
    "    inp_tokens = []\n",
    "    out_tokens = []\n",
    "    for sentence in selected_sentences:\n",
    "        if mode == 'cbow':\n",
    "            selected_word_idx = np.random.randint(context_size, len(sentence)-context_size)\n",
    "        else:\n",
    "            selected_word_idx = np.random.randint(context_size, len(sentence))\n",
    "        context = [word_to_idx[word] for word in sentence[selected_word_idx-context_size:selected_word_idx]]\n",
    "        while(len(context) < context_size):\n",
    "            context.insert(0, word_to_idx[' '])\n",
    "        if mode == 'cbow':\n",
    "            post_context = [word_to_idx[word] for word in sentence[selected_word_idx+1:selected_word_idx+1+context_size]]\n",
    "            while(len(post_context) < context_size):\n",
    "                post_context.append(word_to_idx[' '])\n",
    "            context.extend(post_context)\n",
    "        inp_tokens.append(np.array(context))\n",
    "        out_tokens.append(np.array(word_to_idx[sentence[selected_word_idx]]))\n",
    "    return np.array(inp_tokens), np.array(out_tokens)\n",
    "\n",
    "def get_batch_w2v(sentences, context_size, word_to_idx, batch_size, drop_dict, neg_samples):\n",
    "    selected_sentences = np.random.choice(sentences, batch_size)\n",
    "    vec = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    out_tokens = []\n",
    "    len_longest_sentence = 0\n",
    "    for sentence in selected_sentences:\n",
    "        selected_word_idx = np.random.randint(0, len(sentence))\n",
    "        drop = drop_dict[word_to_idx[sentence[selected_word_idx]]] < np.random.rand()\n",
    "        while drop:\n",
    "            sentence = np.random.choice(sentences)\n",
    "            selected_word_idx = np.random.randint(0, len(sentence))\n",
    "            drop = drop_dict[word_to_idx[sentence[selected_word_idx]]] < np.random.rand()\n",
    "        pos_words = sentence[0:selected_word_idx]\n",
    "        pos_words.extend(sentence[selected_word_idx+1:])\n",
    "        len_longest_sentence = len(pos_words) if len_longest_sentence < len(pos_words) else len_longest_sentence\n",
    "        pos_indices = [word_to_idx[word] for word in pos_words]\n",
    "        vec.append(selected_word_idx)\n",
    "        pos.append(pos_indices)\n",
    "    pos_padded = []\n",
    "    for pos_indices in pos:\n",
    "        padded = [word_to_idx[' '] for i in range(len_longest_sentence-len(pos_indices))]\n",
    "        padded.extend(pos_indices)\n",
    "        pos_padded.append(padded)\n",
    "    return np.array(vec), np.array(pos_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "batch_size = 128\n",
    "n_batches = 64#len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "neg_samples = 20\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipGramLanguageModelerSparse(len(vocab), embedding_dims, context_size, vec_dims, list(vocab.values()), 10)\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        vec, pos = get_batch_w2v(sentences, context_size, word_to_ix,\n",
    "                                               batch_size, drop_dict, neg_samples=neg_samples)\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        vec = torch.from_numpy(vec).long().to(device)\n",
    "        pos = torch.from_numpy(pos).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        loss = model.calculate_loss(vec, pos)\n",
    "\n",
    "        \n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}', end='\\r')\n",
    "    print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}')\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "batch_size = 128\n",
    "n_batches = len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramsLanguageModeler(len(vocab), embedding_dims, context_size, vec_dims)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        inp_tokens, out_tokens = get_batch(sentences, context_size, word_to_ix, batch_size, mode='ngrams')\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.from_numpy(inp_tokens).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        out = model(context_idxs)\n",
    "        log_probs = model.log_probs(out)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.from_numpy(out_tokens).long().to(device))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}', end='\\r')\n",
    "    print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}')\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cuda available {torch.cuda.is_available()}')\n",
    "embedding_dims = 128\n",
    "batch_size = 128\n",
    "n_batches = len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipGramLanguageModeler(len(vocab), embedding_dims, context_size, vec_dims)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        out_tokens, inp_tokens = get_batch(sentences, context_size, word_to_ix, batch_size, mode='cbow')\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.from_numpy(inp_tokens).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        out = model(context_idxs)\n",
    "        log_probs = model.log_probs(out)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = None\n",
    "        for j in range(out_tokens.shape[1]):\n",
    "            try:\n",
    "                loss += loss_function(log_probs, torch.from_numpy(out_tokens[:, j]).long().to(device))\n",
    "            except TypeError:\n",
    "                loss = loss_function(log_probs, torch.from_numpy(out_tokens[:, j]).long().to(device))\n",
    "        loss = loss/out_tokens.shape[1]\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}', end='\\r')\n",
    "    print(f'Epoch: {epoch}, Batch: {i}/{n_batches} loss: {total_loss/(i+1)}')\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "context_size = 2\n",
    "batch_size = 128\n",
    "n_batches = len(sentences)//batch_size+1\n",
    "vec_dims = 128\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOWLanguageModeler(len(vocab), embedding_dims, context_size, vec_dims)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batches):\n",
    "        inp_tokens, out_tokens = get_batch(sentences, context_size, word_to_ix, batch_size, mode='cbow')\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.from_numpy(inp_tokens).long().to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        out = model(context_idxs)\n",
    "        log_probs = model.log_probs(out)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.from_numpy(out_tokens).long().to(device))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss/n_batches)\n",
    "    losses.append(total_loss/n_batches)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cosine_distance(id0, id1, model):\n",
    "    out = model(torch.from_numpy(np.array([id0, id1])).long().to(device))\n",
    "    sk_sim = cosine_similarity(out.data.cpu().numpy())\n",
    "    return (out[0] * out[1] / (out[0].norm() * out[1].norm())).sum(), sk_sim\n",
    "    \n",
    "print(cosine_distance(word_to_ix['nice'], word_to_ix['good'], model))\n",
    "print(cosine_distance(word_to_ix['fantastic'], word_to_ix['good'], model))\n",
    "print(cosine_distance(word_to_ix['terrible'], word_to_ix['good'], model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartman_id = word_to_ix['man']\n",
    "asshole_id = word_to_ix['boy']\n",
    "poor_id = word_to_ix['girl']\n",
    "print([cartman_id, asshole_id, poor_id])\n",
    "out = model(torch.from_numpy(np.array([cartman_id, asshole_id, poor_id])).long().to(device))\n",
    "print(out.shape)\n",
    "print(out[1] - out[2])\n",
    "vec = out[0] - out[1] + out[2]\n",
    "print(out[0], vec)\n",
    "#vec = (out[0] + out[1]).unsqueeze(0)\n",
    "print(vec.shape)\n",
    "print(model.word_embedding.weight.shape)\n",
    "score = vec.view(1, -1).expand_as(model.word_embedding.weight)*model.word_embedding.weight\n",
    "score = score.sum(dim=-1)\n",
    "norm = vec.view(1, -1).expand_as(model.word_embedding.weight).norm(dim=1)*model.word_embedding.weight.norm(dim=1)\n",
    "print(score.shape)\n",
    "print(norm.shape)\n",
    "print(score)\n",
    "print(norm)\n",
    "word = (score / norm)\n",
    "print(word.shape)\n",
    "print(word)\n",
    "word = word.argmax()\n",
    "print(ix_to_word[int(word.cpu().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_name = \"cornell_movie-dialogs_corpus\"\n",
    "corpus = os.path.join(\"../data\", corpus_name)\n",
    "\n",
    "def print_lines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "print_lines(os.path.join(corpus, \"movie_lines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each line of the file into a dictionary of fields\n",
    "def load_lines(file_name, fields):\n",
    "    lines = []\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            line_dict = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                line_dict[field] = values[i]\n",
    "            lines.append(line_dict)\n",
    "    return pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_lines = [\"line_id\", \"character_id\", \"movie_id\", \"character\", \"text\"]\n",
    "header_conversations = [\"character1_id\", \"character2_id\", \"movie_id\", \"utterance_id\"]\n",
    "\n",
    "# Load lines and process conversations\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = load_lines(os.path.join(corpus, \"movie_lines.txt\"), header_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\n",
    "def load_conversations(file_name, lines, fields):\n",
    "    conversations = []\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            conversation = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                conversation[field] = values[i]\n",
    "            # Convert string to list (conversation[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
    "            line_ids = eval(conversation[\"utteranceIDs\"])\n",
    "            # Reassemble lines\n",
    "            conversation[\"lines\"] = []\n",
    "            for line_id in line_ids:\n",
    "                conversation[\"lines\"].append(lines.loc[lines['line_id'] == line_id])\n",
    "            conversations.append(conversation)\n",
    "    return pd.DataFrame(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_conversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
    "                                  lines, header_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
